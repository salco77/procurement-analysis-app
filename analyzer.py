import os
import requests
import pandas as pd
from datetime import datetime, timedelta, date
import urllib.parse
import time
import sqlite3
import google.generativeai as genai
import io
from itertools import groupby
import logging
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# --- 0. [ì‹ ê·œ] íšŒì‚¬ í”„ë¡œí•„ ë° ë¶„ì„ ì„¤ì • ---

# AIê°€ ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ íšŒì‚¬ í”„ë¡œí•„ ì •ì˜
COMPANY_PROFILE = """
ìš°ë¦¬ íšŒì‚¬ëŠ” ê°€ìƒí˜„ì‹¤(VR), ì¦ê°•í˜„ì‹¤(AR), í™•ì¥í˜„ì‹¤(XR) ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” êµ°ì‚¬ ë° ê²½ì°° í›ˆë ¨ì²´ê³„ ì „ë¬¸ ê¸°ì—…ì…ë‹ˆë‹¤.
í•µì‹¬ ê¸°ìˆ :
1. ìœ„ì¹˜ ë° ë™ì‘ ì¸ì‹ ê¸°ìˆ  ê¸°ë°˜ì˜ ì •ë°€í•œ ê³µê°„ ì •í•© (ê°€ìƒí™˜ê²½ê³¼ í˜„ì‹¤ê³µê°„ ë§¤ì¹­)
2. ëª°ì…í˜• ê°€ìƒí™˜ê²½ êµ¬í˜„ ë° ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©
ì£¼ìš” ì‚¬ì—… ë¶„ì•¼:
- ì˜ìƒ ëª¨ì˜ ì‚¬ê²© í›ˆë ¨ ì‹œìŠ¤í…œ
- ê°€ìƒ ê³µìˆ˜ ê°•í•˜(ë‚™í•˜ì‚°) í›ˆë ¨ ì‹œë®¬ë ˆì´í„°
- ë°•ê²©í¬/ì „ì°¨/í•­ê³µê¸° ë“± êµ°ì‚¬ ì¥ë¹„ ìš´ìš© ë° ì „ìˆ  í›ˆë ¨ ì‹œë®¬ë ˆì´í„°
- ëŒ€í…ŒëŸ¬, ì†Œë¶€ëŒ€ ì „íˆ¬(CQB) ë“± íŠ¹ìˆ˜ ëª©ì  í›ˆë ¨ ì‹œìŠ¤í…œ
- ì¥ë¹„ ìœ ì§€ë³´ìˆ˜(MRO)ë¥¼ ìœ„í•œ ê°€ìƒ/ì¦ê°•í˜„ì‹¤ ì†”ë£¨ì…˜
"""

# ê´‘ë²”ìœ„ íƒìƒ‰ìš© í‚¤ì›Œë“œ (AI ë¶„ì„ ëŒ€ìƒ ì„ ë³„ìš©)
BROAD_KEYWORDS = {"í›ˆë ¨", "ì²´ê³„", "ì‹œìŠ¤í…œ", "ëª¨ì˜", "ê°€ìƒ", "ì¦ê°•", "ì‹œë®¬ë ˆì´í„°", "ì‹œë®¬ë ˆì´ì…˜", "ê³¼í•™í™”", "êµìœ¡", "ì—°êµ¬ê°œë°œ"}

# --- 1. í‚¤ì›Œë“œ ê´€ë¦¬ ---
KEYWORD_FILE = "keywords.txt"
# ìƒì„¸ í‚¤ì›Œë“œ (ê¸°ì¡´ í‚¤ì›Œë“œ + ì‹ ê·œ ì¶”ê°€)
INITIAL_KEYWORDS = ["ì§€ë¢°","ë“œë¡ ","ì‹œë®¬ë ˆì´í„°","ì‹œë®¬ë ˆì´ì…˜","ì „ì°¨","ìœ ì§€ë³´ìˆ˜","MRO","í•­ê³µ","ê°€ìƒí˜„ì‹¤","ì¦ê°•í˜„ì‹¤","í›ˆë ¨","VR","AR","MR","XR","ëŒ€í…ŒëŸ¬","ì†Œë¶€ëŒ€","CQB","íŠ¹ì „ì‚¬","ê²½ì°°ì²­","ì˜ìƒì‚¬ê²©","ëª¨ì˜ì‚¬ê²©","ê³µìˆ˜ê°•í•˜","ë°•ê²©í¬"]

def load_keywords(initial_keywords: list) -> set:
    if not os.path.exists(KEYWORD_FILE):
        save_keywords(set(initial_keywords))
        return set(initial_keywords)
    try:
        with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        logging.error(f"Error loading keywords: {e}")
        return set(initial_keywords)

def save_keywords(keywords: set):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            for keyword in sorted(list(keywords)):
                f.write(keyword + '\n')
    except Exception as e:
        logging.error(f"Error saving keywords: {e}")

# --- 2. ìœ í‹¸ë¦¬í‹° ë° API í´ë¼ì´ì–¸íŠ¸ ---

def safe_int(value):
    try:
        return int(float(value)) if value is not None and str(value).strip() != "" else None
    except (ValueError, TypeError):
        return None

def format_price(x):
    if pd.isna(x):
        return ""
    try:
        if isinstance(x, str):
             x = x.replace(',', '')
        if str(x).strip() == "":
             return ""
        return f"{int(float(x)):,}"
    except (ValueError, TypeError):
        return ""

def save_integrated_excel(data_frames: dict) -> bytes:
    """í†µí•© ì—‘ì…€ ì €ì¥. AI ì ìˆ˜ ì¡°ê±´ë¶€ ì„œì‹ ì¶”ê°€."""
    output = io.BytesIO()
    try:
        # xlsxwriter ì„í¬íŠ¸ í™•ì¸ ë° ìœ í‹¸ë¦¬í‹° ì ‘ê·¼ ì‹œë„
        import xlsxwriter.utility

        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            workbook = writer.book

            # ìŠ¤íƒ€ì¼ ì •ì˜
            order_plan_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#FFF2CC', 'border': 1})
            main_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#D3D3D3', 'border': 1})
            # AI ë¶„ì„ ì„¹ì…˜ ìŠ¤íƒ€ì¼ (ì—°í•œ ë…¹ìƒ‰)
            ai_analysis_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#E2F0D9', 'border': 1})

            for sheet_name, df_data in data_frames.items():
                if df_data is None or df_data.empty: continue
                
                # MultiIndex ì²˜ë¦¬ ë¡œì§ (ì¢…í•© í˜„í™© ë³´ê³ ì„œ)
                if isinstance(df_data.columns, pd.MultiIndex) and df_data.columns.nlevels == 2:
                    worksheet = workbook.add_worksheet(sheet_name)
                    writer.sheets[sheet_name] = worksheet
                    
                    # 1. í—¤ë” ì‘ì„± (ë³‘í•© í¬í•¨)
                    col_idx = 0
                    level0_headers = df_data.columns.get_level_values(0)
                    level1_headers = df_data.columns.get_level_values(1)
                    
                    # ì„¹ì…˜ë³„ í—¤ë” ìŠ¤íƒ€ì¼ ì ìš©
                    def get_header_format(header_name):
                        if header_name == 'AI ë¶„ì„ ê²°ê³¼':
                            return ai_analysis_header_format
                        return main_header_format

                    for header, group in groupby(level0_headers):
                        span = len(list(group))
                        current_format = get_header_format(header)
                        
                        if span > 1:
                            worksheet.merge_range(0, col_idx, 0, col_idx + span - 1, str(header), current_format)
                        else:
                            worksheet.write(0, col_idx, str(header), current_format)
                        for i in range(span):
                            worksheet.write(1, col_idx + i, str(level1_headers[col_idx + i]), current_format)
                        col_idx += span

                    # 2. ë°ì´í„° ì‘ì„±
                    start_row = 2
                    def clean_data(x): return x if pd.notna(x) else ""
                    
                    try:
                        if hasattr(df_data, 'map'):
                             data_to_write = df_data.map(clean_data).values.tolist()
                        else:
                             data_to_write = df_data.applymap(clean_data).values.tolist()
                    except Exception:
                        data_to_write = df_data.fillna("").values.tolist()

                    for row_data in data_to_write:
                        worksheet.write_row(start_row, 0, row_data)
                        start_row += 1
                    
                    # 3. ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì • ë° ì¡°ê±´ë¶€ ì„œì‹
                    score_col_idx = -1
                    for i in range(len(df_data.columns)):
                        # ë„ˆë¹„ ì¡°ì •
                        header_len = max(len(str(level0_headers[i])), len(str(level1_headers[i])))
                        try:
                            data_max_len = max((len(str(row[i])) for row in data_to_write), default=0)
                        except Exception:
                            data_max_len = 10
                        max_len = max(header_len, data_max_len)
                        # ë¶„ì„ ì´ìœ ëŠ” ë” ë„“ê²Œ ì„¤ì •
                        width = min(80, max(15, max_len + 2)) if level1_headers[i] == 'ë¶„ì„ ì´ìœ ' else min(60, max(10, max_len + 2))
                        worksheet.set_column(i, i, width)
                        
                        # ì ìˆ˜ ì»¬ëŸ¼ ì¸ë±ìŠ¤ ì°¾ê¸°
                        if level1_headers[i] == 'ê´€ë ¨ì„± ì ìˆ˜':
                            score_col_idx = i

                    # ì¡°ê±´ë¶€ ì„œì‹ ì ìš© (3ìƒ‰ ìŠ¤ì¼€ì¼)
                    if score_col_idx != -1:
                        col_letter = xlsxwriter.utility.xl_col_to_name(score_col_idx)
                        cell_range = f'{col_letter}3:{col_letter}{len(data_to_write) + 2}'
                        worksheet.conditional_format(cell_range, {
                            'type': '3_color_scale',
                            'min_value': 0, 'mid_value': 50, 'max_value': 100,
                            'min_color': "#F8696B", 'mid_color': "#FFEB84", 'max_color': "#63BE7B"
                        })

                # ë°œì£¼ê³„íš í˜„í™© ì‹œíŠ¸ ì²˜ë¦¬
                elif sheet_name == "ë°œì£¼ê³„íš í˜„í™©":
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False, startrow=1, header=False)
                    worksheet = writer.sheets[sheet_name]
                    
                    # í—¤ë” ì‘ì„± ë° ìŠ¤íƒ€ì¼ ì ìš©
                    for col_num, value in enumerate(df_data.columns.values):
                        # AI ê´€ë ¨ ì»¬ëŸ¼ ìŠ¤íƒ€ì¼ ì ìš©
                        current_format = ai_analysis_header_format if 'AI' in value else order_plan_header_format
                        worksheet.write(0, col_num, value, current_format)
                    
                    # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì • ë° ì¡°ê±´ë¶€ ì„œì‹
                    score_col_idx = -1
                    for i, col in enumerate(df_data.columns):
                        # ë„ˆë¹„ ì¡°ì •
                        try:
                            max_len = max(df_data[col].astype(str).map(len).max(), len(str(col)))
                        except Exception:
                            max_len = 15
                        width = min(80, max(15, max_len + 2)) if col == 'AI ë¶„ì„ ì´ìœ ' else min(60, max(10, max_len + 2))
                        worksheet.set_column(i, i, width)

                        if col == 'AI ê´€ë ¨ì„± ì ìˆ˜':
                            score_col_idx = i

                    # ì¡°ê±´ë¶€ ì„œì‹ ì ìš©
                    if score_col_idx != -1:
                        col_letter = xlsxwriter.utility.xl_col_to_name(score_col_idx)
                        cell_range = f'{col_letter}2:{col_letter}{len(df_data) + 1}'
                        worksheet.conditional_format(cell_range, {
                            'type': '3_color_scale',
                            'min_value': 0, 'mid_value': 50, 'max_value': 100,
                            'min_color': "#F8696B", 'mid_color': "#FFEB84", 'max_color': "#63BE7B"
                        })

                else: 
                    # ì¼ë°˜ DF ì²˜ë¦¬
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False)
    except ImportError:
        logging.error("xlsxwriter module not found or initialization failed.")
        raise
    except Exception as e:
        logging.error(f"Error saving Excel file: {e}")
        raise

    return output.getvalue()

class NaraJangteoApiClient:
    # (API í´ë¼ì´ì–¸íŠ¸ êµ¬í˜„ì€ ì´ì „ ë²„ì „ê³¼ ë™ì¼)
    def __init__(self, service_key: str):
        if not service_key: raise ValueError("ì„œë¹„ìŠ¤ í‚¤ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        self.service_key = service_key
        self.base_url_std = "http://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
        self.base_url_plan = "http://apis.data.go.kr/1230000/ao/OrderPlanSttusService"

    def _make_request(self, base_url: str, endpoint: str, params: dict, log_list: list):
        try:
            url = f"{base_url}/{endpoint}?{urllib.parse.urlencode({'ServiceKey': self.service_key, 'pageNo': 1, 'numOfRows': 999, 'type': 'json', **params}, safe='/%')}"
            response = requests.get(url, timeout=90)
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            log_list.append(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹¤íŒ¨ ({endpoint}): {e}")
            return []
        except ValueError:
            log_list.append(f"âš ï¸ API ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ ({endpoint}): {response.text[:200]}...")
            return []

        response_data = data.get('response', {})
        header = response_data.get('header', {})
        result_code = header.get('resultCode', '00')

        if result_code != '00':
            log_list.append(f"âš ï¸ API ì˜¤ë¥˜ ({endpoint}): {header.get('resultMsg', 'ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ')}")
            return []
        
        body = response_data.get('body', {})
        if isinstance(body, list): return body
        if isinstance(body, dict):
            return body.get('items', [])
        return []

    def get_order_plans(self, year, log_list):
        years = [str(year)] if isinstance(year, (str, int)) else [str(y) for y in year]
        endpoints = {'ë¬¼í’ˆ': 'getOrderPlanSttusListThng', 'ìš©ì—­': 'getOrderPlanSttusListServc', 'ê³µì‚¬': 'getOrderPlanSttusListConst'}
        all_plans = []
        
        for current_year in years:
            params = {'year': current_year}
            log_list.append(f"[{current_year}ë…„ë„] ë°œì£¼ê³„íš ì¡°íšŒ ì‹œì‘...")
            year_plans_count = 0
            for category, endpoint in endpoints.items():
                log_list.append(f"  - ì¹´í…Œê³ ë¦¬: {category} ì¡°íšŒ ì¤‘...")
                plans = self._make_request(self.base_url_plan, endpoint, params, log_list)
                if plans:
                    for plan in plans:
                        plan['category'] = category
                        plan['plan_year'] = current_year
                    all_plans.extend(plans)
                    year_plans_count += len(plans)
            log_list.append(f"[{current_year}ë…„ë„] ì´ {year_plans_count}ê±´ ìˆ˜ì‹ .")
        
        log_list.append(f"ë°œì£¼ê³„íš ì „ì²´ ì´ {len(all_plans)}ê±´ ìˆ˜ì‹  ì™„ë£Œ.")
        return all_plans

    def get_pre_standard_specs(self, start_date, end_date, log_list):
        params = {'rgstBgnDt': start_date, 'rgstEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdPrdnmInfo", params, log_list)
    
    def get_bid_announcements(self, start_date, end_date, log_list):
        params = {'bidNtceBgnDt': start_date, 'bidNtceEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdBidPblancInfo", params, log_list)
    
    def get_successful_bid_info(self, start_date, end_date, log_list, bsns_div_cd):
        params = {'opengBgnDt': start_date, 'opengEndDt': end_date, 'bsnsDivCd': bsns_div_cd}
        return self._make_request(self.base_url_std, "getDataSetOpnStdScsbidInfo", params, log_list)
    
    def get_contract_info(self, start_date, end_date, log_list):
        params = {'cntrctCnclsBgnDate': start_date, 'cntrctCnclsEndDate': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdCntrctInfo", params, log_list)


def search_and_process(fetch_function, params, keywords, search_field, log_list, log_prefix=""):
    """API í˜¸ì¶œ ë° í‚¤ì›Œë“œ í•„í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
    if log_prefix != "ë°œì£¼ê³„íš":
        log_list.append(f"[{log_prefix}] ì¡°íšŒ ì‹œì‘...")
    
    raw_data = fetch_function(log_list=log_list, **params)
    
    if log_prefix != "ë°œì£¼ê³„íš":
        if not raw_data: 
            is_error = False
            if log_list:
                 is_error = log_list[-1].startswith("âš ï¸")
            if not is_error:
                log_list.append("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        log_list.append(f"ì´ {len(raw_data)}ê±´ ìˆ˜ì‹ .")

    if not raw_data: return pd.DataFrame()

    log_list.append(f"í‚¤ì›Œë“œ í•„í„°ë§ ì‹œì‘ (ê²€ìƒ‰ í•„ë“œ: {search_field})...")
    filtered_data = [
        item for item in raw_data 
        if isinstance(item, dict) and item.get(search_field) and 
        any(keyword.lower() in str(item[search_field]).lower() for keyword in keywords)
    ]
    log_list.append(f"í•„í„°ë§ í›„ {len(filtered_data)}ê±´ ë°œê²¬.")
    return pd.DataFrame(filtered_data)

# --- 3. ë°ì´í„°ë² ì´ìŠ¤ (ìˆ˜ì •ë¨) ---

def setup_database():
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    
    # projects í…Œì´ë¸” ìƒì„±
    cursor.execute("CREATE TABLE IF NOT EXISTS projects (bidNtceNo TEXT PRIMARY KEY, bidNtceNm TEXT, ntcelnsttNm TEXT, presmptPrce INTEGER, bid_status TEXT DEFAULT 'ê³µê³ ', bidNtceDate TEXT, sucsfCorpNm TEXT, cntrctAmt INTEGER, cntrctDate TEXT)")
    
    # order_plans í…Œì´ë¸” ìƒì„±
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS order_plans (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            plan_year TEXT, category TEXT, dminsttNm TEXT, prdctNm TEXT, asignBdgtAmt INTEGER, 
            orderInsttNm TEXT, orderPlanPrd TEXT, cntrctMthdNm TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderPlanPrd)
        )
    """)

    # ALTER TABLEë¡œ ê¸°ì¡´ ì»¬ëŸ¼ ë° ì‹ ê·œ ì»¬ëŸ¼ ì¶”ê°€
    def add_column_if_not_exists(table, column, definition):
        try:
            cols = [info[1] for info in cursor.execute(f"PRAGMA table_info({table})")]
            if column not in cols:
                cursor.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")
        except Exception as e:
            logging.warning(f"Error altering table {table}: {e}")

    # ê¸°ì¡´ ì»¬ëŸ¼
    add_column_if_not_exists("projects", "prestandard_status", "TEXT")
    add_column_if_not_exists("projects", "prestandard_no", "TEXT")
    add_column_if_not_exists("projects", "prestandard_date", "TEXT")
    
    # [ì‹ ê·œ] AI ê´€ë ¨ì„± ë° ìˆ˜ì§‘ ë°©ì‹ ì»¬ëŸ¼ ì¶”ê°€
    for table in ["projects", "order_plans"]:
        add_column_if_not_exists(table, "relevance_score", "INTEGER")
        add_column_if_not_exists(table, "relevance_reason", "TEXT")
        add_column_if_not_exists(table, "collection_method", "TEXT")

    conn.commit(); conn.close()

# [ìˆ˜ì •] AI ì ìˆ˜ ë° ìˆ˜ì§‘ ë°©ì‹ë„ í•¨ê»˜ ì €ì¥ (UPSERT ì‚¬ìš©)
def upsert_project_data(df, stage):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    
    for _, r in df.iterrows():
        try:
            if stage == 'bid':
                # ON CONFLICT DO UPDATE (UPSERT) ì‚¬ìš© (SQLite 3.24.0 ì´ìƒ í•„ìš”)
                cursor.execute("""
                    INSERT INTO projects 
                    (bidNtceNo, bidNtceNm, ntcelnsttNm, presmptPrce, bidNtceDate, prestandard_status, prestandard_no, prestandard_date, relevance_score, relevance_reason, collection_method) 
                    VALUES (?,?,?,?,?,?,?,?,?,?,?)
                    ON CONFLICT(bidNtceNo) DO UPDATE SET
                        bidNtceNm=excluded.bidNtceNm,
                        ntcelnsttNm=excluded.ntcelnsttNm,
                        presmptPrce=excluded.presmptPrce,
                        bidNtceDate=excluded.bidNtceDate,
                        prestandard_status=excluded.prestandard_status,
                        prestandard_no=excluded.prestandard_no,
                        prestandard_date=excluded.prestandard_date,
                        relevance_score=excluded.relevance_score,
                        relevance_reason=excluded.relevance_reason,
                        collection_method=excluded.collection_method
                """, (
                    r.get('bidNtceNo'), r.get('bidNtceNm'), r.get('ntcelnsttNm'), safe_int(r.get('presmptPrce')), r.get('bidNtceDate'),
                    r.get('prestandard_status'), r.get('prestandard_no'), r.get('prestandard_date'),
                    safe_int(r.get('relevance_score')), r.get('relevance_reason'), r.get('collection_method')
                ))
            elif stage == 'successful_bid':
                cursor.execute("""
                    UPDATE projects SET bid_status='ë‚™ì°°', sucsfCorpNm=? 
                    WHERE bidNtceNo=? AND (bid_status='ê³µê³ ' OR bid_status IS NULL OR bid_status='')
                """, (r.get('sucsfCorpNm'), r.get('bidNtceNo')))
            elif stage == 'contract':
                cursor.execute("""
                    UPDATE projects SET bid_status='ê³„ì•½ì™„ë£Œ', sucsfCorpNm=?, cntrctAmt=?, cntrctDate=? 
                    WHERE bidNtceNo=?
                """, (r.get('rprsntCorpNm'), safe_int(r.get('cntrctAmt')), r.get('cntrctCnclsDate'), r.get('bidNtceNo')))
        except Exception as e:
            logging.error(f"Error upserting project data (stage: {stage}): {e} - Data: {r.to_dict()}")
            continue

    conn.commit(); conn.close()

# [ìˆ˜ì •] AI ì ìˆ˜ ë° ìˆ˜ì§‘ ë°©ì‹ë„ í•¨ê»˜ ì €ì¥ (UPSERT ì‚¬ìš©)
def upsert_order_plan_data(df, log_list):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    
    for _, r in df.iterrows():
        try:
            # ON CONFLICT DO UPDATE (UPSERT) ì‚¬ìš©
            cursor.execute("""
                INSERT INTO order_plans 
                (plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderInsttNm, orderPlanPrd, cntrctMthdNm, relevance_score, relevance_reason, collection_method) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderPlanPrd) DO UPDATE SET
                    relevance_score=excluded.relevance_score,
                    relevance_reason=excluded.relevance_reason,
                    collection_method=excluded.collection_method,
                    created_at=CURRENT_TIMESTAMP -- ê°±ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸
            """, (
                r.get('plan_year'), r.get('category'), r.get('dminsttNm'), r.get('prdctNm'), safe_int(r.get('asignBdgtAmt')),
                r.get('orderInsttNm'), r.get('orderPlanPrd'), r.get('cntrctMthdNm'),
                safe_int(r.get('relevance_score')), r.get('relevance_reason'), r.get('collection_method')
            ))
        except Exception as e:
            log_list.append(f"âš ï¸ ê²½ê³ : ë°œì£¼ê³„íš DB ì‚½ì… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} - ë°ì´í„°: {r.to_dict()}")
            continue
            
    conn.commit()
    log_list.append(f"ë°œì£¼ê³„íš ì •ë³´ DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì²˜ë¦¬ëœ ë ˆì½”ë“œ: {len(df)}ê±´).")
    conn.close()


# --- 4. AI ë¶„ì„, ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ë³´ê³ ì„œ (ëŒ€í­ ìˆ˜ì •) ---

# [ì‹ ê·œ] AI ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ (í•µì‹¬ ë¡œì§)
def calculate_ai_relevance(api_key, df, data_type, log_list):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì˜ ê° í•­ëª©ì— ëŒ€í•œ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (Batch ì²˜ë¦¬)."""
    if df.empty:
        return df
    
    if not api_key:
         log_list.append("â„¹ï¸ Gemini API í‚¤ê°€ ì—†ì–´ AI ê´€ë ¨ì„± ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
         # ì ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
         if 'relevance_score' not in df.columns:
             df['relevance_score'] = None
             df['relevance_reason'] = 'API í‚¤ ì—†ìŒ'
         return df

    # ë°ì´í„° íƒ€ì…ë³„ í•„ë“œ ì •ì˜
    fields_map = {
        'order_plan': {'title': 'prdctNm', 'org': 'dminsttNm', 'budget': 'asignBdgtAmt', 'category': 'category'},
        'bid': {'title': 'bidNtceNm', 'org': 'ntcelnsttNm', 'budget': 'presmptPrce'}
    }
    
    if data_type not in fields_map:
        return df

    fields = fields_map[data_type]
    log_list.append(f"ğŸ¤– AI ê´€ë ¨ì„± ë¶„ì„ ì‹œì‘ ({data_type}, ì´ {len(df)}ê±´)...")

    # í”„ë¡¬í”„íŠ¸ ì •ì˜ (100ì  ë§Œì  ê¸°ì¤€)
    PROMPT_TEMPLATE = f"""ë‹¹ì‹ ì€ ì¡°ë‹¬ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ì˜ëœ [íšŒì‚¬ í”„ë¡œí•„]ì„ ë°”íƒ•ìœ¼ë¡œ, ì œê³µëœ ì¡°ë‹¬ ì‚¬ì—… ëª©ë¡ì´ ì´ íšŒì‚¬ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ í‰ê°€í•´ì£¼ì„¸ìš”. 

[íšŒì‚¬ í”„ë¡œí•„]
{COMPANY_PROFILE}

[í‰ê°€ ê¸°ì¤€ (100ì  ë§Œì )]
- 90~100ì : íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ (XR, ì‹œë®¬ë ˆì´ì…˜)ê³¼ ì£¼ë ¥ ë¶„ì•¼(ì‚¬ê²©, ê°•í•˜, ë°•ê²©í¬)ì— ì™„ë²½íˆ ë¶€í•©í•˜ëŠ” í›ˆë ¨ì²´ê³„ ì‚¬ì—….
- 70~89ì : ì£¼ë ¥ ë¶„ì•¼ëŠ” ì•„ë‹ˆì§€ë§Œ, íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìˆ˜í–‰ ê°€ëŠ¥í•œ ì‹œë®¬ë ˆì´í„°/ê°€ìƒí˜„ì‹¤ ê´€ë ¨ ì‚¬ì—… ë˜ëŠ” MRO.
- 50~69ì : ì‹œë®¬ë ˆì´ì…˜ ìš”ì†Œê°€ ì¼ë¶€ í¬í•¨ë˜ì–´ ìˆìœ¼ë‚˜, íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ê³¼ì˜ ì—°ê´€ì„±ì´ ë‹¤ì†Œ ë‚®ì€ ì‚¬ì—….
- 30~49ì : ë‹¨ìˆœ ì¥ë¹„ ë„ì… ë˜ëŠ” ì¼ë°˜ IT ìš©ì—­ ì‚¬ì—….
- 0~29ì : ì „í˜€ ê´€ë ¨ ì—†ëŠ” ì‚¬ì—….

[ì§€ì‹œ ì‚¬í•­]
1. ì•„ë˜ ì œê³µëœ ì‚¬ì—… ëª©ë¡(JSON í˜•ì‹)ì„ ë¶„ì„í•˜ì„¸ìš”.
2. ê° ì‚¬ì—…ë³„ë¡œ í‰ê°€ ì ìˆ˜(score)ì™€ ê°„ë‹¨í•œ ê·¼ê±°(reason)ë¥¼ ì œì‹œí•˜ì„¸ìš”.
3. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
   ì˜ˆì‹œ: [{{"index": 1, "score": 85, "reason": "XR ê¸°ìˆ  í™œìš© ê°€ëŠ¥í•œ í•´ì–‘ ì‹œë®¬ë ˆì´í„°"}}, ...]

[ì‚¬ì—… ëª©ë¡]
{{data_placeholder}}
"""

    try:
        genai.configure(api_key=api_key)
        # ì‘ë‹µ í˜•ì‹ì„ JSONìœ¼ë¡œ ê°•ì œ ì„¤ì •
        model = genai.GenerativeModel('gemini-1.5-flash', generation_config={"response_mime_type": "application/json"})
        
        results = []
        BATCH_SIZE = 30 # API í˜¸ì¶œ íš¨ìœ¨í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸°

        for i in range(0, len(df), BATCH_SIZE):
            batch_df = df.iloc[i:i+BATCH_SIZE]
            data_list = []
            
            # ë°°ì¹˜ ë°ì´í„°ë¥¼ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            for index, row in batch_df.iterrows():
                item = {
                    "index": index, # DataFrame ì¸ë±ìŠ¤ë¥¼ IDë¡œ ì‚¬ìš©
                    "title": row.get(fields['title'], 'N/A'),
                    "organization": row.get(fields['org'], 'N/A'),
                    "budget": format_price(row.get(fields['budget']))
                }
                if 'category' in fields:
                    item["category"] = row.get(fields['category'], 'N/A')
                data_list.append(item)

            data_str = json.dumps(data_list, ensure_ascii=False)
            prompt = PROMPT_TEMPLATE.replace("{{data_placeholder}}", data_str)
            
            # API í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
            try:
                response = model.generate_content(prompt)
                batch_results = json.loads(response.text)
                
                if isinstance(batch_results, list):
                    results.extend(batch_results)
                else:
                    raise ValueError("ì‘ë‹µì´ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹˜")
                    
                log_list.append(f"  - AI ë¶„ì„ ì§„í–‰ ì¤‘ ({min(i+BATCH_SIZE, len(df))}/{len(df)})...")
                time.sleep(0.5) # API Rate Limit ê³ ë ¤

            except (json.JSONDecodeError, ValueError, Exception) as e:
                log_list.append(f"âš ï¸ AI ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜ (ë°°ì¹˜ {i}~{i+BATCH_SIZE}): {e}. í•´ë‹¹ ë°°ì¹˜ëŠ” ìƒëµí•©ë‹ˆë‹¤.")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°°ì¹˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
                for index in batch_df.index:
                    results.append({"index": index, "score": -1, "reason": f"AI ë¶„ì„ ì˜¤ë¥˜: {e}"})

        # ê²°ê³¼ ë°ì´í„°ë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
        if results:
            valid_results = [r for r in results if isinstance(r, dict) and 'index' in r]
            if valid_results:
                results_df = pd.DataFrame(valid_results).set_index('index')
                results_df['score'] = pd.to_numeric(results_df['score'], errors='coerce').fillna(-1).astype(int)
                
                # ì›ë³¸ DFì— ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€ (ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©)
                df.loc[results_df.index, 'relevance_score'] = results_df['score']
                df.loc[results_df.index, 'relevance_reason'] = results_df['reason']

        log_list.append("âœ… AI ê´€ë ¨ì„± ë¶„ì„ ì™„ë£Œ.")
        return df

    except Exception as e:
        log_list.append(f"âš ï¸ AI ê´€ë ¨ì„± ë¶„ì„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.exception(e)
        return df

# (ì´ì „ ë²„ì „ê³¼ ë™ì¼ - ìƒëµëœ í•¨ìˆ˜ë“¤ ë³µì›)
def get_gemini_analysis(api_key, df, log_list):
    # (ì´ì „ ë²„ì „ ì½”ë“œ ì‚¬ìš©)
    if df.empty: log_list.append("AIê°€ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return None
    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì‹œì‘...")
        
        # í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ì „ ê¸ˆì•¡ í¬ë§·íŒ… ì ìš©
        df_for_prompt = df.copy()
        if 'presmptPrce' in df_for_prompt.columns:
             df_for_prompt['presmptPrce'] = df_for_prompt['presmptPrce'].apply(format_price)
        if 'cntrctAmt' in df_for_prompt.columns:
             df_for_prompt['cntrctAmt'] = df_for_prompt['cntrctAmt'].apply(format_price)
             
        data_for_prompt_str = df_for_prompt[[c for c in ['relevance_score', 'bidNtceNm', 'bid_status', 'ntcelnsttNm', 'presmptPrce', 'cntrctAmt'] if c in df_for_prompt.columns]].head(30).to_string()
        
        prompt = f"""ë‹¹ì‹ ì€ 'ìœ„ì¹˜ ë° ë™ì‘ ì¸ì‹ ê¸°ìˆ 'ì„ ê¸°ë°˜ìœ¼ë¡œ VR/MR/AR/XR ê°€ìƒí™˜ê²½ê³¼ í˜„ì‹¤ê³µê°„ì„ ì •ë°€í•˜ê²Œ ë§¤ì¹­(ê³µê°„ ì •í•©)í•˜ëŠ” ê¸°ìˆ ì„ ë³´ìœ í•œ, êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ 'ì‹ ì‚¬ì—… ì „ëµíŒ€ì¥'ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë‚˜ë¼ì¥í„° ì¡°ë‹¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë°ì´í„°ì˜ 'relevance_score'ëŠ” ìš°ë¦¬ íšŒì‚¬ì™€ì˜ ê´€ë ¨ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ (100ì  ë§Œì ). ì´ ì ìˆ˜ê°€ ë†’ì€ ì‚¬ì—…ì— ì§‘ì¤‘í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n[ë¶„ì„ ë°ì´í„°]\n{data_for_prompt_str}\n\n---\n## êµ°Â·ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ì—… ê¸°íšŒ ë¶„ì„ ë³´ê³ ì„œ\n\n### 1. ì´í‰ ë° í•µì‹¬ ë™í–¥\n(ìš°ë¦¬ íšŒì‚¬ì˜ XR ë° ê³µê°„ ì •í•© ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜, ê°€ìƒí˜„ì‹¤, MRO ì‚¬ì—…ì˜ ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.)\n\n### 2. ì£¼ìš” ì‚¬ì—… ì‹¬ì¸µ ë¶„ì„ (ê´€ë ¨ì„± ì ìˆ˜ ìƒìœ„ 3~5ê°œ)\n(ê´€ë ¨ì„± ì ìˆ˜(relevance_score)ê°€ ê°€ì¥ ë†’ì€ í”„ë¡œì íŠ¸ë¥¼ ì„ ì •í•˜ì—¬ ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. 'ë¶„ì„ ë° ì œì–¸' í•­ëª©ì—ëŠ” **ìš°ë¦¬ íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì¸ 'ê³µê°„ ì •í•©', 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹' ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì§€ì **ì„ ì¤‘ì ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.)\n\n| ì‚¬ì—…ëª… | ë°œì£¼ê¸°ê´€ | ê´€ë ¨ì„± ì ìˆ˜ | ì¶”ì •ê°€ê²©/ê³„ì•½ê¸ˆì•¡ | ë¶„ì„ ë° ì œì–¸ (ìì‚¬ ê¸°ìˆ  ì—°ê³„ ë°©ì•ˆ) |\n|---|---|---|---|---|\n| (ì‚¬ì—…ëª…) | (ê¸°ê´€ëª…) | (ì ìˆ˜) | (ê¸ˆì•¡) | (ì˜ˆ: ì´ ì‚¬ì—…ì€ CQB í›ˆë ¨ ì‹œë®¬ë ˆì´í„°ë¡œ, **í˜„ì‹¤ ê³µê°„ê³¼ ê°€ìƒ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •ë°€í•˜ê²Œ ë§¤ì¹­í•˜ëŠ” ìš°ë¦¬ ê¸°ìˆ **ì´ í•µì‹¬ ê²½ìŸë ¥ì´ ë  ìˆ˜ ìˆìŒ.) |\n\n### 3. ê¸°ìˆ  ì—°ê³„ ê°€ëŠ¥ í‚¤ì›Œë“œ\n(ë°ì´í„°ì—ì„œ ì‹ë³„ëœ í‚¤ì›Œë“œ ì¤‘, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ê³¼ ì—°ê²°ë  ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œ ì´ìƒ ì„ ì •í•˜ì—¬ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.)\n\n### 4. ì°¨ê¸° ì‚¬ì—… ì „ëµ ì œì–¸\n(ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ìš°ë¦¬ íšŒì‚¬ê°€ ë‹¤ìŒ ë¶„ê¸°ì— ì§‘ì¤‘í•´ì•¼ í•  ì‚¬ì—… ì˜ì—­, ê¸°ìˆ  ê³ ë„í™” ë°©í–¥ ë“±ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ì „ëµì„ 1~2ê°€ì§€ ì œì–¸í•´ì£¼ì„¸ìš”.)"""
        response = model.generate_content(prompt); log_list.append("Gemini ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì™„ë£Œ.")
        return response.text
    except Exception as e: 
        log_list.append(f"âš ï¸ Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def expand_keywords_with_gemini(api_key, df, existing_keywords, log_list):
    # (ì´ì „ ë²„ì „ ì½”ë“œ ì‚¬ìš©)
    if df.empty: log_list.append("í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return set()
    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ì§€ëŠ¥í˜• í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘...")
        project_titles = pd.concat([df[col] for col in ['bidNtceNm', 'cntrctNm', 'prdctClsfcNoNm', 'prdctNm'] if col in df.columns]).dropna().unique()
        project_titles_str = '\n- '.join(project_titles[:50])
        
        prompt = f"""ë‹¹ì‹ ì€ êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ ì¡°ë‹¬ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìš°ë¦¬ íšŒì‚¬ëŠ” 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹', 'XR ê³µê°„ ì •í•©' ê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” í˜„ì¬ ìš°ë¦¬ê°€ ì‚¬ìš©ì¤‘ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œì™€, ìµœê·¼ ì¡°ë‹¬ ì‹œìŠ¤í…œì—ì„œ ë°œê²¬ëœ ì‚¬ì—…ëª…/í’ˆëª… ëª©ë¡ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ê¸°ì¡´ í‚¤ì›Œë“œì— ì—†ëŠ” **ìƒˆë¡œìš´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ 5~10ê°œ ì¶”ì²œ**í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.\n\n[ê¸°ì¡´ í‚¤ì›Œë“œ]\n{', '.join(sorted(list(existing_keywords)))}\n\n[ìµœê·¼ ë°œê²¬ëœ ì‚¬ì—…ëª…/í’ˆëª…]\n- {project_titles_str}\n\n[ì¶”ì²œ í‚¤ì›Œë“œ]"""
        response = model.generate_content(prompt)
        new_keywords = {k.strip() for k in response.text.strip().split(',') if k.strip() and len(k.strip()) > 1}
        log_list.append(f"Geminiê°€ ì¶”ì²œí•œ ì‹ ê·œ í‚¤ì›Œë“œ: {new_keywords}")
        return new_keywords
    except Exception as e: 
        log_list.append(f"âš ï¸ Gemini í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return set()

def analyze_project_risk(df: pd.DataFrame) -> pd.DataFrame:
    # (ì´ì „ ë²„ì „ ì½”ë“œ ì‚¬ìš©)
    ongoing_df = df[df['bid_status'].isin(['ê³µê³ ', 'ë‚™ì°°'])].copy()
    if ongoing_df.empty: return pd.DataFrame()
    
    ongoing_df['score'] = 0
    ongoing_df['risk_reason'] = ''
    ongoing_df['bidNtceDate_dt'] = pd.to_datetime(ongoing_df['bidNtceDate'], errors='coerce')
    current_time = datetime.now()

    for index, row in ongoing_df.iterrows():
        reasons = []
        score = 0
        
        if pd.notna(row['bidNtceDate_dt']) and row['bid_status'] == 'ê³µê³ ':
            days_elapsed = (current_time - row['bidNtceDate_dt']).days
            if days_elapsed > 30:
                score += 5
                reasons.append(f'ê³µê³  í›„ {days_elapsed}ì¼ ê²½ê³¼')
        
        if row.get('prestandard_status') == 'í•´ë‹¹ ì—†ìŒ':
            score += 3
            reasons.append('ì‚¬ì „ê·œê²© ë¯¸ê³µê°œ')
        
        price = row.get('presmptPrce')
        if pd.notna(price) and isinstance(price, (int, float)) and 0 < price < 50000000:
            score += 2
            reasons.append('ì†Œê·œëª¨ ì‚¬ì—… (5ì²œë§Œì› ë¯¸ë§Œ)')
        
        ongoing_df.loc[index, 'score'] = score
        ongoing_df.loc[index, 'risk_reason'] = ', '.join(reasons)
    
    ongoing_df['risk_level'] = ongoing_df['score'].apply(lambda s: 'ë†’ìŒ' if s >= 7 else ('ë³´í†µ' if s >= 4 else 'ë‚®ìŒ'))
    
    risk_table = ongoing_df[['bidNtceNm', 'ntcelnsttNm', 'bid_status', 'risk_level', 'risk_reason']]
    return risk_table.rename(columns={'bidNtceNm':'ì‚¬ì—…ëª…','ntcelnsttNm':'ë°œì£¼ê¸°ê´€','bid_status':'ì§„í–‰ ìƒíƒœ','risk_level':'ë¦¬ìŠ¤í¬ ë“±ê¸‰','risk_reason':'ì£¼ìš” ë¦¬ìŠ¤í¬'}).sort_values(by='ë¦¬ìŠ¤í¬ ë“±ê¸‰',key=lambda x:x.map({'ë†’ìŒ':0,'ë³´í†µ':1,'ë‚®ìŒ':2}))


# [ìˆ˜ì •] AI ì ìˆ˜ ë°˜ì˜ ë° ì •ë ¬ ê¸°ì¤€ ë³€ê²½
def create_report_data(db_path, log_list, min_relevance_score=0):
    log_list.append(f"DBì—ì„œ ìµœì¢… ë°ì´í„° ì¡°íšŒ ì¤‘ (ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜: {min_relevance_score})...")
    conn = sqlite3.connect(db_path)
    report_data = {}

    try:
        # 1. í”„ë¡œì íŠ¸(ì…ì°°ê³µê³  ì´í›„) ë°ì´í„° ì²˜ë¦¬
        try:
            # [ìˆ˜ì •] ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§ ë° ê´€ë ¨ì„± ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            query = f"""
                SELECT * FROM projects 
                WHERE relevance_score >= ?
                ORDER BY relevance_score DESC, bidNtceDate DESC
            """
            all_projects_df = pd.read_sql_query(query, conn, params=(min_relevance_score,))
        except pd.errors.DatabaseError as e:
            log_list.append(f"âš ï¸ í”„ë¡œì íŠ¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            all_projects_df = pd.DataFrame()

        
        if not all_projects_df.empty:
            flat_df = all_projects_df.copy()
            
            # ë¶„ì„ìš© ì›ë³¸ ë°ì´í„° ì €ì¥ (ê¸ˆì•¡: ìˆ«ìí˜•)
            report_data["flat"] = flat_df.copy()

            # ë³´ê³ ì„œìš© ë°ì´í„° í¬ë§·íŒ…
            for col in ['prestandard_date','bidNtceDate','cntrctDate']:
                if col in flat_df.columns: 
                    flat_df[col] = pd.to_datetime(flat_df[col],errors='coerce').dt.strftime('%Y-%m-%d')
            
            for col in ['presmptPrce','cntrctAmt']:
                if col in flat_df.columns: 
                    flat_df[col] = flat_df[col].apply(format_price)

            # [ìˆ˜ì •] êµ¬ì¡°í™”ëœ ë°ì´í„°í”„ë ˆì„ (MultiIndex) ìƒì„± - AI ë¶„ì„ ê²°ê³¼ ë° íƒì§€ ë°©ì‹ ì¶”ê°€
            structured_columns = {
                ('AI ë¶„ì„ ê²°ê³¼','ê´€ë ¨ì„± ì ìˆ˜'):flat_df.get('relevance_score'),
                ('AI ë¶„ì„ ê²°ê³¼','ë¶„ì„ ì´ìœ '):flat_df.get('relevance_reason'),
                ('í”„ë¡œì íŠ¸ ê°œìš”','ì‚¬ì—…ëª…'):flat_df.get('bidNtceNm'), 
                ('í”„ë¡œì íŠ¸ ê°œìš”','ë°œì£¼ê¸°ê´€'):flat_df.get('ntcelnsttNm'), 
                ('ì§„í–‰ í˜„í™©','ì¢…í•© ìƒíƒœ'):flat_df.get('bid_status'), 
                ('ì§„í–‰ í˜„í™©','ë‚™ì°°/ê³„ì•½ì‚¬'):flat_df.get('sucsfCorpNm'), 
                ('ì…ì°° ê³µê³  ì •ë³´','ê³µê³ ì¼'):flat_df.get('bidNtceDate'), 
                ('ì…ì°° ê³µê³  ì •ë³´','ì¶”ì •ê°€ê²©'):flat_df.get('presmptPrce'), 
                ('ê³„ì•½ ì²´ê²° ì •ë³´','ê³„ì•½ê¸ˆì•¡'):flat_df.get('cntrctAmt'),
                ('ì°¸ì¡° ì •ë³´','íƒì§€ ë°©ì‹'):flat_df.get('collection_method'),
            }
            report_data["structured"] = pd.DataFrame(structured_columns)
            log_list.append("í”„ë¡œì íŠ¸ í˜„í™© ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ.")

        # 2. ë°œì£¼ê³„íš ë°ì´í„° ì²˜ë¦¬
        try:
            # [ìˆ˜ì •] ìµœì†Œ ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§ ë° ê´€ë ¨ì„± ì ìˆ˜/ì˜ˆì‚°ì•¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            # ì¤‘ë³µ ì œê±° ë¡œì§ í¬í•¨ (ìµœì‹  ë°ì´í„° ê¸°ì¤€)
            query_plan = f"""
                SELECT * FROM (
                    SELECT *, ROW_NUMBER() OVER(PARTITION BY plan_year, category, dminsttNm, prdctNm ORDER BY created_at DESC) as rn
                    FROM order_plans
                    WHERE relevance_score >= ?
                ) WHERE rn = 1
                ORDER BY relevance_score DESC, asignBdgtAmt DESC
            """
            all_order_plans_df = pd.read_sql_query(query_plan, conn, params=(min_relevance_score,))
        except pd.errors.DatabaseError as e:
             log_list.append(f"âš ï¸ ë°œì£¼ê³„íš DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
             all_order_plans_df = pd.DataFrame()

        if not all_order_plans_df.empty:
            order_plan_df = all_order_plans_df.copy()

            # ê¸ˆì•¡ í¬ë§·íŒ…
            order_plan_df['asignBdgtAmt_formatted'] = order_plan_df['asignBdgtAmt'].apply(format_price)
            
            # [ìˆ˜ì •] ë³´ê³ ì„œìš© ì»¬ëŸ¼ëª… ë³€ê²½ ë° ì„ íƒ - AI ì ìˆ˜ ë° ê·¼ê±° ì¶”ê°€
            order_plan_report_df = order_plan_df[[
                'relevance_score', 'relevance_reason', 'plan_year', 'category', 'dminsttNm', 'prdctNm', 'asignBdgtAmt_formatted', 'orderPlanPrd', 'collection_method'
            ]].rename(columns={
                'relevance_score': 'AI ê´€ë ¨ì„± ì ìˆ˜',
                'relevance_reason': 'AI ë¶„ì„ ì´ìœ ',
                'plan_year': 'ë…„ë„',
                'category': 'êµ¬ë¶„',
                'dminsttNm': 'ìˆ˜ìš”ê¸°ê´€ëª…',
                'prdctNm': 'í’ˆëª… (ì‚¬ì—…ëª…)',
                'asignBdgtAmt_formatted': 'ë°°ì •ì˜ˆì‚°ì•¡',
                'orderPlanPrd': 'ë°œì£¼ì˜ˆì •ì‹œê¸°',
                'collection_method': 'íƒì§€ ë°©ì‹'
            })
            
            report_data["order_plan"] = order_plan_report_df
            log_list.append("ë°œì£¼ê³„íš í˜„í™© ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ.")

        if not report_data:
             log_list.append(f"ì„¤ì •ëœ ì¡°ê±´(ì ìˆ˜ {min_relevance_score}ì  ì´ìƒ)ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
             return None
             
        return report_data

    except Exception as e: 
        log_list.append(f"âš ï¸ ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.exception(e)
        return None
    finally: 
        conn.close()

# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ë° íŒŒì´í”„ë¼ì¸ (ì „ë©´ ìˆ˜ì •) ---

# [ì‹ ê·œ] í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í—¬í¼ í•¨ìˆ˜
def collect_and_analyze(fetch_function, params, detailed_keywords, broad_keywords, search_field, gemini_key, log_list, log_prefix, data_type):
    """2ë‹¨ê³„(Keyword/AI Broad) ë°ì´í„° ìˆ˜ì§‘ ë° ê´€ë ¨ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    
    # 1. Detailed Keyword Search
    log_list.append(f"\n--- [{log_prefix}] 1ë‹¨ê³„: ìƒì„¸ í‚¤ì›Œë“œ íƒìƒ‰ ì‹œì‘ ---")
    df_keyword = search_and_process(fetch_function, params, detailed_keywords, search_field, log_list, log_prefix=f"{log_prefix} (Keyword)")
    
    if not df_keyword.empty:
        df_keyword['relevance_score'] = 100
        df_keyword['relevance_reason'] = 'ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­'
        df_keyword['collection_method'] = 'Keyword'

    # 2. Broad Keyword Search
    log_list.append(f"\n--- [{log_prefix}] 2ë‹¨ê³„: ê´‘ë²”ìœ„ í‚¤ì›Œë“œ íƒìƒ‰ ì‹œì‘ ---")
    df_broad = search_and_process(fetch_function, params, broad_keywords, search_field, log_list, log_prefix=f"{log_prefix} (Broad)")

    # 3. Identify Unique Broad Projects (1ë‹¨ê³„ì—ì„œ ë†“ì¹œ í”„ë¡œì íŠ¸)
    if df_broad.empty:
        return df_keyword

    # ê³ ìœ  ì‹ë³„ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    if data_type == 'bid' and 'bidNtceNo' in df_broad.columns:
        keyword_ids = set(df_keyword['bidNtceNo']) if not df_keyword.empty else set()
        df_unique_broad = df_broad[~df_broad['bidNtceNo'].isin(keyword_ids)].copy()
    else:
        # ì‹ë³„ìê°€ ì—†ìœ¼ë©´ ì‚¬ì—…ëª… ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ë°œì£¼ê³„íš ë“±)
        keyword_titles = set(df_keyword[search_field].str.strip().str.lower()) if not df_keyword.empty else set()
        df_unique_broad = df_broad[~df_broad[search_field].str.strip().str.lower().isin(keyword_titles)].copy()

    if df_unique_broad.empty:
        log_list.append("ê´‘ë²”ìœ„ íƒìƒ‰ ê²°ê³¼ëŠ” ëª¨ë‘ ìƒì„¸ í‚¤ì›Œë“œ íƒìƒ‰ì— í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤. AI ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return df_keyword

    # 4. AI Relevance Analysis
    log_list.append(f"\n--- [{log_prefix}] 3ë‹¨ê³„: AI ê´€ë ¨ì„± ë¶„ì„ ì‹œì‘ (ëŒ€ìƒ: {len(df_unique_broad)}ê±´) ---")
    
    # AI ë¶„ì„ ìˆ˜í–‰
    df_analyzed = calculate_ai_relevance(gemini_key, df_unique_broad, data_type, log_list)
    
    # AI ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ìˆ˜í–‰ëœ ê²½ìš°ì—ë§Œ collection_method ì„¤ì •
    if 'relevance_score' in df_analyzed.columns:
        df_analyzed['collection_method'] = 'AI_Broad'
    
    # 5. Combine Results (Keyword ë§¤ì¹­ ê²°ê³¼ + AI ë¶„ì„ ê²°ê³¼ ì „ì²´)
    # í•„í„°ë§ì€ ë³´ê³ ì„œ ìƒì„± ë‹¨ê³„ì—ì„œ ìˆ˜í–‰ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ëª¨ë“  ê²°ê³¼ë¥¼ í•©ì¹¨
    final_df = pd.concat([df_keyword, df_analyzed], ignore_index=True)
    
    # ì¤‘ë³µ ì œê±° (AI ë¶„ì„ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì¸í•´ ë‹¤ì‹œ ì¤‘ë³µì´ ë°œìƒí•  ê²½ìš° ëŒ€ë¹„)
    if data_type == 'bid' and 'bidNtceNo' in final_df.columns:
         final_df = final_df.drop_duplicates(subset=['bidNtceNo'])
    
    log_list.append(f"âœ… [{log_prefix}] ìµœì¢… ê²°ê³¼: ì´ {len(final_df)}ê±´ ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ.")
    return final_df


def run_analysis(search_keywords: set, client: NaraJangteoApiClient, gemini_key: str, start_date: date, end_date: date, auto_expand_keywords: bool = True, min_relevance_score: int = 60):
    log = []; all_found_data = {}
    
    # ì‹¤í–‰ ì‹œì  ê¸°ë¡ ë° ë‚ ì§œ ì„¤ì • (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
    execution_time = datetime.now() 
    fmt_date = '%Y%m%d'; fmt_datetime = '%Y%m%d%H%M'
    start_dt = datetime.combine(start_date, datetime.min.time())
    end_dt_limit = datetime.combine(end_date, datetime.max.time().replace(microsecond=0))
    end_dt = min(execution_time, end_dt_limit)

    start_date_str = start_dt.strftime(fmt_date); end_date_str = end_dt.strftime(fmt_date)
    start_datetime_str = start_dt.strftime(fmt_datetime); end_datetime_str = end_dt.strftime(fmt_datetime)

    log.append(f"ğŸ’¡ ë¶„ì„ ì‹œì‘: ê²€ìƒ‰ ê¸°ê°„ {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    if end_dt != end_dt_limit:
        log.append(f"â„¹ï¸ ì°¸ê³ : ì¢…ë£Œ ì‹œê°ì´ í˜„ì¬ ì‹œê°({end_dt.strftime('%H:%M')})ìœ¼ë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì ìš©) ---

    # 1. ë°œì£¼ê³„íš
    log.append("\n========== 1. ë°œì£¼ê³„íš ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ==========")
    target_years = list(range(start_date.year, end_date.year + 1))
    order_plan_params = {'year': target_years}
    
    all_found_data['order_plan'] = collect_and_analyze(
        client.get_order_plans, order_plan_params, search_keywords, BROAD_KEYWORDS, 'prdctNm', 
        gemini_key, log, log_prefix="ë°œì£¼ê³„íš", data_type="order_plan"
    )
    # DB ì €ì¥ (AI ì ìˆ˜ í¬í•¨)
    if not all_found_data['order_plan'].empty:
        upsert_order_plan_data(all_found_data['order_plan'], log)

    # 2. ì‚¬ì „ê·œê²© (ì°¸ì¡°ìš©, ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    log.append("\n========== 2. ì‚¬ì „ê·œê²© ì •ë³´ ìˆ˜ì§‘ (ì°¸ì¡°ìš©) ==========")
    pre_standard_params = {'start_date': start_date_str, 'end_date': end_date_str}
    all_found_data['pre_standard'] = search_and_process(
        client.get_pre_standard_specs, pre_standard_params, search_keywords, 'prdctClsfcNoNm', log,
        log_prefix=f"ì‚¬ì „ê·œê²©(Keyword)"
    )
    pre_standard_map = {
        r['bfSpecRgstNo']: r for _, r in all_found_data['pre_standard'].iterrows() 
        if r.get('bfSpecRgstNo')
    } if not all_found_data['pre_standard'].empty else {}
    
    # 3. ì…ì°°ê³µê³ 
    log.append("\n========== 3. ì…ì°° ê³µê³  ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ==========")
    bid_params = {'start_date': start_datetime_str, 'end_date': end_datetime_str}

    bid_df = collect_and_analyze(
        client.get_bid_announcements, bid_params, search_keywords, BROAD_KEYWORDS, 'bidNtceNm',
        gemini_key, log, log_prefix="ì…ì°°ê³µê³ ", data_type="bid"
    )
    
    # ì‚¬ì „ê·œê²© ì—°ê²°
    if not bid_df.empty:
        def link_pre_standard(row):
            spec_no = row.get('bfSpecRgstNo')
            if spec_no and spec_no in pre_standard_map:
                return ("í™•ì¸", spec_no, pre_standard_map[spec_no].get('registDt'))
            return ("í•´ë‹¹ ì—†ìŒ", None, None)
        bid_df[['prestandard_status', 'prestandard_no', 'prestandard_date']] = bid_df.apply(link_pre_standard, axis=1, result_type='expand')

    all_found_data['bid'] = bid_df
    # DB ì €ì¥ (AI ì ìˆ˜ í¬í•¨)
    upsert_project_data(bid_df, 'bid')

    # 4. ë‚™ì°°ì •ë³´ (ìƒíƒœ ì—…ë°ì´íŠ¸ìš©, ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    log.append("\n========== 4. ë‚™ì°° ì •ë³´ ìˆ˜ì§‘ ==========")
    succ_bid_base_params = {'start_date': start_datetime_str, 'end_date': end_datetime_str}
    succ_dfs = []
    for code in ['1','2','3','5']:
        params_with_code = {**succ_bid_base_params, 'bsns_div_cd': code}
        df = search_and_process(
            client.get_successful_bid_info, params_with_code, search_keywords, 'bidNtceNm', log,
            log_prefix=f"ë‚™ì°°ì •ë³´(ì½”ë“œ:{code})"
        )
        if not df.empty:
            succ_dfs.append(df)
            
    all_found_data['successful_bid'] = pd.concat(succ_dfs, ignore_index=True) if succ_dfs else pd.DataFrame()
    if not all_found_data['successful_bid'].empty: 
        upsert_project_data(all_found_data['successful_bid'], 'successful_bid')


    # 5. ê³„ì•½ì •ë³´ (ìƒíƒœ ì—…ë°ì´íŠ¸ìš©, ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    log.append("\n========== 5. ê³„ì•½ ì •ë³´ ìˆ˜ì§‘ ==========")
    contract_params = {'start_date': start_date_str, 'end_date': end_date_str}
    all_found_data['contract'] = search_and_process(
        client.get_contract_info, contract_params, search_keywords, 'cntrctNm', log,
        log_prefix=f"ê³„ì•½ì •ë³´"
    )
    if not all_found_data['contract'].empty: 
        upsert_project_data(all_found_data['contract'], 'contract')

    
    # --- ë³´ê³ ì„œ ìƒì„± ë° í›„ì²˜ë¦¬ ---
    log.append("\n========== 6. ë³´ê³ ì„œ ìƒì„± ë° ì „ëµ ë¶„ì„ ì‹œì‘ ==========")
    # [ìˆ˜ì •] ë³´ê³ ì„œ ìƒì„± ì‹œ ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜ ì „ë‹¬
    report_dfs = create_report_data("procurement_data.db", log, min_relevance_score)
    risk_df, report_data_bytes, gemini_report = pd.DataFrame(), None, None

    if report_dfs:
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        if "flat" in report_dfs and report_dfs["flat"] is not None:
             risk_df = analyze_project_risk(report_dfs["flat"])

        # ì—‘ì…€ ì‹œíŠ¸ êµ¬ì„±
        excel_sheets = {
            "ì¢…í•© í˜„í™© ë³´ê³ ì„œ": report_dfs.get("structured"),
            "ë°œì£¼ê³„íš í˜„í™©": report_dfs.get("order_plan"),
            "ë¦¬ìŠ¤í¬ ë¶„ì„": risk_df,
            # ì›ë³¸ ë°ì´í„°ëŠ” ìˆ˜ì§‘ëœ ì „ì²´ ë°ì´í„° ì œê³µ (ì°¸ê³ ìš©)
            "ë°œì£¼ê³„íš ì›ë³¸(ìˆ˜ì§‘ ì „ì²´)": all_found_data.get('order_plan'),
            "ì…ì°°ê³µê³  ì›ë³¸(ìˆ˜ì§‘ ì „ì²´)": all_found_data.get('bid'),
        }
        # ì—‘ì…€ íŒŒì¼ ìƒì„±
        try:
            report_data_bytes = save_integrated_excel(excel_sheets)
            log.append("âœ… í†µí•© ì—‘ì…€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            log.append(f"âš ï¸ ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # AI ì „ëµ ë¶„ì„ ë° í‚¤ì›Œë“œ í™•ì¥
    if gemini_key:
        if report_dfs and "flat" in report_dfs and report_dfs["flat"] is not None:
             # AI ì „ëµ ë¶„ì„ (ë³´ê³ ì„œ ë°ì´í„° ê¸°ì¤€)
            gemini_report = get_gemini_analysis(gemini_key, report_dfs["flat"], log)
        
        # í‚¤ì›Œë“œ í™•ì¥ (ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ìˆ˜í–‰)
        if auto_expand_keywords:
            combined_df_list = [df for df in all_found_data.values() if df is not None and not df.empty]
            if combined_df_list:
                combined_df = pd.concat(combined_df_list, ignore_index=True)
                new_keywords = expand_keywords_with_gemini(gemini_key, combined_df, search_keywords, log)
                if new_keywords:
                    updated_keywords = search_keywords.union(new_keywords)
                    save_keywords(updated_keywords)
                    log.append("ğŸ‰ í‚¤ì›Œë“œ íŒŒì¼ì´ ìƒˆë¡­ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

    
    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "log": log, 
        "risk_df": risk_df, 
        "report_file_data": report_data_bytes, 
        "report_filename": f"integrated_report_AI_{execution_time.strftime('%Y%m%d_%H%M%S')}.xlsx", 
        "gemini_report": gemini_report
    }