import os
import requests
import ssl
import requests.adapters
import urllib3
# [ì¶”ê°€] ì¬ì‹œë„ ë¡œì§ì„ ìœ„í•œ ì„í¬íŠ¸
from urllib3.util.retry import Retry

import pandas as pd
from datetime import datetime, timedelta, date
import urllib.parse
import time
import sqlite3
import google.generativeai as genai
import io
from itertools import groupby
import logging
import json
from typing import List, Union, Set, Tuple, Dict, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# --- 0. íšŒì‚¬ í”„ë¡œí•„ ë° ë¶„ì„ ì„¤ì • ---

# AIê°€ ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ì„ íšŒì‚¬ í”„ë¡œí•„ ì •ì˜ (ì£¼ìš” ê³ ê° ë° ë¶„ì•¼ ëª…ì‹œ ê°•í™”)
COMPANY_PROFILE = """
ìš°ë¦¬ íšŒì‚¬ëŠ” ê°€ìƒí˜„ì‹¤(VR/AR/XR) ë° ì‹œë®¬ë ˆì´ì…˜ ê¸°ìˆ ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” êµ°ì‚¬ ë° ê²½ì°° í›ˆë ¨ì²´ê³„ ì „ë¬¸ ê¸°ì—…ì…ë‹ˆë‹¤.
í•µì‹¬ ê¸°ìˆ :
1. ìœ„ì¹˜ ë° ë™ì‘ ì¸ì‹ ê¸°ìˆ  ê¸°ë°˜ì˜ ì •ë°€í•œ ê³µê°„ ì •í•© (ê°€ìƒí™˜ê²½ê³¼ í˜„ì‹¤ê³µê°„ ë§¤ì¹­)
2. ëª°ì…í˜• ê°€ìƒí™˜ê²½ êµ¬í˜„ ë° ì‹¤ì‹œê°„ ìƒí˜¸ì‘ìš©
ì£¼ìš” ì‚¬ì—… ë¶„ì•¼ ë° ê³ ê°:
- ì˜ìƒ ëª¨ì˜ ì‚¬ê²© í›ˆë ¨ ì‹œìŠ¤í…œ ë° ê³¼í•™í™” ì‚¬ê²©ì¥ êµ¬ì¶• (êµ°, ê²½ì°°, ì˜ˆë¹„êµ° í¬í•¨)
- ê°€ìƒ ê³µìˆ˜ ê°•í•˜(ë‚™í•˜ì‚°) í›ˆë ¨ ì‹œë®¬ë ˆì´í„°
- ë°•ê²©í¬/ì „ì°¨/í•­ê³µê¸° ë“± êµ°ì‚¬ ì¥ë¹„ ìš´ìš© ë° ì „ìˆ  í›ˆë ¨ ì‹œë®¬ë ˆì´í„°
- ì†Œë¶€ëŒ€ ì „íˆ¬(CQB) ë° ëŒ€í…ŒëŸ¬ í›ˆë ¨ ì‹œìŠ¤í…œ (ê²½ì°°íŠ¹ê³µëŒ€, íŠ¹ì „ì‚¬ ë“±)
- ì˜ˆë¹„êµ° ê³¼í•™í™” í›ˆë ¨ì²´ê³„ êµ¬ì¶• ë° ìœ ì§€ë³´ìˆ˜
- ì¥ë¹„ ìœ ì§€ë³´ìˆ˜(MRO)ë¥¼ ìœ„í•œ ê°€ìƒ/ì¦ê°•í˜„ì‹¤ ì†”ë£¨ì…˜
"""

# ê´‘ë²”ìœ„ íƒìƒ‰ìš© í‚¤ì›Œë“œ
BROAD_KEYWORDS = {"í›ˆë ¨", "ì²´ê³„", "ì‹œìŠ¤í…œ", "ëª¨ì˜", "ê°€ìƒ", "ì¦ê°•", "ì‹œë®¬ë ˆì´í„°", "ì‹œë®¬ë ˆì´ì…˜", "ê³¼í•™í™”", "êµìœ¡", "ì—°êµ¬ê°œë°œ", "ì„±ëŠ¥ê°œëŸ‰"}

# --- 1. ë³´ì•ˆ ë° ë„¤íŠ¸ì›Œí¬ ì„¤ì • í´ë˜ìŠ¤ ---

# [ìˆ˜ì •] ì¬ì‹œë„(Retry) ì „ëµì„ ì§€ì›í•˜ë„ë¡ CustomHttpAdapter ìˆ˜ì •
class CustomHttpAdapter(requests.adapters.HTTPAdapter):
    # retry_strategy ì¸ì ì¶”ê°€ ë° kwargs ì²˜ë¦¬
    def __init__(self, ssl_context=None, retry_strategy=None, **kwargs):
        self.ssl_context = ssl_context
        # retry_strategyê°€ ì œê³µë˜ë©´ max_retries ì„¤ì • (requestsê°€ ì¸ì‹í•˜ëŠ” ë°©ì‹)
        if retry_strategy:
            kwargs['max_retries'] = retry_strategy
        super().__init__(**kwargs)

    def init_poolmanager(self, connections, maxsize, block=False):
        # urllib3.PoolManager ì‚¬ìš©
        self.poolmanager = urllib3.PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=self.ssl_context
        )


# --- 1. í‚¤ì›Œë“œ ê´€ë¦¬ ---
KEYWORD_FILE = "keywords.txt"
# ìƒì„¸ í‚¤ì›Œë“œ
INITIAL_KEYWORDS = ["ì§€ë¢°","ë“œë¡ ","ì‹œë®¬ë ˆì´í„°","ì‹œë®¬ë ˆì´ì…˜","ì „ì°¨","ìœ ì§€ë³´ìˆ˜","MRO","í•­ê³µ","ê°€ìƒí˜„ì‹¤","ì¦ê°•í˜„ì‹¤","í›ˆë ¨","VR","AR","MR","XR","ëŒ€í…ŒëŸ¬","ì†Œë¶€ëŒ€","CQB","íŠ¹ì „ì‚¬","ê²½ì°°ì²­","ì˜ìƒì‚¬ê²©","ëª¨ì˜ì‚¬ê²©","ì˜ìƒ ëª¨ì˜","ê³µìˆ˜ê°•í•˜","ë°•ê²©í¬","ì˜ˆë¹„êµ°"]

def load_keywords(initial_keywords: list) -> set:
    if not os.path.exists(KEYWORD_FILE):
        save_keywords(set(initial_keywords))
        return set(initial_keywords)
    try:
        with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        logging.error(f"Error loading keywords: {e}")
        return set(initial_keywords)

def save_keywords(keywords: set):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            for keyword in sorted(list(keywords)):
                f.write(keyword + '\n')
    except Exception as e:
        logging.error(f"Error saving keywords: {e}")

# --- 2. ìœ í‹¸ë¦¬í‹° ë° API í´ë¼ì´ì–¸íŠ¸ ---

def safe_int(value):
    try:
        return int(float(value)) if value is not None and str(value).strip() != "" else None
    except (ValueError, TypeError):
        return None

def format_price(x):
    if pd.isna(x):
        return ""
    try:
        if isinstance(x, str):
             x = x.replace(',', '')
        if str(x).strip() == "":
             return ""
        return f"{int(float(x)):,}"
    except (ValueError, TypeError):
        return ""

def save_integrated_excel(data_frames: dict) -> bytes:
    """í†µí•© ì—‘ì…€ ì €ì¥. AI ì ìˆ˜ ì¡°ê±´ë¶€ ì„œì‹, ìë™ ì¤„ ë°”ê¿ˆ ë° í–‰ ë†’ì´ ìë™ ì¡°ì ˆ ê¸°ëŠ¥ ì¶”ê°€."""
    output = io.BytesIO()
    try:
        import xlsxwriter.utility

        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            workbook = writer.book

            # --- 1. ìŠ¤íƒ€ì¼ ì •ì˜ ---
            # í—¤ë” ìŠ¤íƒ€ì¼
            order_plan_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#FFF2CC', 'border': 1})
            main_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#D3D3D3', 'border': 1})
            ai_analysis_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#E2F0D9', 'border': 1})
            
            # ë°ì´í„° ì…€ ìŠ¤íƒ€ì¼ (í•µì‹¬: ìë™ ì¤„ ë°”ê¿ˆ ì ìš©)
            data_cell_format = workbook.add_format({
                'text_wrap': True,
                'valign': 'top',
                'border': 1
            })

            for sheet_name, df_data in data_frames.items():
                if df_data is None or df_data.empty: continue
                
                worksheet = workbook.add_worksheet(sheet_name)
                writer.sheets[sheet_name] = worksheet

                # --- 2. í—¤ë” ì‘ì„± ---
                is_multi_index = isinstance(df_data.columns, pd.MultiIndex) and df_data.columns.nlevels == 2
                
                if is_multi_index:
                    # MultiIndex í—¤ë” ì²˜ë¦¬ ('ì¢…í•© í˜„í™© ë³´ê³ ì„œ')
                    col_idx = 0
                    level0_headers = df_data.columns.get_level_values(0)
                    level1_headers = df_data.columns.get_level_values(1)

                    def get_header_format(header_name):
                        if header_name == 'AI ë¶„ì„ ê²°ê³¼': return ai_analysis_header_format
                        return main_header_format

                    for header, group in groupby(level0_headers):
                        span = len(list(group))
                        current_format = get_header_format(header)
                        if span > 1:
                            worksheet.merge_range(0, col_idx, 0, col_idx + span - 1, str(header), current_format)
                        else:
                            worksheet.write(0, col_idx, str(header), current_format)
                        for i in range(span):
                            worksheet.write(1, col_idx + i, str(level1_headers[col_idx + i]), current_format)
                        col_idx += span
                    start_row = 2 # ë°ì´í„° ì‹œì‘ í–‰
                else:
                    # ì¼ë°˜ í—¤ë” ì²˜ë¦¬ ('ë°œì£¼ê³„íš í˜„í™©' ë“±)
                    for col_num, value in enumerate(df_data.columns.values):
                        current_format = order_plan_header_format
                        if 'AI' in value: current_format = ai_analysis_header_format
                        worksheet.write(0, col_num, value, current_format)
                    start_row = 1 # ë°ì´í„° ì‹œì‘ í–‰

                # --- 3. ì»¬ëŸ¼ ë„ˆë¹„ ê³„ì‚° ë° ì„¤ì • ---
                col_widths = []
                data_for_calc = df_data.fillna("").values.tolist()
                headers = df_data.columns
                
                for i in range(len(headers)):
                    header_text = str(headers[i]) if not is_multi_index else f"{headers.get_level_values(0)[i]}\n{headers.get_level_values(1)[i]}"
                    header_len = max(len(s) for s in header_text.split('\n'))
                    
                    try:
                        data_max_len = max((len(str(row[i])) for row in data_for_calc), default=0)
                    except Exception:
                        data_max_len = 10
                        
                    # ìµœì¢… ë„ˆë¹„ ê²°ì • (ë¶„ì„ ì´ìœ ëŠ” ë” ë„“ê²Œ)
                    is_reason_col = (is_multi_index and headers.get_level_values(1)[i] == 'ë¶„ì„ ì´ìœ ') or \
                                    (not is_multi_index and headers[i] == 'AI ë¶„ì„ ì´ìœ ')
                                    
                    width = min(80, max(15, header_len, data_max_len) + 2) if is_reason_col else min(60, max(10, header_len, data_max_len) + 2)
                    
                    worksheet.set_column(i, i, width)
                    col_widths.append(width)

                # --- 4. ë°ì´í„° ì‘ì„± ë° í–‰ ë†’ì´ ìë™ ì¡°ì ˆ ---
                # to_excel ëŒ€ì‹  ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì“°ë©´ì„œ í–‰ ë†’ì´ë¥¼ ê³„ì‚°í•˜ê³  ì„œì‹ì„ ì ìš©
                for row_num, row_data in enumerate(data_for_calc, start=start_row):
                    max_lines = 1
                    for i, cell_text in enumerate(row_data):
                        if len(str(cell_text)) > 0 and col_widths[i] > 0:
                            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì¤„ ìˆ˜ ê³„ì‚° (ê¸€ì ìˆ˜ / (ì—´ ë„ˆë¹„ / ë³´ì •ê³„ìˆ˜))
                            # ë³´ì •ê³„ìˆ˜ 1.8ì€ í•œê¸€/ì˜ë¬¸ í˜¼ìš© í™˜ê²½ì—ì„œ í‰ê· ì ìœ¼ë¡œ ì˜ ë™ì‘í•˜ëŠ” ê°’
                            lines_needed = -(-len(str(cell_text)) // (col_widths[i] / 1.8))
                            if lines_needed > max_lines:
                                max_lines = lines_needed
                    
                    # í–‰ ë†’ì´ ì„¤ì • (ê¸°ë³¸ 15pt * ì¤„ ìˆ˜)
                    row_height = 15 * max_lines
                    worksheet.set_row(row_num, min(400, row_height)) # ìµœëŒ€ ë†’ì´ ì œí•œ
                    
                    # ì„œì‹ê³¼ í•¨ê»˜ ë°ì´í„° í–‰ ì‘ì„±
                    worksheet.write_row(row_num, 0, row_data, data_cell_format)

                # --- 5. ì¡°ê±´ë¶€ ì„œì‹ ì ìš© ---
                score_col_idx = -1
                score_col_name = 'ê´€ë ¨ì„± ì ìˆ˜' if is_multi_index else 'AI ê´€ë ¨ì„± ì ìˆ˜'
                cols = headers.get_level_values(1) if is_multi_index else headers
                
                try:
                    score_col_idx = list(cols).index(score_col_name)
                except ValueError:
                    score_col_idx = -1

                if score_col_idx != -1:
                    col_letter = xlsxwriter.utility.xl_col_to_name(score_col_idx)
                    cell_range = f'{col_letter}{start_row + 1}:{col_letter}{len(data_for_calc) + start_row}'
                    worksheet.conditional_format(cell_range, {
                        'type': '3_color_scale',
                        'min_value': 0, 'mid_value': 50, 'max_value': 100,
                        'min_color': "#F8696B", 'mid_color': "#FFEB84", 'max_color': "#63BE7B"
                    })

    except ImportError:
        logging.error("xlsxwriter module not found or initialization failed.")
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            for sheet_name, df_data in data_frames.items():
                 if df_data is not None and not df_data.empty:
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False)
        logging.info("Fallback to openpyxl engine for Excel export. Formatting will be limited.")
    except Exception as e:
        logging.error(f"Error saving Excel file: {e}")
        raise

    return output.getvalue()

# --- 2. ìˆ˜ì •ëœ NaraJangteoApiClient í´ë˜ìŠ¤ (ì•ˆì •ì„± ê°•í™”) ---
class NaraJangteoApiClient:
    # [ìˆ˜ì •] ì¬ì‹œë„ ë¡œì§ ì ìš©
    def __init__(self, service_key: str):
        if not service_key: raise ValueError("ì„œë¹„ìŠ¤ í‚¤ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        self.service_key = service_key
        self.base_url_std = "https://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
        self.base_url_plan = "https://apis.data.go.kr/1230000/ao/OrderPlanSttusService"

        # 1. ë³´ì•ˆ ì„¤ì • (SSL Context)
        ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
        # ì„œë²„ì™€ í˜¸í™˜ì„±ì„ ë†’ì´ê¸° ìœ„í•´ ì•”í˜¸í™” ë°©ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤. (ê³µê³µë°ì´í„°í¬í„¸ í˜¸í™˜ì„± ì´ìŠˆ ëŒ€ì‘)
        ctx.set_ciphers('ALL:@SECLEVEL=1')
        
        # 2. [ì‹ ê·œ] ìë™ ì¬ì‹œë„ ì „ëµ ì„¤ì • (HTTP 500 ë° ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ì‘)
        retry_strategy = Retry(
            total=3,  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
            status_forcelist=[500, 502, 503, 504], # ì´ ìƒíƒœ ì½”ë“œì—ì„œ ì¬ì‹œë„
            allowed_methods=["HEAD", "GET", "OPTIONS"], # GET ìš”ì²­ì— ëŒ€í•´ì„œë§Œ ì¬ì‹œë„
            backoff_factor=1 # ì¬ì‹œë„ ê°„ê²© (1ì´ˆ, 2ì´ˆ, 4ì´ˆ...)
        )

        # 3. ì„¸ì…˜ ì„¤ì • ë° ì–´ëŒ‘í„° ì—°ê²°
        self.session = requests.Session()
        
        # ë³´ì•ˆ ì„¤ì •(ctx)ê³¼ ì¬ì‹œë„ ì „ëµ(retry_strategy)ì„ ëª¨ë‘ ì ìš©í•œ ì–´ëŒ‘í„° ìƒì„±
        adapter = CustomHttpAdapter(ssl_context=ctx, retry_strategy=retry_strategy)
        
        self.session.mount("https://", adapter)


    # [í•µì‹¬ ìˆ˜ì •: ì˜¤ë¥˜ ì²˜ë¦¬ ìˆœì„œ êµì • ë° ë¡œê¹… ê°•í™”]
    def _make_request(self, base_url: str, endpoint: str, params: dict, log_list: list):
        
        decoded_key = "NOT_SET"
        response = None

        try:
            # 1. ì„œë¹„ìŠ¤ í‚¤ ë””ì½”ë”© ë° URL êµ¬ì„±
            try:
                decoded_key = urllib.parse.unquote(self.service_key)
            except Exception as e:
                log_list.append(f"âš ï¸ [ë‚´ë¶€ì˜¤ë¥˜] ({type(e).__name__}) ì„œë¹„ìŠ¤ í‚¤ ë””ì½”ë”© ì‹¤íŒ¨: {e}.")
                return []

            url = f"{base_url}/{endpoint}?ServiceKey={decoded_key}"

            # 2. ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„° ì¤€ë¹„
            other_params = {
                'pageNo': 1,
                'numOfRows': 999,
                'type': 'json',
                **params
            }
            
            # 3. ìš”ì²­ ìˆ˜í–‰
            response = self.session.get(url, params=other_params, timeout=90)
            
            # 4. HTTP ìƒíƒœ ì½”ë“œ í™•ì¸
            # ì¬ì‹œë„ ë¡œì§ì´ ëª¨ë‘ ì‹¤íŒ¨í•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ HTTPErrorê°€ ë°œìƒí•©ë‹ˆë‹¤.
            response.raise_for_status()

            # 5. ì‘ë‹µ ë³¸ë¬¸ ê²€ì¦
            content = response.text.strip()
            if not content:
                log_list.append(f"â„¹ï¸ [APIí†µì‹ ] ì‘ë‹µ ë³¸ë¬¸ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤ ({endpoint}). Status: {response.status_code}.")
                return []

            # 6. JSON íŒŒì‹± ì‹œë„
            # requests.exceptions.JSONDecodeError ë˜ëŠ” ValueError/json.JSONDecodeError ë°œìƒ ê°€ëŠ¥
            data = response.json()

        except requests.exceptions.SSLError as e:
            log_list.append(f"âš ï¸ [SSLì˜¤ë¥˜] ({type(e).__name__}) SSL í†µì‹  ì˜¤ë¥˜ ë°œìƒ ({endpoint}): {e}")
            return []
        
        # *** HTTP ì˜¤ë¥˜ ì²˜ë¦¬ ***
        except requests.exceptions.HTTPError as e:
            log_list.append(f"âš ï¸ [HTTPì˜¤ë¥˜] ({type(e).__name__}) ì„œë²„ ì—ëŸ¬ ë°œìƒ ({endpoint}): {e}. (ìë™ ì¬ì‹œë„ ì‹¤íŒ¨)")
            if response:
                log_list.append(f"  - ì„œë²„ ì‘ë‹µ ìƒ˜í”Œ (ì§„ë‹¨ìš©): {response.text[:1000]}")
                if response.status_code >= 500:
                     log_list.append("  ğŸ’¡ [ì§„ë‹¨] HTTP 500/50x ì—ëŸ¬ëŠ” ì„œë²„ ë‚´ë¶€ ë¬¸ì œì…ë‹ˆë‹¤. ìë™ ì¬ì‹œë„ë¥¼ ì‹œë„í–ˆìœ¼ë‚˜ ê³„ì† ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì„œë²„ ì•ˆì •í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return []

        # *** JSON ë””ì½”ë”© ì˜¤ë¥˜ ì²˜ë¦¬ (ìˆœì„œ êµì •) ***
        # [ì¤‘ìš”] requests.exceptions.JSONDecodeErrorëŠ” RequestExceptionì„ ìƒì†í•˜ë¯€ë¡œ, RequestExceptionë³´ë‹¤ ë¨¼ì € ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
        except (requests.exceptions.JSONDecodeError, json.JSONDecodeError, ValueError) as e:
            # ë¡œê·¸ì—ì„œ ë°œìƒí•œ "Expecting value..." ì˜¤ë¥˜ë¥¼ ì—¬ê¸°ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            log_list.append(f"âš ï¸ [JSONíŒŒì‹±ì˜¤ë¥˜] ({type(e).__name__}) API ì‘ë‹µ í˜•ì‹ì´ JSONì´ ì•„ë‹™ë‹ˆë‹¤ ({endpoint}). ì˜¤ë¥˜: {e}")
            if response:
                content_sample = response.text[:1000]
                log_list.append(f"  - ì„œë²„ ì‘ë‹µ ìƒ˜í”Œ (ì§„ë‹¨ìš©): {content_sample}")
                if "<" in content_sample and ">" in content_sample:
                    log_list.append("  ğŸ’¡ [ì§„ë‹¨] ì„œë²„ê°€ JSON ëŒ€ì‹  XML/HTML í˜•ì‹ì˜ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ì˜ ì¼ì‹œì  ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return []
        
        # *** ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì²˜ë¦¬ ***
        except requests.exceptions.RequestException as e:
            # ì§„ì§œ ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ì²˜ë¦¬ (ì—°ê²° ì‹¤íŒ¨, íƒ€ì„ì•„ì›ƒ ë“±)
            log_list.append(f"âš ï¸ [ë„¤íŠ¸ì›Œí¬ì˜¤ë¥˜] ({type(e).__name__}) ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ({endpoint}): {e}. (ìë™ ì¬ì‹œë„ ì‹¤íŒ¨)")
            return []
        
        except Exception as e:
            # ê¸°íƒ€ ì˜ˆìƒì¹˜ ëª»í•œ ëª¨ë“  ì˜¤ë¥˜ ì²˜ë¦¬
            log_list.append(f"âš ï¸ [ì˜ˆìƒì¹˜ëª»í•œì˜¤ë¥˜] ({type(e).__name__}) ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ ({endpoint}): {e}")
            return []


        # 7. ì„±ê³µì ì¸ JSON ì‘ë‹µ ì²˜ë¦¬ (API ë¡œì§)
        response_data = data.get('response', {})
        
        if not isinstance(response_data, dict):
             log_list.append(f"âš ï¸ [APIêµ¬ì¡°ì˜¤ë¥˜] ì˜ˆìƒì¹˜ ëª»í•œ API ì‘ë‹µ êµ¬ì¡°ì…ë‹ˆë‹¤ ({endpoint}). ì‘ë‹µ: {data}")
             return []

        header = response_data.get('header', {})
        result_code = header.get('resultCode', '00')

        if result_code != '00':
            log_list.append(f"âš ï¸ [APIë‚´ë¶€ì˜¤ë¥˜] API ì˜¤ë¥˜ ì½”ë“œ ìˆ˜ì‹  ({endpoint}) - ì½”ë“œ: {result_code}, ë©”ì‹œì§€: {header.get('resultMsg', 'ë©”ì‹œì§€ ì—†ìŒ')}")
            return []

        body = response_data.get('body', {})

        if isinstance(body, list): return body
        if isinstance(body, dict):
            if body.get('totalCount', 0) == 0:
                 return []
            items = body.get('items', [])
            if isinstance(items, list):
                return items
        
        log_list.append(f"âš ï¸ [APIêµ¬ì¡°ì˜¤ë¥˜] API ì‘ë‹µì˜ body í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤ ({endpoint}). Body: {body}")
        return []

    # (ë‚˜ë¨¸ì§€ API í˜¸ì¶œ í•¨ìˆ˜ë“¤ì€ ë³€ê²½ì‚¬í•­ ì—†ìŒ)
    def get_order_plans(self, year, log_list):
        years = [str(year)] if isinstance(year, (str, int)) else [str(y) for y in year]
        endpoints = {'ë¬¼í’ˆ': 'getOrderPlanSttusListThng', 'ìš©ì—­': 'getOrderPlanSttusListServc', 'ê³µì‚¬': 'getOrderPlanSttusListConst'}
        all_plans = []

        for current_year in years:
            params = {'year': current_year}
            log_list.append(f"[{current_year}ë…„ë„] ë°œì£¼ê³„íš ì¡°íšŒ ì‹œì‘...")
            year_plans_count = 0
            for category, endpoint in endpoints.items():
                log_list.append(f"  - ì¹´í…Œê³ ë¦¬: {category} ì¡°íšŒ ì¤‘...")
                plans = self._make_request(self.base_url_plan, endpoint, params, log_list)
                if plans:
                    for plan in plans:
                        plan['category'] = category
                        plan['plan_year'] = current_year
                    all_plans.extend(plans)
                    year_plans_count += len(plans)
            log_list.append(f"[{current_year}ë…„ë„] ì´ {year_plans_count}ê±´ ìˆ˜ì‹ .")

        log_list.append(f"ë°œì£¼ê³„íš ì „ì²´ ì´ {len(all_plans)}ê±´ ìˆ˜ì‹  ì™„ë£Œ.")
        return all_plans

    def get_pre_standard_specs(self, start_date, end_date, log_list):
        params = {'rgstBgnDt': start_date, 'rgstEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdPrdnmInfo", params, log_list)

    def get_bid_announcements(self, start_date, end_date, log_list):
        params = {'bidNtceBgnDt': start_date, 'bidNtceEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdBidPblancInfo", params, log_list)

    def get_successful_bid_info(self, start_date, end_date, log_list, bsns_div_cd):
        params = {'opengBgnDt': start_date, 'opengEndDt': end_date, 'bsnsDivCd': bsns_div_cd}
        return self._make_request(self.base_url_std, "getDataSetOpnStdScsbidInfo", params, log_list)

    def get_contract_info(self, start_date, end_date, log_list):
        params = {'cntrctCnclsBgnDate': start_date, 'cntrctCnclsEndDate': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdCntrctInfo", params, log_list)


# --- 3. ìµœì í™”ëœ ë°ì´í„° ì²˜ë¦¬ í•¨ìˆ˜ (API í˜¸ì¶œ ë‹¨ì¼í™”) ë¶€í„°ëŠ” ë³€ê²½ì‚¬í•­ ì—†ìŒ ---
# (ì´í•˜ ì½”ë“œëŠ” ì´ì „ ë‹µë³€ì˜ ì½”ë“œì™€ ë™ì¼í•©ë‹ˆë‹¤.)

def prepare_keywords(keywords: Set[str]) -> List[Tuple[str, str]]:
    """í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ì¤€ë¹„ (ì†Œë¬¸ìí™”, ë„ì–´ì“°ê¸° ì œê±° ë²„ì „)."""
    prepared = []
    for kw in keywords:
        kw_lower = kw.lower()
        kw_no_space = kw_lower.replace(" ", "")
        if kw_no_space:
             prepared.append((kw_lower, kw_no_space))
    return prepared

def fetch_api_data(fetch_function, params, log_list, log_prefix="") -> List[dict]:
    """API í˜¸ì¶œ ë° ê¸°ë³¸ ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤."""
    # ë°œì£¼ê³„íšì€ ì´ë¯¸ API í´ë¼ì´ì–¸íŠ¸ ë‚´ë¶€ì—ì„œ ìƒì„¸ ë¡œê·¸ë¥¼ ê¸°ë¡í•¨
    is_order_plan = log_prefix == "ë°œì£¼ê³„íš"

    if not is_order_plan:
        log_list.append(f"[{log_prefix}] API ë°ì´í„° ìš”ì²­ ì‹œì‘...")

    raw_data = fetch_function(log_list=log_list, **params)

    if not is_order_plan:
        if not raw_data:
            is_handled = False
            if log_list:
                 # ë§ˆì§€ë§‰ ë¡œê·¸ê°€ ì˜¤ë¥˜(âš ï¸), ì •ë³´(â„¹ï¸), ë˜ëŠ” ì§„ë‹¨(ğŸ’¡)ì¸ ê²½ìš° ì´ë¯¸ ì²˜ë¦¬ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                 is_handled = log_list[-1].startswith("âš ï¸") or log_list[-1].startswith("â„¹ï¸") or log_list[-1].startswith("ğŸ’¡")
            if not is_handled:
                # ëª…ì‹œì ì¸ ì˜¤ë¥˜ë‚˜ ì •ë³´ ë¡œê·¸ê°€ ì—†ëŠ”ë° ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
                log_list.append("APIì—ì„œ ìˆ˜ì‹ ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        log_list.append(f"APIë¡œë¶€í„° ì´ {len(raw_data)}ê±´ ìˆ˜ì‹ .")

    return raw_data if raw_data else []

def filter_data(data: List[dict], prepared_keywords: List[Tuple[str, str]], search_fields: List[str]) -> List[dict]:
    """ì¤€ë¹„ëœ í‚¤ì›Œë“œì™€ ë‹¤ì¤‘ í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤ (ë„ì–´ì“°ê¸° ë¬´ì‹œ í¬í•¨)."""
    filtered_data = []

    for item in data:
        if not isinstance(item, dict): continue

        match_found = False
        # ì§€ì •ëœ ëª¨ë“  ê²€ìƒ‰ í•„ë“œ(ì‚¬ì—…ëª…, ê¸°ê´€ëª… ë“±)ì— ëŒ€í•´ ë°˜ë³µ
        for field in search_fields:
            field_value = item.get(field)
            if not field_value: continue

            # ë¹„êµ ëŒ€ìƒ í…ìŠ¤íŠ¸ ì¤€ë¹„
            target_text = str(field_value).lower()
            target_text_no_space = target_text.replace(" ", "")

            # ì¤€ë¹„ëœ ëª¨ë“  í‚¤ì›Œë“œì™€ ë¹„êµ
            for kw_lower, kw_no_space in prepared_keywords:
                # 1. ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ë¹„êµ (ë„ì–´ì“°ê¸° ìœ ì§€)
                if kw_lower in target_text:
                    match_found = True
                    break
                # 2. ë„ì–´ì“°ê¸° ì œê±°ëœ í…ìŠ¤íŠ¸ì™€ ë¹„êµ
                if kw_no_space in target_text_no_space:
                     match_found = True
                     break
            if match_found:
                break

        if match_found:
            filtered_data.append(item)

    return filtered_data

# --- 4. ë°ì´í„°ë² ì´ìŠ¤ ---

def setup_database():
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()

    # projects í…Œì´ë¸” ìƒì„±
    cursor.execute("CREATE TABLE IF NOT EXISTS projects (bidNtceNo TEXT PRIMARY KEY, bidNtceNm TEXT, ntcelnsttNm TEXT, presmptPrce INTEGER, bid_status TEXT DEFAULT 'ê³µê³ ', bidNtceDate TEXT, sucsfCorpNm TEXT, cntrctAmt INTEGER, cntrctDate TEXT)")

    # order_plans í…Œì´ë¸” ìƒì„±
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS order_plans (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            plan_year TEXT, category TEXT, dminsttNm TEXT, prdctNm TEXT, asignBdgtAmt INTEGER,
            orderInsttNm TEXT, orderPlanPrd TEXT, cntrctMthdNm TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderPlanPrd)
        )
    """)

    # ALTER TABLEë¡œ ê¸°ì¡´ ì»¬ëŸ¼ ë° ì‹ ê·œ ì»¬ëŸ¼ ì¶”ê°€
    def add_column_if_not_exists(table, column, definition):
        try:
            cols = [info[1] for info in cursor.execute(f"PRAGMA table_info({table})")]
            if column not in cols:
                cursor.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")
        except Exception as e:
            logging.warning(f"Error altering table {table}: {e}")

    # ê¸°ì¡´ ì»¬ëŸ¼
    add_column_if_not_exists("projects", "prestandard_status", "TEXT")
    add_column_if_not_exists("projects", "prestandard_no", "TEXT")
    add_column_if_not_exists("projects", "prestandard_date", "TEXT")

    # AI ê´€ë ¨ì„± ë° ìˆ˜ì§‘ ë°©ì‹ ì»¬ëŸ¼ ì¶”ê°€
    for table in ["projects", "order_plans"]:
        add_column_if_not_exists(table, "relevance_score", "INTEGER")
        add_column_if_not_exists(table, "relevance_reason", "TEXT")
        add_column_if_not_exists(table, "collection_method", "TEXT")

    conn.commit(); conn.close()

def upsert_project_data(df, stage):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()

    for _, r in df.iterrows():
        try:
            if stage == 'bid':
                # ON CONFLICT DO UPDATE (UPSERT) ì‚¬ìš© (SQLite 3.24.0 ì´ìƒ í•„ìš”)
                cursor.execute("""
                    INSERT INTO projects
                    (bidNtceNo, bidNtceNm, ntcelnsttNm, presmptPrce, bidNtceDate, prestandard_status, prestandard_no, prestandard_date, relevance_score, relevance_reason, collection_method)
                    VALUES (?,?,?,?,?,?,?,?,?,?,?)
                    ON CONFLICT(bidNtceNo) DO UPDATE SET
                        bidNtceNm=excluded.bidNtceNm,
                        ntcelnsttNm=excluded.ntcelnsttNm,
                        presmptPrce=excluded.presmptPrce,
                        bidNtceDate=excluded.bidNtceDate,
                        prestandard_status=excluded.prestandard_status,
                        prestandard_no=excluded.prestandard_no,
                        prestandard_date=excluded.prestandard_date,
                        relevance_score=excluded.relevance_score,
                        relevance_reason=excluded.relevance_reason,
                        collection_method=excluded.collection_method
                """, (
                    r.get('bidNtceNo'), r.get('bidNtceNm'), r.get('ntcelnsttNm'), safe_int(r.get('presmptPrce')), r.get('bidNtceDate'),
                    r.get('prestandard_status'), r.get('prestandard_no'), r.get('prestandard_date'),
                    safe_int(r.get('relevance_score')), r.get('relevance_reason'), r.get('collection_method')
                ))
            elif stage == 'successful_bid':
                cursor.execute("""
                    UPDATE projects SET bid_status='ë‚™ì°°', sucsfCorpNm=?
                    WHERE bidNtceNo=? AND (bid_status='ê³µê³ ' OR bid_status IS NULL OR bid_status='')
                """, (r.get('sucsfCorpNm'), r.get('bidNtceNo')))
            elif stage == 'contract':
                cursor.execute("""
                    UPDATE projects SET bid_status='ê³„ì•½ì™„ë£Œ', sucsfCorpNm=?, cntrctAmt=?, cntrctDate=?
                    WHERE bidNtceNo=?
                """, (r.get('rprsntCorpNm'), safe_int(r.get('cntrctAmt')), r.get('cntrctCnclsDate'), r.get('bidNtceNo')))
        except Exception as e:
            logging.error(f"Error upserting project data (stage: {stage}): {e} - Data: {r.to_dict()}")
            continue

    conn.commit(); conn.close()

def upsert_order_plan_data(df, log_list):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()

    for _, r in df.iterrows():
        try:
            # ON CONFLICT DO UPDATE (UPSERT) ì‚¬ìš©
            cursor.execute("""
                INSERT INTO order_plans
                (plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderInsttNm, orderPlanPrd, cntrctMthdNm, relevance_score, relevance_reason, collection_method)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ON CONFLICT(plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderPlanPrd) DO UPDATE SET
                    relevance_score=excluded.relevance_score,
                    relevance_reason=excluded.relevance_reason,
                    collection_method=excluded.collection_method,
                    created_at=CURRENT_TIMESTAMP -- ê°±ì‹  ì‹œê°„ ì—…ë°ì´íŠ¸
            """, (
                r.get('plan_year'), r.get('category'), r.get('dminsttNm'), r.get('prdctNm'), safe_int(r.get('asignBdgtAmt')),
                r.get('orderInsttNm'), r.get('orderPlanPrd'), r.get('cntrctMthdNm'),
                safe_int(r.get('relevance_score')), r.get('relevance_reason'), r.get('collection_method')
            ))
        except Exception as e:
            log_list.append(f"âš ï¸ ê²½ê³ : ë°œì£¼ê³„íš DB ì‚½ì… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} - ë°ì´í„°: {r.to_dict()}")
            continue

    conn.commit()
    log_list.append(f"ë°œì£¼ê³„íš ì •ë³´ DB ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì²˜ë¦¬ëœ ë ˆì½”ë“œ: {len(df)}ê±´).")
    conn.close()


# --- 5. AI ë¶„ì„, ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ë³´ê³ ì„œ ---

# AI ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ - í‰ê°€ ê¸°ì¤€ ê°•í™”
def calculate_ai_relevance(api_key, df, data_type, log_list):
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (Batch ì²˜ë¦¬)."""
    if df.empty:
        return df

    if not api_key:
         log_list.append("â„¹ï¸ Gemini API í‚¤ê°€ ì—†ì–´ AI ê´€ë ¨ì„± ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
         if 'relevance_score' not in df.columns:
             df['relevance_score'] = None
             df['relevance_reason'] = 'API í‚¤ ì—†ìŒ'
         return df

    # ë°ì´í„° íƒ€ì…ë³„ í•„ë“œ ì •ì˜
    fields_map = {
        'order_plan': {'title': 'prdctNm', 'org': 'dminsttNm', 'budget': 'asignBdgtAmt', 'category': 'category'},
        'bid': {'title': 'bidNtceNm', 'org': 'ntcelnsttNm', 'budget': 'presmptPrce'}
    }

    if data_type not in fields_map:
        return df

    fields = fields_map[data_type]
    log_list.append(f"ğŸ¤– AI ê´€ë ¨ì„± ë¶„ì„ ì‹œì‘ ({data_type}, ì´ {len(df)}ê±´)...")

    # í”„ë¡¬í”„íŠ¸ ì •ì˜ - [ìˆ˜ì •] SyntaxError ë°©ì§€ë¥¼ ìœ„í•´ ì•ˆì •ì ì¸ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
    PROMPT_TEMPLATE = (
        "ë‹¹ì‹ ì€ ì¡°ë‹¬ ì •ë³´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì •ì˜ëœ [íšŒì‚¬ í”„ë¡œí•„]ì„ ë°”íƒ•ìœ¼ë¡œ, ì œê³µëœ ì¡°ë‹¬ ì‚¬ì—… ëª©ë¡ì´ ì´ íšŒì‚¬ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.\n\n"
        f"[íšŒì‚¬ í”„ë¡œí•„]\n{COMPANY_PROFILE}\n\n"
        "[í‰ê°€ ê¸°ì¤€ (100ì  ë§Œì ) - ì¤‘ìš”]\n"
        "- 90~100ì : íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ (XR, ì‹œë®¬ë ˆì´ì…˜)ê³¼ ì£¼ë ¥ ë¶„ì•¼(ì‚¬ê²©, ê°•í•˜, ë°•ê²©í¬, CQB, ì˜ˆë¹„êµ° í›ˆë ¨, ê²½ì°°íŠ¹ê³µëŒ€ í›ˆë ¨, ê³¼í•™í™” í›ˆë ¨ì¥)ì— ì™„ë²½íˆ ë¶€í•©í•˜ëŠ” í›ˆë ¨ì²´ê³„ ì‚¬ì—….\n"
        "- 70~89ì : ì£¼ë ¥ ë¶„ì•¼ëŠ” ì•„ë‹ˆì§€ë§Œ, íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìˆ˜í–‰ ê°€ëŠ¥í•œ ì‹œë®¬ë ˆì´í„°/ê°€ìƒí˜„ì‹¤ ê´€ë ¨ ì‚¬ì—… ë˜ëŠ” MRO. (ì˜ˆ: í•­ê³µ/í•¨ì • ì‹œë®¬ë ˆì´í„°)\n"
        "- 50~69ì : ì‹œë®¬ë ˆì´ì…˜ ìš”ì†Œê°€ ì¼ë¶€ í¬í•¨ë˜ì–´ ìˆìœ¼ë‚˜, íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ê³¼ì˜ ì—°ê´€ì„±ì´ ë‹¤ì†Œ ë‚®ì€ ì‚¬ì—…. (ì˜ˆ: ë‹¨ìˆœ êµìœ¡ ì—°êµ¬)\n"
        "- 30~49ì : ë‹¨ìˆœ ì¥ë¹„ ë„ì… ë˜ëŠ” ì¼ë°˜ IT ìš©ì—­ ì‚¬ì—….\n"
        "- 0~29ì : ì „í˜€ ê´€ë ¨ ì—†ëŠ” ì‚¬ì—….\n\n"
        "[ì§€ì‹œ ì‚¬í•­]\n"
        "1. ì•„ë˜ ì œê³µëœ ì‚¬ì—… ëª©ë¡(JSON í˜•ì‹)ì„ ë¶„ì„í•˜ì„¸ìš”.\n"
        "2. ê° ì‚¬ì—…ë³„ë¡œ í‰ê°€ ì ìˆ˜(score)ì™€ ê°„ë‹¨í•œ ê·¼ê±°(reason)ë¥¼ ì œì‹œí•˜ì„¸ìš”.\n"
        "3. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "   ì˜ˆì‹œ: [{\"index\": 1, \"score\": 85, \"reason\": \"XR ê¸°ìˆ  í™œìš© ê°€ëŠ¥í•œ í•´ì–‘ ì‹œë®¬ë ˆì´í„°\"}, ...]\n\n"
        "[ì‚¬ì—… ëª©ë¡]\n"
        "{data_placeholder}"
    )

    try:
        genai.configure(api_key=api_key)
        # ì‘ë‹µ í˜•ì‹ì„ JSONìœ¼ë¡œ ê°•ì œ ì„¤ì •
        model = genai.GenerativeModel('gemini-1.5-flash', generation_config={"response_mime_type": "application/json"})

        results = []
        BATCH_SIZE = 30 # API í˜¸ì¶œ íš¨ìœ¨í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ í¬ê¸°

        for i in range(0, len(df), BATCH_SIZE):
            batch_df = df.iloc[i:i+BATCH_SIZE]
            data_list = []

            # ë°°ì¹˜ ë°ì´í„°ë¥¼ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
            for index, row in batch_df.iterrows():
                item = {
                    "index": index,
                    "title": row.get(fields['title'], 'N/A'),
                    "organization": row.get(fields['org'], 'N/A'),
                    "budget": format_price(row.get(fields['budget']))
                }
                if 'category' in fields:
                    item["category"] = row.get(fields['category'], 'N/A')
                data_list.append(item)

            data_str = json.dumps(data_list, ensure_ascii=False)
            prompt = PROMPT_TEMPLATE.replace("{data_placeholder}", data_str)

            # API í˜¸ì¶œ ë° ì‘ë‹µ ì²˜ë¦¬
            try:
                response = model.generate_content(prompt)
                batch_results = json.loads(response.text)

                if isinstance(batch_results, list):
                    results.extend(batch_results)
                else:
                    raise ValueError("ì‘ë‹µì´ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì´ ì•„ë‹˜")

                log_list.append(f"  - AI ë¶„ì„ ì§„í–‰ ì¤‘ ({min(i+BATCH_SIZE, len(df))}/{len(df)})...")
                time.sleep(0.5) # API Rate Limit ê³ ë ¤

            except (json.JSONDecodeError, ValueError, Exception) as e:
                log_list.append(f"âš ï¸ AI ì‘ë‹µ ì²˜ë¦¬ ì˜¤ë¥˜ (ë°°ì¹˜ {i}~{i+BATCH_SIZE}): {e}. í•´ë‹¹ ë°°ì¹˜ëŠ” ìƒëµí•©ë‹ˆë‹¤.")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë°°ì¹˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
                for index in batch_df.index:
                    results.append({"index": index, "score": -1, "reason": f"AI ë¶„ì„ ì˜¤ë¥˜: {e}"})

        # ê²°ê³¼ ë°ì´í„°ë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ë³‘í•©
        if results:
            valid_results = [r for r in results if isinstance(r, dict) and 'index' in r]
            if valid_results:
                results_df = pd.DataFrame(valid_results).set_index('index')
                results_df['score'] = pd.to_numeric(results_df['score'], errors='coerce').fillna(-1).astype(int)

                # ì›ë³¸ DFì— ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€ (ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©)
                df.loc[results_df.index, 'relevance_score'] = results_df['score']
                df.loc[results_df.index, 'relevance_reason'] = results_df['reason']

        log_list.append("âœ… AI ê´€ë ¨ì„± ë¶„ì„ ì™„ë£Œ.")
        return df

    except Exception as e:
        log_list.append(f"âš ï¸ AI ê´€ë ¨ì„± ë¶„ì„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.exception(e)
        return df

def get_gemini_analysis(api_key, df, log_list):
    if df.empty: log_list.append("AIê°€ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return None

    if not api_key:
        log_list.append("â„¹ï¸ Gemini API í‚¤ê°€ ì—†ì–´ ì „ëµ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return None

    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì‹œì‘...")

        # í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ì „ ê¸ˆì•¡ í¬ë§·íŒ… ì ìš©
        df_for_prompt = df.copy()
        if 'presmptPrce' in df_for_prompt.columns:
             df_for_prompt['presmptPrce'] = df_for_prompt['presmptPrce'].apply(format_price)
        if 'cntrctAmt' in df_for_prompt.columns:
             df_for_prompt['cntrctAmt'] = df_for_prompt['cntrctAmt'].apply(format_price)

        data_for_prompt_str = df_for_prompt[[c for c in ['relevance_score', 'bidNtceNm', 'bid_status', 'ntcelnsttNm', 'presmptPrce', 'cntrctAmt'] if c in df_for_prompt.columns]].head(30).to_string()

        # [ìˆ˜ì •] SyntaxError ë°©ì§€ë¥¼ ìœ„í•´ ì•ˆì •ì ì¸ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
        prompt = (
            "ë‹¹ì‹ ì€ 'ìœ„ì¹˜ ë° ë™ì‘ ì¸ì‹ ê¸°ìˆ 'ì„ ê¸°ë°˜ìœ¼ë¡œ VR/MR/AR/XR ê°€ìƒí™˜ê²½ê³¼ í˜„ì‹¤ê³µê°„ì„ ì •ë°€í•˜ê²Œ ë§¤ì¹­(ê³µê°„ ì •í•©)í•˜ëŠ” ê¸°ìˆ ì„ ë³´ìœ í•œ, êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ 'ì‹ ì‚¬ì—… ì „ëµíŒ€ì¥'ì…ë‹ˆë‹¤. "
            "ì•„ë˜ ì œê³µëœ ë‚˜ë¼ì¥í„° ì¡°ë‹¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ë°ì´í„°ì˜ 'relevance_score'ëŠ” ìš°ë¦¬ íšŒì‚¬ì™€ì˜ ê´€ë ¨ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ (100ì  ë§Œì ). ì´ ì ìˆ˜ê°€ ë†’ì€ ì‚¬ì—…ì— ì§‘ì¤‘í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
            f"[ë¶„ì„ ë°ì´í„°]\n{data_for_prompt_str}\n\n---\n"
            "## êµ°Â·ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ì—… ê¸°íšŒ ë¶„ì„ ë³´ê³ ì„œ\n\n"
            "### 1. ì´í‰ ë° í•µì‹¬ ë™í–¥\n"
            "(ìš°ë¦¬ íšŒì‚¬ì˜ XR ë° ê³µê°„ ì •í•© ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜, ê°€ìƒí˜„ì‹¤, MRO ì‚¬ì—…ì˜ ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.)\n\n"
            "### 2. ì£¼ìš” ì‚¬ì—… ì‹¬ì¸µ ë¶„ì„ (ê´€ë ¨ì„± ì ìˆ˜ ìƒìœ„ 3~5ê°œ)\n"
            "(ê´€ë ¨ì„± ì ìˆ˜(relevance_score)ê°€ ê°€ì¥ ë†’ì€ í”„ë¡œì íŠ¸ë¥¼ ì„ ì •í•˜ì—¬ ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. 'ë¶„ì„ ë° ì œì–¸' í•­ëª©ì—ëŠ” **ìš°ë¦¬ íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì¸ 'ê³µê°„ ì •í•©', 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹' ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì§€ì **ì„ ì¤‘ì ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.)\n\n"
            "| ì‚¬ì—…ëª… | ë°œì£¼ê¸°ê´€ | ê´€ë ¨ì„± ì ìˆ˜ | ì¶”ì •ê°€ê²©/ê³„ì•½ê¸ˆì•¡ | ë¶„ì„ ë° ì œì–¸ (ìì‚¬ ê¸°ìˆ  ì—°ê³„ ë°©ì•ˆ) |\n"
            "|---|---|---|---|---|\n"
            "| (ì‚¬ì—…ëª…) | (ê¸°ê´€ëª…) | (ì ìˆ˜) | (ê¸ˆì•¡) | (ì˜ˆ: ì´ ì‚¬ì—…ì€ CQB í›ˆë ¨ ì‹œë®¬ë ˆì´í„°ë¡œ, **í˜„ì‹¤ ê³µê°„ê³¼ ê°€ìƒ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •ë°€í•˜ê²Œ ë§¤ì¹­í•˜ëŠ” ìš°ë¦¬ ê¸°ìˆ **ì´ í•µì‹¬ ê²½ìŸë ¥ì´ ë  ìˆ˜ ìˆìŒ.) |\n\n"
            "### 3. ê¸°ìˆ  ì—°ê³„ ê°€ëŠ¥ í‚¤ì›Œë“œ\n"
            "(ë°ì´í„°ì—ì„œ ì‹ë³„ëœ í‚¤ì›Œë“œ ì¤‘, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ê³¼ ì—°ê²°ë  ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œ ì´ìƒ ì„ ì •í•˜ì—¬ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.)\n\n"
            "### 4. ì°¨ê¸° ì‚¬ì—… ì „ëµ ì œì–¸\n"
            "(ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ìš°ë¦¬ íšŒì‚¬ê°€ ë‹¤ìŒ ë¶„ê¸°ì— ì§‘ì¤‘í•´ì•¼ í•  ì‚¬ì—… ì˜ì—­, ê¸°ìˆ  ê³ ë„í™” ë°©í–¥ ë“±ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ì „ëµì„ 1~2ê°€ì§€ ì œì–¸í•´ì£¼ì„¸ìš”.)"
        )
        response = model.generate_content(prompt); log_list.append("Gemini ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì™„ë£Œ.")
        return response.text
    except Exception as e:
        log_list.append(f"âš ï¸ Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def expand_keywords_with_gemini(api_key, df, existing_keywords, log_list):
    if df.empty: log_list.append("í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return set()

    if not api_key:
        log_list.append("â„¹ï¸ Gemini API í‚¤ê°€ ì—†ì–´ í‚¤ì›Œë“œ í™•ì¥ì„ ìƒëµí•©ë‹ˆë‹¤.")
        return set()

    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ì§€ëŠ¥í˜• í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘...")
        project_titles = pd.concat([df[col] for col in ['bidNtceNm', 'cntrctNm', 'prdctClsfcNoNm', 'prdctNm'] if col in df.columns]).dropna().unique()
        project_titles_str = '\n- '.join(project_titles[:50])

        # [ìˆ˜ì •] SyntaxError ë°©ì§€ë¥¼ ìœ„í•´ ì•ˆì •ì ì¸ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
        prompt = (
            "ë‹¹ì‹ ì€ êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ ì¡°ë‹¬ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìš°ë¦¬ íšŒì‚¬ëŠ” 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹', 'XR ê³µê°„ ì •í•©' ê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. "
            "ì•„ë˜ëŠ” í˜„ì¬ ìš°ë¦¬ê°€ ì‚¬ìš©ì¤‘ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œì™€, ìµœê·¼ ì¡°ë‹¬ ì‹œìŠ¤í…œì—ì„œ ë°œê²¬ëœ ì‚¬ì—…ëª…/í’ˆëª… ëª©ë¡ì…ë‹ˆë‹¤. "
            "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ê¸°ì¡´ í‚¤ì›Œë“œì— ì—†ëŠ” **ìƒˆë¡œìš´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ 5~10ê°œ ì¶”ì²œ**í•´ì£¼ì„¸ìš”. "
            "ê²°ê³¼ëŠ” ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.\n\n"
            f"[ê¸°ì¡´ í‚¤ì›Œë“œ]\n{', '.join(sorted(list(existing_keywords)))}\n\n"
            f"[ìµœê·¼ ë°œê²¬ëœ ì‚¬ì—…ëª…/í’ˆëª…]\n- {project_titles_str}\n\n"
            "[ì¶”ì²œ í‚¤ì›Œë“œ]"
        )
        response = model.generate_content(prompt)
        new_keywords = {k.strip() for k in response.text.strip().split(',') if k.strip() and len(k.strip()) > 1}
        log_list.append(f"Geminiê°€ ì¶”ì²œí•œ ì‹ ê·œ í‚¤ì›Œë“œ: {new_keywords}")
        return new_keywords
    except Exception as e:
        log_list.append(f"âš ï¸ Gemini í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return set()

def analyze_project_risk(df: pd.DataFrame) -> pd.DataFrame:
    # (ì´ì „ ë²„ì „ ì½”ë“œ ì‚¬ìš©)
    ongoing_df = df[df['bid_status'].isin(['ê³µê³ ', 'ë‚™ì°°'])].copy()
    if ongoing_df.empty: return pd.DataFrame()

    ongoing_df['score'] = 0
    ongoing_df['risk_reason'] = ''
    ongoing_df['bidNtceDate_dt'] = pd.to_datetime(ongoing_df['bidNtceDate'], errors='coerce')
    current_time = datetime.now()

    for index, row in ongoing_df.iterrows():
        reasons = []
        score = 0

        if pd.notna(row['bidNtceDate_dt']) and row['bid_status'] == 'ê³µê³ ':
            days_elapsed = (current_time - row['bidNtceDate_dt']).days
            if days_elapsed > 30:
                score += 5
                reasons.append(f'ê³µê³  í›„ {days_elapsed}ì¼ ê²½ê³¼')

        if row.get('prestandard_status') == 'í•´ë‹¹ ì—†ìŒ':
            score += 3
            reasons.append('ì‚¬ì „ê·œê²© ë¯¸ê³µê°œ')

        price = row.get('presmptPrce')
        if pd.notna(price) and isinstance(price, (int, float)) and 0 < price < 50000000:
            score += 2
            reasons.append('ì†Œê·œëª¨ ì‚¬ì—… (5ì²œë§Œì› ë¯¸ë§Œ)')

        ongoing_df.loc[index, 'score'] = score
        ongoing_df.loc[index, 'risk_reason'] = ', '.join(reasons)

    ongoing_df['risk_level'] = ongoing_df['score'].apply(lambda s: 'ë†’ìŒ' if s >= 7 else ('ë³´í†µ' if s >= 4 else 'ë‚®ìŒ'))

    risk_table = ongoing_df[['bidNtceNm', 'ntcelnsttNm', 'bid_status', 'risk_level', 'risk_reason']]
    return risk_table.rename(columns={'bidNtceNm':'ì‚¬ì—…ëª…','ntcelnsttNm':'ë°œì£¼ê¸°ê´€','bid_status':'ì§„í–‰ ìƒíƒœ','risk_level':'ë¦¬ìŠ¤í¬ ë“±ê¸‰','risk_reason':'ì£¼ìš” ë¦¬ìŠ¤í¬'}).sort_values(by='ë¦¬ìŠ¤í¬ ë“±ê¸‰',key=lambda x:x.map({'ë†’ìŒ':0,'ë³´í†µ':1,'ë‚®ìŒ':2}))


# ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
def create_report_data(db_path, log_list, min_relevance_score=0):
    log_list.append(f"DBì—ì„œ ìµœì¢… ë°ì´í„° ì¡°íšŒ ì¤‘ (ìµœì†Œ ê´€ë ¨ì„± ì ìˆ˜: {min_relevance_score})...")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor() # ì»¤ì„œ ìƒì„±
    report_data = {}

    try:
        # 1. í”„ë¡œì íŠ¸(ì…ì°°ê³µê³  ì´í›„) ë°ì´í„° ì²˜ë¦¬
        try:
            # í•„í„°ë§ ì „ ì „ì²´ í‰ê°€ ì™„ë£Œëœ ê°œìˆ˜ í™•ì¸ìš© ì¿¼ë¦¬ (AI ë¶„ì„ ì˜¤ë¥˜ ì œì™¸)
            total_count_query = "SELECT COUNT(*) FROM projects WHERE relevance_score IS NOT NULL AND relevance_score >= 0"
            cursor.execute(total_count_query)
            result = cursor.fetchone()
            total_projects_scored = result[0] if result else 0

            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¿¼ë¦¬ (í•„í„°ë§ ì ìš©)
            query = f"""
                SELECT * FROM projects
                WHERE relevance_score >= ?
                ORDER BY relevance_score DESC, bidNtceDate DESC
            """
            all_projects_df = pd.read_sql_query(query, conn, params=(min_relevance_score,))

            # ì§„ë‹¨ ë¡œê·¸ ì¶”ê°€
            log_list.append(f"ğŸ“Š DB í”„ë¡œì íŠ¸ í˜„í™©: ì´ {total_projects_scored}ê±´ í‰ê°€ë¨ -> {len(all_projects_df)}ê±´ì´ ì„ê³„ê°’({min_relevance_score}ì ) í†µê³¼.")

        except pd.errors.DatabaseError as e:
            log_list.append(f"âš ï¸ í”„ë¡œì íŠ¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            all_projects_df = pd.DataFrame()


        if not all_projects_df.empty:
            flat_df = all_projects_df.copy()

            # ë¶„ì„ìš© ì›ë³¸ ë°ì´í„° ì €ì¥ (ê¸ˆì•¡: ìˆ«ìí˜•)
            report_data["flat"] = flat_df.copy()

            # ë³´ê³ ì„œìš© ë°ì´í„° í¬ë§·íŒ…
            for col in ['prestandard_date','bidNtceDate','cntrctDate']:
                if col in flat_df.columns:
                    flat_df[col] = pd.to_datetime(flat_df[col],errors='coerce').dt.strftime('%Y-%m-%d')

            for col in ['presmptPrce','cntrctAmt']:
                if col in flat_df.columns:
                    flat_df[col] = flat_df[col].apply(format_price)

            # êµ¬ì¡°í™”ëœ ë°ì´í„°í”„ë ˆì„ (MultiIndex) ìƒì„± - AI ë¶„ì„ ê²°ê³¼ ë° íƒì§€ ë°©ì‹ ì¶”ê°€
            structured_columns = {
                ('AI ë¶„ì„ ê²°ê³¼','ê´€ë ¨ì„± ì ìˆ˜'):flat_df.get('relevance_score'),
                ('AI ë¶„ì„ ê²°ê³¼','ë¶„ì„ ì´ìœ '):flat_df.get('relevance_reason'),
                ('í”„ë¡œì íŠ¸ ê°œìš”','ì‚¬ì—…ëª…'):flat_df.get('bidNtceNm'),
                ('í”„ë¡œì íŠ¸ ê°œìš”','ë°œì£¼ê¸°ê´€'):flat_df.get('ntcelnsttNm'),
                ('ì§„í–‰ í˜„í™©','ì¢…í•© ìƒíƒœ'):flat_df.get('bid_status'),
                ('ì§„í–‰ í˜„í™©','ë‚™ì°°/ê³„ì•½ì‚¬'):flat_df.get('sucsfCorpNm'),
                ('ì…ì°° ê³µê³  ì •ë³´','ê³µê³ ì¼'):flat_df.get('bidNtceDate'),
                ('ì…ì°° ê³µê³  ì •ë³´','ì¶”ì •ê°€ê²©'):flat_df.get('presmptPrce'),
                ('ê³„ì•½ ì²´ê²° ì •ë³´','ê³„ì•½ê¸ˆì•¡'):flat_df.get('cntrctAmt'),
                ('ì°¸ì¡° ì •ë³´','íƒì§€ ë°©ì‹'):flat_df.get('collection_method'),
            }
            report_data["structured"] = pd.DataFrame(structured_columns)
            log_list.append("í”„ë¡œì íŠ¸ í˜„í™© ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ.")

        # 2. ë°œì£¼ê³„íš ë°ì´í„° ì²˜ë¦¬
        try:
            # í•„í„°ë§ ì „ ì „ì²´ í‰ê°€ ì™„ë£Œëœ ê°œìˆ˜ í™•ì¸ìš© ì¿¼ë¦¬ (ì¤‘ë³µ ì œê±° ê³ ë ¤, AI ë¶„ì„ ì˜¤ë¥˜ ì œì™¸)
            total_count_plan_query = """
                SELECT COUNT(*) FROM (
                    SELECT 1 FROM order_plans
                    WHERE relevance_score IS NOT NULL AND relevance_score >= 0
                    GROUP BY plan_year, category, dminsttNm, prdctNm
                )
            """
            cursor.execute(total_count_plan_query)
            result_plan = cursor.fetchone()
            total_plans_scored = result_plan[0] if result_plan else 0

            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¿¼ë¦¬ (í•„í„°ë§ ì ìš©)
            query_plan = f"""
                SELECT * FROM (
                    SELECT *, ROW_NUMBER() OVER(PARTITION BY plan_year, category, dminsttNm, prdctNm ORDER BY created_at DESC) as rn
                    FROM order_plans
                    WHERE relevance_score >= ?
                ) WHERE rn = 1
                ORDER BY relevance_score DESC, asignBdgtAmt DESC
            """
            all_order_plans_df = pd.read_sql_query(query_plan, conn, params=(min_relevance_score,))

            # ì§„ë‹¨ ë¡œê·¸ ì¶”ê°€
            log_list.append(f"ğŸ“Š DB ë°œì£¼ê³„íš í˜„í™©: ì´ {total_plans_scored}ê±´ í‰ê°€ë¨ -> {len(all_order_plans_df)}ê±´ì´ ì„ê³„ê°’({min_relevance_score}ì ) í†µê³¼.")

        except pd.errors.DatabaseError as e:
             log_list.append(f"âš ï¸ ë°œì£¼ê³„íš DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
             all_order_plans_df = pd.DataFrame()

        if not all_order_plans_df.empty:
            order_plan_df = all_order_plans_df.copy()

            # ê¸ˆì•¡ í¬ë§·íŒ…
            order_plan_df['asignBdgtAmt_formatted'] = order_plan_df['asignBdgtAmt'].apply(format_price)

            # ë³´ê³ ì„œìš© ì»¬ëŸ¼ëª… ë³€ê²½ ë° ì„ íƒ - AI ì ìˆ˜ ë° ê·¼ê±° ì¶”ê°€
            order_plan_report_df = order_plan_df[[
                'relevance_score', 'relevance_reason', 'plan_year', 'category', 'dminsttNm', 'prdctNm', 'asignBdgtAmt_formatted', 'orderPlanPrd', 'collection_method'
            ]].rename(columns={
                'relevance_score': 'AI ê´€ë ¨ì„± ì ìˆ˜',
                'relevance_reason': 'AI ë¶„ì„ ì´ìœ ',
                'plan_year': 'ë…„ë„',
                'category': 'êµ¬ë¶„',
                'dminsttNm': 'ìˆ˜ìš”ê¸°ê´€ëª…',
                'prdctNm': 'í’ˆëª… (ì‚¬ì—…ëª…)',
                'asignBdgtAmt_formatted': 'ë°°ì •ì˜ˆì‚°ì•¡',
                'orderPlanPrd': 'ë°œì£¼ì˜ˆì •ì‹œê¸°',
                'collection_method': 'íƒì§€ ë°©ì‹'
            })

            report_data["order_plan"] = order_plan_report_df
            log_list.append("ë°œì£¼ê³„íš í˜„í™© ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ.")

        if not report_data:
             # ì•ˆë‚´ ë©”ì‹œì§€ ê°•í™”
             log_list.append(f"âŒ ì„¤ì •ëœ ì¡°ê±´(ì ìˆ˜ {min_relevance_score}ì  ì´ìƒ)ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶”ê±°ë‚˜ ê¸°ê°„ì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
             return None

        return report_data

    except Exception as e:
        log_list.append(f"âš ï¸ ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.exception(e)
        return None
    finally:
        conn.close()

# --- 6. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ë° íŒŒì´í”„ë¼ì¸ (ìµœì í™” ì ìš©) ---

# í•˜ì´ë¸Œë¦¬ë“œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ í•¨ìˆ˜ (ìµœì í™” ì ìš©)
def collect_and_analyze(fetch_function, params, detailed_keywords: Set[str], broad_keywords: Set[str], search_fields: Union[str, List[str]], gemini_key, log_list, log_prefix, data_type):
    """ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (API í˜¸ì¶œ ë‹¨ì¼í™”)."""

    # search_fieldsë¥¼ í•­ìƒ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
    if isinstance(search_fields, str):
        search_fields_list = [search_fields]
    else:
        search_fields_list = search_fields

    # 1. ë°ì´í„° ìˆ˜ì§‘ (API í˜¸ì¶œ ë‹¨ 1íšŒ)
    log_list.append(f"\n--- [{log_prefix}] 1ë‹¨ê³„: í†µí•© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ---")

    # API í˜¸ì¶œ (ê¸°ê°„ ë‚´ ëª¨ë“  ë°ì´í„° í™•ë³´)
    raw_data = fetch_api_data(fetch_function, params, log_list, log_prefix)

    if not raw_data:
        return pd.DataFrame()

    # ê´‘ë²”ìœ„ í‚¤ì›Œë“œë¡œ 1ì°¨ í•„í„°ë§ (ê´€ë ¨ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ë°ì´í„°ë§Œ ì„ ë³„)
    log_list.append(f"ê´‘ë²”ìœ„ í‚¤ì›Œë“œ í•„í„°ë§ ì‹œì‘ (í•„ë“œ: {', '.join(search_fields_list)})...")
    prepared_broad_keywords = prepare_keywords(broad_keywords)
    broad_data = filter_data(raw_data, prepared_broad_keywords, search_fields_list)
    log_list.append(f"ê´‘ë²”ìœ„ í‚¤ì›Œë“œ í•„í„°ë§ í›„ {len(broad_data)}ê±´ í™•ë³´.")

    if not broad_data:
        return pd.DataFrame()

    # 2. ìƒì„¸ í‚¤ì›Œë“œ ë§¤ì¹­ (Score 100ì )
    log_list.append(f"\n--- [{log_prefix}] 2ë‹¨ê³„: ìƒì„¸ í‚¤ì›Œë“œ ë§¤ì¹­ (Score 100) ---")

    # ìƒì„¸ í‚¤ì›Œë“œ ì¤€ë¹„
    prepared_detailed_keywords = prepare_keywords(detailed_keywords)

    # í™•ë³´ëœ ë°ì´í„° ë‚´ì—ì„œ ìƒì„¸ í‚¤ì›Œë“œ ë§¤ì¹­ ìˆ˜í–‰
    keyword_hits_data = filter_data(broad_data, prepared_detailed_keywords, search_fields_list)
    log_list.append(f"ìƒì„¸ í‚¤ì›Œë“œ ë§¤ì¹­ ê²°ê³¼: {len(keyword_hits_data)}ê±´.")

    df_keyword = pd.DataFrame(keyword_hits_data)
    if not df_keyword.empty:
        df_keyword['relevance_score'] = 100
        df_keyword['relevance_reason'] = 'ì •í™•í•œ í‚¤ì›Œë“œ/ê¸°ê´€ëª… ë§¤ì¹­ (ê°•í™”ë¨)'
        df_keyword['collection_method'] = 'Keyword'

    # 3. AI ë¶„ì„ ëŒ€ìƒ ì„ ë³„ (ìƒì„¸ í‚¤ì›Œë“œ ë§¤ì¹­ ì œì™¸)
    # ë©”ëª¨ë¦¬ ì£¼ì†Œ(id)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ë³„ (ê°€ì¥ í™•ì‹¤í•˜ê³  ë¹ ë¦„)
    keyword_data_ids = {id(item) for item in keyword_hits_data}
    ai_candidate_data = [item for item in broad_data if id(item) not in keyword_data_ids]

    df_ai_candidates = pd.DataFrame(ai_candidate_data)

    # 4. AI ê´€ë ¨ì„± ë¶„ì„
    if df_ai_candidates.empty:
        log_list.append(f"\n--- [{log_prefix}] 3ë‹¨ê³„: AI ë¶„ì„ ---")
        log_list.append("ëª¨ë“  ë°ì´í„°ê°€ ìƒì„¸ í‚¤ì›Œë“œì— ë§¤ì¹­ë˜ì–´ AI ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤.")
        df_analyzed = pd.DataFrame()
    else:
        log_list.append(f"\n--- [{log_prefix}] 3ë‹¨ê³„: AI ê´€ë ¨ì„± ë¶„ì„ ì‹œì‘ (ëŒ€ìƒ: {len(df_ai_candidates)}ê±´) ---")
        # AI ë¶„ì„ ìˆ˜í–‰ (calculate_ai_relevance í•¨ìˆ˜ ì‚¬ìš©)
        df_analyzed = calculate_ai_relevance(gemini_key, df_ai_candidates, data_type, log_list)

        # collection_method ì„¤ì •
        if 'relevance_score' in df_analyzed.columns:
             # AI ë¶„ì„ ì˜¤ë¥˜(-1)ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ AI_Broadë¡œ í‘œì‹œ
            df_analyzed['collection_method'] = df_analyzed.apply(lambda row: 'AI_Broad' if row.get('relevance_score', -1) != -1 else 'Broad_Error', axis=1)

    # 5. ê²°ê³¼ í†µí•©
    final_df = pd.concat([df_keyword, df_analyzed], ignore_index=True)

    # ì¤‘ë³µ ì œê±° (ìµœì¢… í™•ì¸ - ê³ ìœ í‚¤ ì‚¬ìš©)
    if data_type == 'bid' and 'bidNtceNo' in final_df.columns:
         final_df = final_df.drop_duplicates(subset=['bidNtceNo'])

    log_list.append(f"âœ… [{log_prefix}] ìµœì¢… ê²°ê³¼: ì´ {len(final_df)}ê±´ ìˆ˜ì§‘ ë° ë¶„ì„ ì™„ë£Œ.")
    return final_df


# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì ìš©
def run_analysis(search_keywords: set, client: NaraJangteoApiClient, gemini_key: str, start_date: date, end_date: date, auto_expand_keywords: bool = True, min_relevance_score: int = 60):
    log = []; all_found_data = {}

    # ì‹¤í–‰ ì‹œì  ê¸°ë¡ ë° ë‚ ì§œ ì„¤ì •
    execution_time = datetime.now()
    fmt_date = '%Y%m%d'; fmt_datetime = '%Y%m%d%H%M'
    start_dt = datetime.combine(start_date, datetime.min.time())
    end_dt_limit = datetime.combine(end_date, datetime.max.time().replace(microsecond=0))
    end_dt = min(execution_time, end_dt_limit)

    start_date_str = start_dt.strftime(fmt_date); end_date_str = end_dt.strftime(fmt_date)
    start_datetime_str = start_dt.strftime(fmt_datetime); end_datetime_str = end_dt.strftime(fmt_datetime)

    log.append(f"ğŸ’¡ ë¶„ì„ ì‹œì‘: ê²€ìƒ‰ ê¸°ê°„ {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    if end_dt != end_dt_limit:
        # ì‹œê°„ í‘œê¸° ê°œì„  (KST ëª…ì‹œ ì‹œë„)
        try:
            import pytz
            kst = pytz.timezone('Asia/Seoul')
            current_time_kst = datetime.now(kst).strftime('%H:%M')
            time_info = f"(KST {current_time_kst})"
        except ImportError:
             # pytzê°€ ì—†ìœ¼ë©´ ì„œë²„ ì‹œê°„ ì‚¬ìš©
            current_time_kst = end_dt.strftime('%H:%M')
            time_info = f"({current_time_kst})"
        
        log.append(f"â„¹ï¸ ì°¸ê³ : ì¢…ë£Œ ì‹œê°ì´ í˜„ì¬ ì‹œê°{time_info} ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìµœì í™”ëœ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì ìš©) ---

    # 1. ë°œì£¼ê³„íš
    log.append("\n========== 1. ë°œì£¼ê³„íš ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ==========")
    # ê²€ìƒ‰ ê¸°ê°„ ì™¸ì— ë¯¸ë˜ë…„ë„(í˜„ì¬ ê¸°ì¤€ ë‹¤ìŒí•´)ê¹Œì§€ ì¡°íšŒí•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë¯€ë¡œ ë²”ìœ„ í™•ì¥ ê³ ë ¤
    current_year = datetime.now().year
    target_years = list(set(range(start_date.year, max(end_date.year, current_year + 1) + 1)))
    
    order_plan_params = {'year': target_years}

    # ê²€ìƒ‰ í•„ë“œ: í’ˆëª…(prdctNm) + ìˆ˜ìš”ê¸°ê´€ëª…(dminsttNm)
    all_found_data['order_plan'] = collect_and_analyze(
        client.get_order_plans, order_plan_params, search_keywords, BROAD_KEYWORDS,
        search_fields=['prdctNm', 'dminsttNm'],
        gemini_key=gemini_key, log_list=log, log_prefix="ë°œì£¼ê³„íš", data_type="order_plan"
    )
    # DB ì €ì¥
    if not all_found_data['order_plan'].empty:
        upsert_order_plan_data(all_found_data['order_plan'], log)

    # 2. ì‚¬ì „ê·œê²© (ì°¸ì¡°ìš©, ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    log.append("\n========== 2. ì‚¬ì „ê·œê²© ì •ë³´ ìˆ˜ì§‘ (ì°¸ì¡°ìš©) ==========")
    pre_standard_params = {'start_date': start_date_str, 'end_date': end_date_str}

    # API í˜¸ì¶œ
    pre_std_raw = fetch_api_data(client.get_pre_standard_specs, pre_standard_params, log, log_prefix="ì‚¬ì „ê·œê²©")
    # í•„í„°ë§ (ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    prepared_detailed = prepare_keywords(search_keywords)

    # pre_std_rawê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ê²½ìš°ì—ë§Œ í•„í„°ë§ ìˆ˜í–‰
    pre_std_filtered = []
    if pre_std_raw:
        # ì‚¬ì „ê·œê²©ì€ í’ˆëª…(prdctClsfcNoNm) ì™¸ì— ì‚¬ì—…ëª…(bsnsNm)ìœ¼ë¡œë„ ê²€ìƒ‰
        pre_std_filtered = filter_data(pre_std_raw, prepared_detailed, ['prdctClsfcNoNm', 'bsnsNm'])
        log.append(f"ì‚¬ì „ê·œê²© í•„í„°ë§ ì™„ë£Œ: {len(pre_std_filtered)}ê±´.")


    all_found_data['pre_standard'] = pd.DataFrame(pre_std_filtered)
    
    pre_standard_map = {
        r['bfSpecRgstNo']: r for r in pre_std_filtered
        if r.get('bfSpecRgstNo')
    }

    # 3. ì…ì°°ê³µê³ 
    log.append("\n========== 3. ì…ì°° ê³µê³  ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„ ==========")
    bid_params = {'start_date': start_datetime_str, 'end_date': end_datetime_str}

    # ê²€ìƒ‰ í•„ë“œ: ì…ì°°ê³µê³ ëª…(bidNtceNm) + ê³µê³ ê¸°ê´€ëª…(ntcelnsttNm)
    bid_df = collect_and_analyze(
        client.get_bid_announcements, bid_params, search_keywords, BROAD_KEYWORDS,
        search_fields=['bidNtceNm', 'ntcelnsttNm'],
        gemini_key=gemini_key, log_list=log, log_prefix="ì…ì°°ê³µê³ ", data_type="bid"
    )

    # ì‚¬ì „ê·œê²© ì—°ê²°
    if not bid_df.empty:
        def link_pre_standard(row):
            spec_no = row.get('bfSpecRgstNo')
            if spec_no and spec_no in pre_standard_map:
                return ("í™•ì¸", spec_no, pre_standard_map[spec_no].get('registDt'))
            return ("í•´ë‹¹ ì—†ìŒ", None, None)
        bid_df[['prestandard_status', 'prestandard_no', 'prestandard_date']] = bid_df.apply(link_pre_standard, axis=1, result_type='expand')

    all_found_data['bid'] = bid_df
    # DB ì €ì¥
    if not bid_df.empty:
        upsert_project_data(bid_df, 'bid')

    # 4. ë‚™ì°°ì •ë³´ (ìƒíƒœ ì—…ë°ì´íŠ¸ìš©, ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    log.append("\n========== 4. ë‚™ì°° ì •ë³´ ìˆ˜ì§‘ ==========")
    succ_bid_base_params = {'start_date': start_datetime_str, 'end_date': end_datetime_str}
    succ_dfs = []
    # prepared_detailed í‚¤ì›Œë“œ ì¬ì‚¬ìš©

    for code in ['1','2','3','5']:
        params_with_code = {**succ_bid_base_params, 'bsns_div_cd': code}

        # API í˜¸ì¶œ
        succ_raw = fetch_api_data(client.get_successful_bid_info, params_with_code, log, log_prefix=f"ë‚™ì°°ì •ë³´(ì½”ë“œ:{code})")
        
        # succ_rawê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ê²½ìš°ì—ë§Œ í•„í„°ë§ ìˆ˜í–‰
        succ_filtered = []
        if succ_raw:
            # í•„í„°ë§ (ìƒì„¸ í‚¤ì›Œë“œ, ë‹¤ì¤‘ í•„ë“œ)
            succ_filtered = filter_data(succ_raw, prepared_detailed, ['bidNtceNm', 'ntcelnsttNm'])

        if succ_filtered:
            succ_dfs.append(pd.DataFrame(succ_filtered))
            log.append(f"ë‚™ì°°ì •ë³´(ì½”ë“œ:{code}) í•„í„°ë§ ì™„ë£Œ: {len(succ_filtered)}ê±´.")

    all_found_data['successful_bid'] = pd.concat(succ_dfs, ignore_index=True) if succ_dfs else pd.DataFrame()
    if not all_found_data['successful_bid'].empty:
        upsert_project_data(all_found_data['successful_bid'], 'successful_bid')


    # 5. ê³„ì•½ì •ë³´ (ìƒíƒœ ì—…ë°ì´íŠ¸ìš©, ìƒì„¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
    log.append("\n========== 5. ê³„ì•½ ì •ë³´ ìˆ˜ì§‘ ==========")
    contract_params = {'start_date': start_date_str, 'end_date': end_date_str}

    # API í˜¸ì¶œ
    contract_raw = fetch_api_data(client.get_contract_info, contract_params, log, log_prefix="ê³„ì•½ì •ë³´")

    # contract_rawê°€ ë¹„ì–´ìˆì§€ ì•Šì„ ê²½ìš°ì—ë§Œ í•„í„°ë§ ìˆ˜í–‰
    contract_filtered = []
    if contract_raw:
        # í•„í„°ë§ (ìƒì„¸ í‚¤ì›Œë“œ, ë‹¤ì¤‘ í•„ë“œ)
        contract_filtered = filter_data(contract_raw, prepared_detailed, ['cntrctNm', 'dminsttNm'])
        log.append(f"ê³„ì•½ì •ë³´ í•„í„°ë§ ì™„ë£Œ: {len(contract_filtered)}ê±´.")


    all_found_data['contract'] = pd.DataFrame(contract_filtered)
    
    if not all_found_data['contract'].empty:
        upsert_project_data(all_found_data['contract'], 'contract')


    # --- ë³´ê³ ì„œ ìƒì„± ë° í›„ì²˜ë¦¬ ---
    log.append("\n========== 6. ë³´ê³ ì„œ ìƒì„± ë° ì „ëµ ë¶„ì„ ì‹œì‘ ==========")
    # ë³´ê³ ì„œ ìƒì„± (ì§„ë‹¨ ë¡œê·¸ê°€ í¬í•¨ëœ í•¨ìˆ˜ í˜¸ì¶œ)
    report_dfs = create_report_data("procurement_data.db", log, min_relevance_score)

    risk_df, report_data_bytes, gemini_report = pd.DataFrame(), None, None

    if report_dfs:
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        if "flat" in report_dfs and report_dfs["flat"] is not None:
             risk_df = analyze_project_risk(report_dfs["flat"])

        # ì—‘ì…€ ì‹œíŠ¸ êµ¬ì„±
        excel_sheets = {
            "ì¢…í•© í˜„í™© ë³´ê³ ì„œ": report_dfs.get("structured"),
            "ë°œì£¼ê³„íš í˜„í™©": report_dfs.get("order_plan"),
            "ë¦¬ìŠ¤í¬ ë¶„ì„": risk_df,
            # ì›ë³¸ ë°ì´í„°ëŠ” ìˆ˜ì§‘ëœ ì „ì²´ ë°ì´í„° ì œê³µ (ì°¸ê³ ìš©)
            "ë°œì£¼ê³„íš ì›ë³¸(ìˆ˜ì§‘ ì „ì²´)": all_found_data.get('order_plan'),
            "ì…ì°°ê³µê³  ì›ë³¸(ìˆ˜ì§‘ ì „ì²´)": all_found_data.get('bid'),
        }
        # ì—‘ì…€ íŒŒì¼ ìƒì„±
        try:
            report_data_bytes = save_integrated_excel(excel_sheets)
            log.append("âœ… í†µí•© ì—‘ì…€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            log.append(f"âš ï¸ ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # AI ì „ëµ ë¶„ì„ ë° í‚¤ì›Œë“œ í™•ì¥ (API í‚¤ ìœ ë¬´ í™•ì¸ì€ ê° í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
    if report_dfs and "flat" in report_dfs and report_dfs["flat"] is not None:
            # AI ì „ëµ ë¶„ì„ (ë³´ê³ ì„œ ë°ì´í„° ê¸°ì¤€)
        gemini_report = get_gemini_analysis(gemini_key, report_dfs["flat"], log)

    # í‚¤ì›Œë“œ í™•ì¥ (ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ìˆ˜í–‰)
    if auto_expand_keywords:
        combined_df_list = [df for df in all_found_data.values() if df is not None and not df.empty]
        if combined_df_list:
            combined_df = pd.concat(combined_df_list, ignore_index=True)
            new_keywords = expand_keywords_with_gemini(gemini_key, combined_df, search_keywords, log)
            if new_keywords:
                updated_keywords = search_keywords.union(new_keywords)
                save_keywords(updated_keywords)
                log.append("ğŸ‰ í‚¤ì›Œë“œ íŒŒì¼ì´ ìƒˆë¡­ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "log": log,
        "risk_df": risk_df,
        "report_file_data": report_data_bytes,
        "report_filename": f"integrated_report_AI_{execution_time.strftime('%Y%m%d_%H%M%S')}.xlsx",
        "gemini_report": gemini_report
    }