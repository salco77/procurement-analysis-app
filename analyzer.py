import os
import requests
import pandas as pd
# [ìˆ˜ì •] date íƒ€ìž… ížŒíŒ… ì¶”ê°€
from datetime import datetime, timedelta, date
import urllib.parse
import time
import sqlite3
import google.generativeai as genai
import io
from itertools import groupby
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# --- 1. í‚¤ì›Œë“œ ê´€ë¦¬ ---
KEYWORD_FILE = "keywords.txt"
INITIAL_KEYWORDS = ["ì§€ë¢°","ë“œë¡ ","ì‹œë®¬ë ˆì´í„°","ì‹œë®¬ë ˆì´ì…˜","ì „ì°¨","ìœ ì§€ë³´ìˆ˜","MRO","í•­ê³µ","ê°€ìƒí˜„ì‹¤","ì¦ê°•í˜„ì‹¤","í›ˆë ¨","VR","AR","MR","XR","ëŒ€í…ŒëŸ¬","ì†Œë¶€ëŒ€","CQB","íŠ¹ì „ì‚¬","ê²½ì°°ì²­"]

def load_keywords(initial_keywords: list) -> set:
    if not os.path.exists(KEYWORD_FILE):
        save_keywords(set(initial_keywords))
        return set(initial_keywords)
    try:
        with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        logging.error(f"Error loading keywords: {e}")
        return set(initial_keywords)

def save_keywords(keywords: set):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            for keyword in sorted(list(keywords)):
                f.write(keyword + '\n')
    except Exception as e:
        logging.error(f"Error saving keywords: {e}")

# --- 2. ìœ í‹¸ë¦¬í‹° ë° API í´ë¼ì´ì–¸íŠ¸ ---
def save_integrated_excel(data_frames: dict) -> bytes:
    """ì—¬ëŸ¬ ë°ì´í„°í”„ë ˆìž„ì„ í†µí•© ì—‘ì…€ë¡œ ì €ìž¥í•©ë‹ˆë‹¤. MultiIndex ë° ë°œì£¼ê³„íš ìŠ¤íƒ€ì¼ë§ ì§€ì›."""
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            workbook = writer.book

            # ë°œì£¼ê³„íš ì‹œíŠ¸ ìŠ¤íƒ€ì¼ ì •ì˜ (ì—°í•œ ë…¸ëž€ìƒ‰)
            order_plan_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#FFF2CC', 'border': 1})

            for sheet_name, df_data in data_frames.items():
                if df_data is None or df_data.empty: continue
                
                # MultiIndex ì²˜ë¦¬ ë¡œì§ (ì¢…í•© í˜„í™© ë³´ê³ ì„œ)
                if isinstance(df_data.columns, pd.MultiIndex) and df_data.columns.nlevels == 2:
                    worksheet = workbook.add_worksheet(sheet_name)
                    writer.sheets[sheet_name] = worksheet
                    header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#D3D3D3', 'border': 1})
                    
                    # 1. í—¤ë” ìž‘ì„± (ë³‘í•© í¬í•¨)
                    col_idx = 0
                    level0_headers = df_data.columns.get_level_values(0)
                    level1_headers = df_data.columns.get_level_values(1)

                    for header, group in groupby(level0_headers):
                        span = len(list(group))
                        if span > 1:
                            worksheet.merge_range(0, col_idx, 0, col_idx + span - 1, str(header), header_format)
                        else:
                            worksheet.write(0, col_idx, str(header), header_format)
                        for i in range(span):
                            worksheet.write(1, col_idx + i, str(level1_headers[col_idx + i]), header_format)
                        col_idx += span

                    # 2. ë°ì´í„° ìž‘ì„±
                    start_row = 2
                    def clean_data(x): return x if pd.notna(x) else ""
                    
                    # Pandas ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
                    try:
                        if hasattr(df_data, 'map'):
                             data_to_write = df_data.map(clean_data).values.tolist()
                        else:
                             # applymap (êµ¬ë²„ì „ í˜¸í™˜ì„±)
                             data_to_write = df_data.applymap(clean_data).values.tolist()
                    except Exception:
                        # Fallback
                        data_to_write = df_data.fillna("").values.tolist()

                    for row_data in data_to_write:
                        worksheet.write_row(start_row, 0, row_data)
                        start_row += 1
                    
                    # 3. ì»¬ëŸ¼ ë„ˆë¹„ ìžë™ ì¡°ì •
                    for i in range(len(df_data.columns)):
                        header_len = max(len(str(level0_headers[i])), len(str(level1_headers[i])))
                        try:
                            data_max_len = max((len(str(row[i])) for row in data_to_write), default=0)
                        except Exception:
                            data_max_len = 10
                        max_len = max(header_len, data_max_len)
                        worksheet.set_column(i, i, min(60, max(10, max_len + 2)))

                # ë°œì£¼ê³„íš í˜„í™© ì‹œíŠ¸ ì²˜ë¦¬ (ìŠ¤íƒ€ì¼ ì ìš©)
                elif sheet_name == "ë°œì£¼ê³„íš í˜„í™©":
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False, startrow=1, header=False)
                    worksheet = writer.sheets[sheet_name]
                    
                    # í—¤ë” ìž‘ì„± ë° ìŠ¤íƒ€ì¼ ì ìš©
                    for col_num, value in enumerate(df_data.columns.values):
                        worksheet.write(0, col_num, value, order_plan_header_format)
                    
                    # ì»¬ëŸ¼ ë„ˆë¹„ ìžë™ ì¡°ì •
                    for i, col in enumerate(df_data.columns):
                        try:
                            max_len = max(df_data[col].astype(str).map(len).max(), len(str(col)))
                        except Exception:
                            max_len = 15
                        worksheet.set_column(i, i, min(60, max(10, max_len + 2)))

                else: 
                    # ì¼ë°˜ DF ì²˜ë¦¬
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False)
    except Exception as e:
        logging.error(f"Error saving Excel file: {e}")
        raise

    return output.getvalue()


class NaraJangteoApiClient:
    def __init__(self, service_key: str):
        if not service_key: raise ValueError("ì„œë¹„ìŠ¤ í‚¤ëŠ” í•„ìˆ˜ìž…ë‹ˆë‹¤.")
        self.service_key = service_key
        self.base_url_std = "http://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
        self.base_url_plan = "http://apis.data.go.kr/1230000/ao/OrderPlanSttusService"

    def _make_request(self, base_url: str, endpoint: str, params: dict, log_list: list):
        try:
            url = f"{base_url}/{endpoint}?{urllib.parse.urlencode({'ServiceKey': self.service_key, 'pageNo': 1, 'numOfRows': 999, 'type': 'json', **params}, safe='/%')}"
            # íƒ€ìž„ì•„ì›ƒì„ 90ì´ˆë¡œ ì„¤ì •
            response = requests.get(url, timeout=90)
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            log_list.append(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹¤íŒ¨ ({endpoint}): {e}")
            return []
        except ValueError:
            log_list.append(f"âš ï¸ API ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ ({endpoint}): {response.text[:200]}...")
            return []

        response_data = data.get('response', {})
        header = response_data.get('header', {})
        if header.get('resultCode') != '00':
            log_list.append(f"âš ï¸ API ì˜¤ë¥˜ ({endpoint}): {header.get('resultMsg', 'ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ')}")
            return []
        
        body = response_data.get('body', {})
        if isinstance(body, list): return body
        if isinstance(body, dict):
            return body.get('items', [])
        return []

    # --- ë°œì£¼ê³„íší˜„í™©ì„œë¹„ìŠ¤ (OrderPlanSttusService) ---
    # [ìˆ˜ì •] years ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬ (year íŒŒë¼ë¯¸í„°ëª…ì€ ìœ ì§€í•˜ë˜ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€)
    def get_order_plans(self, year, log_list):
        # ë‹¨ì¼ ì—°ë„(str/int) ìž…ë ¥ ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        years = [str(year)] if isinstance(year, (str, int)) else [str(y) for y in year]

        endpoints = {
            'ë¬¼í’ˆ': 'getOrderPlanSttusListThng',
            'ìš©ì—­': 'getOrderPlanSttusListServc',
            'ê³µì‚¬': 'getOrderPlanSttusListConst'
        }
        all_plans = []
        
        # ì—°ë„ë³„ë¡œ ë°˜ë³µ í˜¸ì¶œ
        for current_year in years:
            params = {'year': current_year}
            log_list.append(f"[{current_year}ë…„ë„] ë°œì£¼ê³„íš ì¡°íšŒ ì‹œìž‘...")
            year_plans_count = 0
            for category, endpoint in endpoints.items():
                log_list.append(f"  - ì¹´í…Œê³ ë¦¬: {category} ì¡°íšŒ ì¤‘...")
                plans = self._make_request(self.base_url_plan, endpoint, params, log_list)
                if plans:
                    # ë°ì´í„°ì— ì¹´í…Œê³ ë¦¬ ë° ì—°ë„ ì •ë³´ ì¶”ê°€ (DB ì €ìž¥ ì‹œ í™œìš©)
                    for plan in plans:
                        plan['category'] = category
                        plan['plan_year'] = current_year
                    all_plans.extend(plans)
                    year_plans_count += len(plans)
            log_list.append(f"[{current_year}ë…„ë„] ì´ {year_plans_count}ê±´ ìˆ˜ì‹ .")
        
        log_list.append(f"ë°œì£¼ê³„íš ì „ì²´ ì´ {len(all_plans)}ê±´ ìˆ˜ì‹  ì™„ë£Œ.")
        return all_plans

    # --- ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤ (PubDataOpnStdService) ---
    def get_pre_standard_specs(self, start_date, end_date, log_list):
        params = {'rgstBgnDt': start_date, 'rgstEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdPrdnmInfo", params, log_list)
    
    def get_bid_announcements(self, start_date, end_date, log_list):
        params = {'bidNtceBgnDt': start_date, 'bidNtceEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdBidPblancInfo", params, log_list)
    
    def get_successful_bid_info(self, start_date, end_date, log_list, bsns_div_cd):
        params = {'opengBgnDt': start_date, 'opengEndDt': end_date, 'bsnsDivCd': bsns_div_cd}
        return self._make_request(self.base_url_std, "getDataSetOpnStdScsbidInfo", params, log_list)
    
    def get_contract_info(self, start_date, end_date, log_list):
        params = {'cntrctCnclsBgnDate': start_date, 'cntrctCnclsEndDate': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdCntrctInfo", params, log_list)


def search_and_process(fetch_function, params, keywords, search_field, log_list, log_prefix=""):
    """API í˜¸ì¶œ ë° í‚¤ì›Œë“œ í•„í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
    # ë°œì£¼ê³„íšì€ fetch_function ë‚´ë¶€ì—ì„œ ë¡œê·¸ë¥¼ ì‹œìž‘í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëžµ
    if log_prefix != "ë°œì£¼ê³„íš":
        log_list.append(f"[{log_prefix}] ì¡°íšŒ ì‹œìž‘...")
    
    # fetch_functionì— params ë”•ì…”ë„ˆë¦¬ë¥¼ ì–¸íŒ¨í‚¹í•˜ì—¬ ì „ë‹¬
    raw_data = fetch_function(log_list=log_list, **params)
    
    # ë°œì£¼ê³„íšì€ í•¨ìˆ˜ ë‚´ì—ì„œ ë¡œê·¸ë¥¼ ìƒì„¸ížˆ ê¸°ë¡í–ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€
    if log_prefix != "ë°œì£¼ê³„íš":
        if not raw_data: 
            # API/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜(âš ï¸)ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ì„ ê²½ìš°ì—ë§Œ "ë°ì´í„° ì—†ìŒ" ë¡œê·¸ ê¸°ë¡
            is_error = any("âš ï¸" in log for log in log_list[-2:]) # ìµœê·¼ 2ê°œ ë¡œê·¸ í™•ì¸
            if not is_error:
                log_list.append("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        log_list.append(f"ì´ {len(raw_data)}ê±´ ìˆ˜ì‹ .")

    if not raw_data: return pd.DataFrame()

    log_list.append(f"í‚¤ì›Œë“œ í•„í„°ë§ ì‹œìž‘ (ê²€ìƒ‰ í•„ë“œ: {search_field})...")
    # í‚¤ì›Œë“œ í•„í„°ë§ ë¡œì§
    filtered_data = [
        item for item in raw_data 
        if isinstance(item, dict) and item.get(search_field) and 
        # ê²€ìƒ‰ í•„ë“œ ê°’ì„ ë¬¸ìžì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        any(keyword.lower() in str(item[search_field]).lower() for keyword in keywords)
    ]
    log_list.append(f"í•„í„°ë§ í›„ {len(filtered_data)}ê±´ ë°œê²¬.")
    return pd.DataFrame(filtered_data)

# --- 3. ë°ì´í„°ë² ì´ìŠ¤ ---
# (setup_database, upsert_project_data í•¨ìˆ˜ëŠ” ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•˜ì—¬ ìƒëžµ)
# ì£¼ì˜: ì‹¤ì œ íŒŒì¼ì—ëŠ” í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

# [ìˆ˜ì •] year ì¸ìž ì œê±°, DataFrameì˜ 'plan_year' ì»¬ëŸ¼ ì‚¬ìš©
def upsert_order_plan_data(df, log_list):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    initial_count = conn.total_changes
    
    for _, r in df.iterrows():
        try:
            # ë°°ì •ì˜ˆì‚°ì•¡(asignBdgtAmt) ì •ìˆ˜í˜• ë³€í™˜ ì‹œë„
            try:
                budget = int(float(r.get('asignBdgtAmt'))) if r.get('asignBdgtAmt') else None
            except (ValueError, TypeError):
                budget = None

            # 'plan_year'ëŠ” API í˜¸ì¶œ ì‹œ ë°ì´í„°ì— ì¶”ê°€ë¨
            cursor.execute("""
                INSERT OR IGNORE INTO order_plans 
                (plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderInsttNm, orderPlanPrd, cntrctMthdNm) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                r.get('plan_year'), # API í˜¸ì¶œ ì‹œ ì¶”ê°€ëœ ì—°ë„ ì •ë³´ ì‚¬ìš©
                r.get('category'),
                r.get('dminsttNm'),
                r.get('prdctNm'),
                budget,
                r.get('orderInsttNm'),
                r.get('orderPlanPrd'),
                r.get('cntrctMthdNm')
            ))
        except Exception as e:
            log_list.append(f"âš ï¸ ê²½ê³ : ë°œì£¼ê³„íš DB ì‚½ìž… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} - ë°ì´í„°: {r.to_dict()}")
            # ê°œë³„ ë ˆì½”ë“œ ì˜¤ë¥˜ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
            continue
            
    conn.commit()
    new_records_count = conn.total_changes - initial_count
    log_list.append(f"ë°œì£¼ê³„íš ì •ë³´ DB ì €ìž¥ ì™„ë£Œ (ì‹ ê·œ {new_records_count}ê±´).")
    conn.close()


# --- 4. AI ë¶„ì„, ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ë³´ê³ ì„œ ---
# (format_price, get_gemini_analysis, expand_keywords_with_gemini, analyze_project_risk, create_report_data í•¨ìˆ˜ëŠ” ì´ì „ ë‹µë³€ê³¼ ë™ì¼í•˜ì—¬ ìƒëžµ)
# ì£¼ì˜: ì‹¤ì œ íŒŒì¼ì—ëŠ” í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.


# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì „ë©´ ìˆ˜ì •ë¨) ---
# [ìˆ˜ì •] start_date, end_dateë¥¼ ìž…ë ¥ë°›ë„ë¡ ë³€ê²½ (Streamlitì—ì„œ date ê°ì²´ë¡œ ì „ë‹¬ë¨)
def run_analysis(search_keywords: set, client: NaraJangteoApiClient, gemini_key: str, start_date: date, end_date: date, auto_expand_keywords: bool = True):
    log = []; all_found_data = {}
    
    # ì‹¤í–‰ ì‹œì  ê¸°ë¡
    execution_time = datetime.now() 

    # --- ë‚ ì§œ í˜•ì‹ ì •ì˜ ë° ë³€í™˜ ---
    fmt_date = '%Y%m%d'
    fmt_datetime = '%Y%m%d%H%M'

    # ì‹œìž‘ì¼ (00ì‹œ 00ë¶„) ì„¤ì •
    start_dt = datetime.combine(start_date, datetime.min.time())
    
    # ì¢…ë£Œì¼ (23ì‹œ 59ë¶„) ì„¤ì • ë° í˜„ìž¬ ì‹œê°„ê³¼ ë¹„êµ (APIëŠ” ë¯¸ëž˜ ë‚ ì§œ ì¡°íšŒ ë¶ˆê°€)
    end_dt_limit = datetime.combine(end_date, datetime.max.time().replace(microsecond=0))
    end_dt = min(execution_time, end_dt_limit)

    # í¬ë§·íŒ…ëœ ë¬¸ìžì—´ ìƒì„±
    start_date_str = start_dt.strftime(fmt_date)
    end_date_str = end_dt.strftime(fmt_date)
    start_datetime_str = start_dt.strftime(fmt_datetime)
    end_datetime_str = end_dt.strftime(fmt_datetime)

    log.append(f"ðŸ’¡ ë¶„ì„ ì‹œìž‘: ê²€ìƒ‰ ê¸°ê°„ {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    if end_dt != end_dt_limit:
        log.append(f"â„¹ï¸ ì°¸ê³ : ì¢…ë£Œ ì‹œê°ì´ í˜„ìž¬ ì‹œê°({end_dt.strftime('%H:%M')})ìœ¼ë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")


    # --- ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ---

    # 1. ë°œì£¼ê³„íš (ì—°ê°„)
    log.append("\n--- 1. ë°œì£¼ê³„íš ì •ë³´ ì¡°íšŒ ì‹œìž‘ ---")
    # [ìˆ˜ì •] ì‹œìž‘ ì—°ë„ë¶€í„° ì¢…ë£Œ ì—°ë„ê¹Œì§€ ëª¨ë“  ì—°ë„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    target_years = list(range(start_date.year, end_date.year + 1))
    
    # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° (yearì— ì—°ë„ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬ - Clientê°€ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì§€ì›í•˜ë„ë¡ ìˆ˜ì •ë¨)
    order_plan_params = {'year': target_years}
    
    # ë°ì´í„° ì¡°íšŒ ë° í•„í„°ë§
    all_found_data['order_plan'] = search_and_process(
        client.get_order_plans, order_plan_params, search_keywords, 'prdctNm', log, 
        log_prefix="ë°œì£¼ê³„íš"
    )
    # DB ì €ìž¥
    if not all_found_data['order_plan'].empty:
        # ê°œì„ ëœ upsert í•¨ìˆ˜ í˜¸ì¶œ (ë°ì´í„°í”„ë ˆìž„ ë‚´ 'plan_year' ì‚¬ìš©)
        upsert_order_plan_data(all_found_data['order_plan'], log)

    # 2. ì‚¬ì „ê·œê²© (ì§€ì • ê¸°ê°„)
    log.append("\n--- 2. ì‚¬ì „ê·œê²© ì •ë³´ ì¡°íšŒ ì‹œìž‘ ---")
    pre_standard_params = {
        'start_date': start_date_str, 
        'end_date': end_date_str
    }
    all_found_data['pre_standard'] = search_and_process(
        client.get_pre_standard_specs, pre_standard_params, search_keywords, 'prdctClsfcNoNm', log,
        log_prefix=f"ì‚¬ì „ê·œê²©({start_date_str}~{end_date_str})"
    )
    # ì‚¬ì „ê·œê²© ë²ˆí˜¸ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    pre_standard_map = {
        r['bfSpecRgstNo']: r for _, r in all_found_data['pre_standard'].iterrows() 
        if 'bfSpecRgstNo' in r and r['bfSpecRgstNo']
    } if not all_found_data['pre_standard'].empty else {}
    
    # 3. ìž…ì°°ê³µê³  (ì§€ì • ê¸°ê°„, ì‹œê°„ í¬í•¨)
    log.append("\n--- 3. ìž…ì°° ê³µê³  ì •ë³´ ì¡°íšŒ ì‹œìž‘ ---")
    bid_params = {
        'start_date': start_datetime_str, 
        'end_date': end_datetime_str
    }
    bid_df = search_and_process(
        client.get_bid_announcements, bid_params, search_keywords, 'bidNtceNm', log,
        log_prefix=f"ìž…ì°°ê³µê³ ({start_datetime_str}~{end_datetime_str})"
    )
    
    # ìž…ì°°ê³µê³ ì™€ ì‚¬ì „ê·œê²© ì—°ê²°
    if not bid_df.empty:
        def link_pre_standard(row):
            spec_no = row.get('bfSpecRgstNo')
            if spec_no and spec_no in pre_standard_map:
                return ("í™•ì¸", spec_no, pre_standard_map[spec_no].get('registDt'))
            return ("í•´ë‹¹ ì—†ìŒ", None, None)
        bid_df[['prestandard_status', 'prestandard_no', 'prestandard_date']] = bid_df.apply(link_pre_standard, axis=1, result_type='expand')
    
    all_found_data['bid'] = bid_df
    upsert_project_data(bid_df, 'bid')

    # 4. ë‚™ì°°ì •ë³´ (ì§€ì • ê¸°ê°„, ì‹œê°„ í¬í•¨)
    log.append("\n--- 4. ë‚™ì°° ì •ë³´ ì¡°íšŒ ì‹œìž‘ ---")
    succ_bid_base_params = {
        'start_date': start_datetime_str, 
        'end_date': end_datetime_str
    }
    succ_dfs = []
    # ì—…ë¬´ êµ¬ë¶„ ì½”ë“œë³„ ì¡°íšŒ
    for code in ['1','2','3','5']:
        params_with_code = {**succ_bid_base_params, 'bsns_div_cd': code}
        df = search_and_process(
            client.get_successful_bid_info, params_with_code, search_keywords, 'bidNtceNm', log,
            log_prefix=f"ë‚™ì°°ì •ë³´(ì½”ë“œ:{code})"
        )
        if not df.empty:
            succ_dfs.append(df)
            
    all_found_data['successful_bid'] = pd.concat(succ_dfs, ignore_index=True) if succ_dfs else pd.DataFrame()
    if not all_found_data['successful_bid'].empty: 
        upsert_project_data(all_found_data['successful_bid'], 'successful_bid')

    # 5. ê³„ì•½ì •ë³´ (ì§€ì • ê¸°ê°„)
    log.append("\n--- 5. ê³„ì•½ ì •ë³´ ì¡°íšŒ ì‹œìž‘ ---")
    contract_params = {
        'start_date': start_date_str, 
        'end_date': end_date_str
    }
    all_found_data['contract'] = search_and_process(
        client.get_contract_info, contract_params, search_keywords, 'cntrctNm', log,
        log_prefix=f"ê³„ì•½ì •ë³´({start_date_str}~{end_date_str})"
    )
    if not all_found_data['contract'].empty: 
        upsert_project_data(all_found_data['contract'], 'contract')
    
    # --- ë³´ê³ ì„œ ìƒì„± ë° í›„ì²˜ë¦¬ ---
    log.append("\n--- 6. ë³´ê³ ì„œ ìƒì„± ë° ë¶„ì„ ì‹œìž‘ ---")
    # ë³´ê³ ì„œëŠ” DBì— ëˆ„ì ëœ ë°ì´í„° ì¤‘ í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
    report_dfs = create_report_data("procurement_data.db", list(search_keywords), log)
    risk_df, report_data_bytes, gemini_report = pd.DataFrame(), None, None

    # ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ì¡´ìž¬í•˜ë©´ ì—‘ì…€ ìƒì„± ì§„í–‰
    if report_dfs:
        # ë¦¬ìŠ¤í¬ ë¶„ì„
        if "flat" in report_dfs and report_dfs["flat"] is not None:
             risk_df = analyze_project_risk(report_dfs["flat"])

        # ì—‘ì…€ ì‹œíŠ¸ êµ¬ì„± (ìˆœì„œ ì§€ì •)
        excel_sheets = {
            "ì¢…í•© í˜„í™© ë³´ê³ ì„œ": report_dfs.get("structured"),
            "ë°œì£¼ê³„íš í˜„í™©": report_dfs.get("order_plan"),
            "ë¦¬ìŠ¤í¬ ë¶„ì„": risk_df,
            "ë°œì£¼ê³„íš ì›ë³¸": all_found_data.get('order_plan'),
            "ì‚¬ì „ê·œê²© ì›ë³¸": all_found_data.get('pre_standard'),
            "ìž…ì°°ê³µê³  ì›ë³¸": all_found_data.get('bid'),
            "ë‚™ì°°ì •ë³´ ì›ë³¸": all_found_data.get('successful_bid'),
            "ê³„ì•½ì •ë³´ ì›ë³¸": all_found_data.get('contract')
        }
        # ì—‘ì…€ íŒŒì¼ ìƒì„±
        try:
            report_data_bytes = save_integrated_excel(excel_sheets)
            log.append("âœ… í†µí•© ì—‘ì…€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            log.append(f"âš ï¸ ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # AI ë¶„ì„ ë° í‚¤ì›Œë“œ í™•ìž¥
    if gemini_key and report_dfs:
        # AI ì „ëžµ ë¶„ì„
        if "flat" in report_dfs and report_dfs["flat"] is not None:
            gemini_report = get_gemini_analysis(gemini_key, report_dfs["flat"], log)
        
        # í‚¤ì›Œë“œ í™•ìž¥
        if auto_expand_keywords:
            combined_df_list = [df for df in all_found_data.values() if df is not None and not df.empty]
            if combined_df_list:
                combined_df = pd.concat(combined_df_list, ignore_index=True)
                new_keywords = expand_keywords_with_gemini(gemini_key, combined_df, search_keywords, log)
                if new_keywords:
                    updated_keywords = search_keywords.union(new_keywords)
                    save_keywords(updated_keywords)
                    log.append("ðŸŽ‰ í‚¤ì›Œë“œ íŒŒì¼ì´ ìƒˆë¡­ê²Œ í™•ìž¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "log": log, 
        "risk_df": risk_df, 
        "report_file_data": report_data_bytes, 
        "report_filename": f"integrated_report_{execution_time.strftime('%Y%m%d_%H%M%S')}.xlsx", 
        "gemini_report": gemini_report
    }

# ====================================================================================
# ì°¸ê³ : ì•„ëž˜ í•¨ìˆ˜ë“¤ì€ ì´ì „ ë‹µë³€ì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì§€ë©´ ê´€ê³„ìƒ ìƒëžµ)
# ì‹¤ì œ analyzer.py íŒŒì¼ì—ëŠ” ì•„ëž˜ í•¨ìˆ˜ë“¤ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# ====================================================================================
# def setup_database(): ...
# def upsert_project_data(df, stage): ...
# def format_price(x): ...
# def get_gemini_analysis(api_key, df, log_list): ...
# def expand_keywords_with_gemini(api_key, df, existing_keywords, log_list): ...
# def analyze_project_risk(df: pd.DataFrame) -> pd.DataFrame: ...
# def create_report_data(db_path, keywords, log_list): ...