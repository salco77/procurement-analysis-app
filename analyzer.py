import os
import requests
import pandas as pd
from datetime import datetime, timedelta
import urllib.parse
import time
import sqlite3
import google.generativeai as genai
import io
from itertools import groupby # [ì¶”ê°€] í—¤ë” ë³‘í•©ì„ ìœ„í•´ í•„ìš”

# --- 1. í‚¤ì›Œë“œ ê´€ë¦¬ ---
KEYWORD_FILE = "keywords.txt"
INITIAL_KEYWORDS = ["ì§€ë¢°","ë“œë¡ ","ì‹œë®¬ë ˆì´í„°","ì‹œë®¬ë ˆì´ì…˜","ì „ì°¨","ìœ ì§€ë³´ìˆ˜","MRO","í•­ê³µ","ê°€ìƒí˜„ì‹¤","ì¦ê°•í˜„ì‹¤","í›ˆë ¨","VR","AR","MR","XR","ëŒ€í…ŒëŸ¬","ì†Œë¶€ëŒ€","CQB","íŠ¹ì „ì‚¬","ê²½ì°°ì²­"]

def load_keywords(initial_keywords: list) -> set:
    if not os.path.exists(KEYWORD_FILE):
        save_keywords(set(initial_keywords))
        return set(initial_keywords)
    with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
        return {line.strip() for line in f if line.strip()}

def save_keywords(keywords: set):
    with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
        for keyword in sorted(list(keywords)):
            f.write(keyword + '\n')

# --- 2. ìœ í‹¸ë¦¬í‹° ë° API í´ë¼ì´ì–¸íŠ¸ (ìˆ˜ì •ë¨) ---
def save_integrated_excel(data_frames: dict) -> bytes:
    """[í•µì‹¬ ìˆ˜ì •] ì—¬ëŸ¬ ë°ì´í„°í”„ë ˆì„ì„ í†µí•© ì—‘ì…€ë¡œ ì €ì¥í•©ë‹ˆë‹¤. MultiIndex ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê³  í—¤ë” ë³‘í•© ë° ìë™ ë„ˆë¹„ ë§ì¶¤ì„ ì§€ì›í•©ë‹ˆë‹¤."""
    output = io.BytesIO()
    # ìˆ˜ë™ ì œì–´ë¥¼ ìœ„í•´ xlsxwriter ì—”ì§„ ì‚¬ìš©
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        workbook = writer.book

        for sheet_name, df_data in data_frames.items():
            if df_data is None or df_data.empty: continue
            
            # --- [ì˜¤ë¥˜ í•´ê²° ë° ê¸°ëŠ¥ ê°œì„ ] MultiIndex ì²˜ë¦¬ ë¡œì§ ì „ë©´ ìˆ˜ì • ---
            # create_report_dataëŠ” 2ë‹¨ê³„ MultiIndexë¥¼ ìƒì„±í•˜ë¯€ë¡œ ì´ì— ë§ì¶° ìµœì í™”ëœ ë¡œì§ ì ìš©
            if isinstance(df_data.columns, pd.MultiIndex) and df_data.columns.nlevels == 2:
                # Pandas to_excel ëŒ€ì‹  xlsxwriterë¡œ ì§ì ‘ ì‘ì„±í•˜ì—¬ NotImplementedError íšŒí”¼
                
                # ì›Œí¬ì‹œíŠ¸ ìƒì„± ë° ë“±ë¡
                worksheet = workbook.add_worksheet(sheet_name)
                writer.sheets[sheet_name] = worksheet

                header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#D3D3D3', 'border': 1})
                
                # 1. í—¤ë” ì‘ì„± (2ë‹¨ê³„)
                col_idx = 0
                level0_headers = df_data.columns.get_level_values(0)
                level1_headers = df_data.columns.get_level_values(1)

                # Level 0 í—¤ë” ê·¸ë£¹ë³„ë¡œ ë°˜ë³µ ì²˜ë¦¬ (ì…€ ë³‘í•©)
                for header, group in groupby(level0_headers):
                    span = len(list(group))
                    if span > 1:
                        # ì…€ ë³‘í•©í•˜ì—¬ Level 0 í—¤ë” ì‘ì„± (0í–‰)
                        # merge_range(first_row, first_col, last_row, last_col, data, format)
                        worksheet.merge_range(0, col_idx, 0, col_idx + span - 1, str(header), header_format)
                    else:
                        # ë‹¨ì¼ ì…€ì— Level 0 í—¤ë” ì‘ì„±
                        worksheet.write(0, col_idx, str(header), header_format)
                    
                    # Level 1 í—¤ë” ì‘ì„± (1í–‰)
                    for i in range(span):
                        worksheet.write(1, col_idx + i, str(level1_headers[col_idx + i]), header_format)
                    
                    col_idx += span

                # 2. ë°ì´í„° ì‘ì„± (2í–‰ë¶€í„° ì‹œì‘)
                start_row = 2
                
                # NaN/NaT ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                # Pandas ë²„ì „ì— ë”°ë¥¸ í˜¸í™˜ì„± ì²˜ë¦¬ (applymap vs map)
                def clean_data(x):
                    return x if pd.notna(x) else ""

                try:
                    # Pandas ìµœì‹  ë²„ì „ (map)
                    data_to_write = df_data.map(clean_data).values.tolist()
                except AttributeError:
                    # Pandas êµ¬ë²„ì „ í˜¸í™˜ì„± (applymap)
                    data_to_write = df_data.applymap(clean_data).values.tolist()

                for row_data in data_to_write:
                    worksheet.write_row(start_row, 0, row_data)
                    start_row += 1
                
                # 3. ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì • (ê°€ë…ì„± í–¥ìƒ)
                for i in range(len(df_data.columns)):
                    # í—¤ë” ê¸¸ì´ì™€ ë°ì´í„° ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
                    header_len = max(len(str(level0_headers[i])), len(str(level1_headers[i])))
                    data_max_len = max((len(str(row[i])) for row in data_to_write), default=0)
                    max_len = max(header_len, data_max_len)
                    # ë„ˆë¹„ ì„¤ì • (ìµœì†Œ 10, ìµœëŒ€ 60ìœ¼ë¡œ ì œí•œ)
                    worksheet.set_column(i, i, min(60, max(10, max_len + 2)))

            else: 
                # ì¼ë°˜ DF ë˜ëŠ” ì˜ˆìƒì¹˜ ëª»í•œ MultiIndex ë ˆë²¨ ì²˜ë¦¬
                df_data.to_excel(writer, sheet_name=sheet_name, index=False)

    return output.getvalue()


class NaraJangteoApiClient:
    # (ì´í•˜ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë“¤ì€ ë³€ê²½ ì—†ìŒ)
    def __init__(self, service_key: str):
        if not service_key: raise ValueError("ì„œë¹„ìŠ¤ í‚¤ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        self.service_key = service_key
        self.base_url = "http://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
    def _make_request(self, endpoint: str, params: dict, log_list: list):
        try:
            url = f"{self.base_url}/{endpoint}?{urllib.parse.urlencode({'ServiceKey': self.service_key, 'pageNo': 1, 'numOfRows': 999, 'type': 'json', **params}, safe='/%')}"
            response = requests.get(url, timeout=60)
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            log_list.append(f"ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []
        except ValueError:
            log_list.append(f"API ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤: {response.text}")
            return []

        response_data = data.get('response', {})
        header = response_data.get('header', {})
        if header.get('resultCode') != '00':
            log_list.append(f"API ì˜¤ë¥˜: {header.get('resultMsg', 'ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ')}")
        return response_data.get('body', {}).get('items', [])

    def get_pre_standard_specs(self, start_date, end_date, log_list):
        params = {'rgstBgnDt': start_date, 'rgstEndDt': end_date}
        return self._make_request("getDataSetOpnStdPrdnmInfo", params, log_list)
    def get_bid_announcements(self, start_date, end_date, log_list):
        params = {'bidNtceBgnDt': start_date, 'bidNtceEndDt': end_date}
        return self._make_request("getDataSetOpnStdBidPblancInfo", params, log_list)
    def get_successful_bid_info(self, start_date, end_date, log_list, bsns_div_cd):
        params = {'opengBgnDt': start_date, 'opengEndDt': end_date, 'bsnsDivCd': bsns_div_cd}
        return self._make_request("getDataSetOpnStdScsbidInfo", params, log_list)
    def get_contract_info(self, start_date, end_date, log_list):
        params = {'cntrctCnclsBgnDate': start_date, 'cntrctCnclsEndDate': end_date}
        return self._make_request("getDataSetOpnStdCntrctInfo", params, log_list)


def search_and_process(api_client, fetch_function, date_params, keywords, search_field, log_list, **kwargs):
    start_date, end_date = date_params
    log_list.append(f"ê¸°ê°„({search_field}): {start_date} ~ {end_date} ì¡°íšŒ ì‹œì‘...")
    raw_data = fetch_function(start_date=start_date, end_date=end_date, log_list=log_list, **kwargs)
    if not raw_data: log_list.append("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return pd.DataFrame()
    log_list.append(f"ì´ {len(raw_data)}ê±´ ìˆ˜ì‹ . í‚¤ì›Œë“œ í•„í„°ë§ ì‹œì‘...")
    f = [i for i in raw_data if i and search_field in i and i[search_field] and any(k.lower() in i[search_field].lower() for k in keywords)]
    log_list.append(f"í•„í„°ë§ í›„ {len(f)}ê±´ ë°œê²¬.")
    return pd.DataFrame(f)

# --- 3. ë°ì´í„°ë² ì´ìŠ¤ ---
def setup_database():
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    cursor.execute("CREATE TABLE IF NOT EXISTS projects (bidNtceNo TEXT PRIMARY KEY, bidNtceNm TEXT, ntcelnsttNm TEXT, presmptPrce INTEGER, bid_status TEXT DEFAULT 'ê³µê³ ', bidNtceDate TEXT, sucsfCorpNm TEXT, cntrctAmt INTEGER, cntrctDate TEXT)")
    try:
        for col in ['prestandard_status', 'prestandard_no', 'prestandard_date']: cursor.execute(f"ALTER TABLE projects ADD COLUMN {col} TEXT")
    except sqlite3.OperationalError: pass
    conn.commit(); conn.close()

def upsert_project_data(df, stage):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    for _, r in df.iterrows():
        if stage=='bid': conn.execute("INSERT OR IGNORE INTO projects (bidNtceNo, bidNtceNm, ntcelnsttNm, presmptPrce, bidNtceDate, prestandard_status, prestandard_no, prestandard_date) VALUES (?,?,?,?,?,?,?,?)", (r.get('bidNtceNo'),r.get('bidNtceNm'),r.get('ntcelnsttNm'),r.get('presmptPrce'),r.get('bidNtceDate'),r.get('prestandard_status'),r.get('prestandard_no'),r.get('prestandard_date')))
        elif stage=='successful_bid': conn.execute("UPDATE projects SET bid_status='ë‚™ì°°', sucsfCorpNm=? WHERE bidNtceNo=? AND bid_status='ê³µê³ '",(r.get('sucsfCorpNm'),r.get('bidNtceNo')))
        elif stage=='contract': conn.execute("UPDATE projects SET bid_status='ê³„ì•½ì™„ë£Œ', sucsfCorpNm=?, cntrctAmt=?, cntrctDate=? WHERE bidNtceNo=?",(r.get('rprsntCorpNm'),r.get('cntrctAmt'),r.get('cntrctCnclsDate'),r.get('bidNtceNo')))
    conn.commit(); conn.close()


# --- 4. AI ë¶„ì„, ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ë³´ê³ ì„œ ---
def get_gemini_analysis(api_key, df, log_list):
    if df.empty: log_list.append("AIê°€ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return None
    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì‹œì‘...")
        data_for_prompt = df[[c for c in ['prestandard_status', 'bidNtceNm', 'bid_status', 'ntcelnsttNm', 'presmptPrce', 'sucsfCorpNm', 'cntrctAmt'] if c in df.columns]].head(30).to_string()
        prompt = f"""ë‹¹ì‹ ì€ 'ìœ„ì¹˜ ë° ë™ì‘ ì¸ì‹ ê¸°ìˆ 'ì„ ê¸°ë°˜ìœ¼ë¡œ VR/MR/AR/XR ê°€ìƒí™˜ê²½ê³¼ í˜„ì‹¤ê³µê°„ì„ ì •ë°€í•˜ê²Œ ë§¤ì¹­(ê³µê°„ ì •í•©)í•˜ëŠ” ê¸°ìˆ ì„ ë³´ìœ í•œ, êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ 'ì‹ ì‚¬ì—… ì „ëµíŒ€ì¥'ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë‚˜ë¼ì¥í„° ì¡°ë‹¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ì  ê°•ì ì„ ê·¹ëŒ€í™”í•˜ê³  ìƒˆë¡œìš´ ì‚¬ì—… ê¸°íšŒë¥¼ ë°œêµ´í•˜ê¸° ìœ„í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n[ë¶„ì„ ë°ì´í„°]\n{data_for_prompt}\n\n---\n## êµ°Â·ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ì—… ê¸°íšŒ ë¶„ì„ ë³´ê³ ì„œ\n\n### 1. ì´í‰ ë° í•µì‹¬ ë™í–¥\n(ìš°ë¦¬ íšŒì‚¬ì˜ XR ë° ê³µê°„ ì •í•© ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜, ê°€ìƒí˜„ì‹¤, MRO ì‚¬ì—…ì˜ ì¦ê° ì¶”ì„¸ë‚˜, ì£¼ëª©í•´ì•¼ í•  ë°œì£¼ ê¸°ê´€(ìœ¡êµ°, ê²½ì°°ì²­ ë“±)ì˜ ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.)\n\n### 2. ì£¼ìš” ì‚¬ì—… ì‹¬ì¸µ ë¶„ì„\n(ìš°ë¦¬ íšŒì‚¬ ê¸°ìˆ ê³¼ì˜ ì—°ê´€ì„±ì´ ê°€ì¥ ë†’ê±°ë‚˜ ì‚¬ì—…ì  ê°€ì¹˜ê°€ í° í”„ë¡œì íŠ¸ 3~5ê°œë¥¼ ì„ ì •í•˜ì—¬ ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. 'ë¶„ì„ ë° ì œì–¸' í•­ëª©ì—ëŠ” **ìš°ë¦¬ íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì¸ 'ê³µê°„ ì •í•©', 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹' ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì§€ì ì´ë‚˜, ê¸°ì¡´ ì‹œìŠ¤í…œì„ ê³ ë„í™”í•  ìˆ˜ ìˆëŠ” ì‚¬ì—… ê¸°íšŒ**ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.)\n\n| ì‚¬ì—…ëª… | ë°œì£¼ê¸°ê´€ | ì¶”ì •ê°€ê²©/ê³„ì•½ê¸ˆì•¡ | ì§„í–‰ ìƒíƒœ | ë¶„ì„ ë° ì œì–¸ (ìì‚¬ ê¸°ìˆ  ì—°ê³„ ë°©ì•ˆ) |\n|---|---|---|---|---|\n| (ì‚¬ì—…ëª…) | (ê¸°ê´€ëª…) | (ê¸ˆì•¡) | (ìƒíƒœ) | (ì˜ˆ: ì´ ì‚¬ì—…ì€ CQB í›ˆë ¨ ì‹œë®¬ë ˆì´í„°ë¡œ, **í˜„ì‹¤ ê³µê°„ê³¼ ê°€ìƒ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •ë°€í•˜ê²Œ ë§¤ì¹­í•˜ëŠ” ìš°ë¦¬ ê¸°ìˆ **ì´ í•µì‹¬ ê²½ìŸë ¥ì´ ë  ìˆ˜ ìˆìŒ.) |\n\n### 3. ê¸°ìˆ  ì—°ê³„ ê°€ëŠ¥ í‚¤ì›Œë“œ\n(ë°ì´í„°ì—ì„œ ì‹ë³„ëœ í‚¤ì›Œë“œ ì¤‘, ìš°ë¦¬ íšŒì‚¬ì˜ 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹' ë° 'XR ê°€ì‹œí™”' ê¸°ìˆ ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë  ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œ ì´ìƒ ì„ ì •í•˜ì—¬ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.)\n\n### 4. ì°¨ê¸° ì‚¬ì—… ì „ëµ ì œì–¸\n(ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ìš°ë¦¬ íšŒì‚¬ê°€ ë‹¤ìŒ ë¶„ê¸°ì— ì§‘ì¤‘í•´ì•¼ í•  ì‚¬ì—… ì˜ì—­, ê¸°ìˆ  ê³ ë„í™” ë°©í–¥ ë“±ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ì „ëµì„ 1~2ê°€ì§€ ì œì–¸í•´ì£¼ì„¸ìš”.)"""
        response = model.generate_content(prompt); log_list.append("Gemini ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì™„ë£Œ.")
        return response.text
    except Exception as e: log_list.append(f"Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return None

def expand_keywords_with_gemini(api_key, df, existing_keywords, log_list):
    if df.empty: log_list.append("í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return set()
    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ì§€ëŠ¥í˜• í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘...")
        project_titles = pd.concat([df[col] for col in ['bidNtceNm', 'cntrctNm'] if col in df.columns]).dropna().unique()
        project_titles_str = '\n- '.join(project_titles[:30])
        prompt = f"""ë‹¹ì‹ ì€ êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ ì¡°ë‹¬ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìš°ë¦¬ íšŒì‚¬ëŠ” 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹', 'XR ê³µê°„ ì •í•©' ê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” í˜„ì¬ ìš°ë¦¬ê°€ ì‚¬ìš©ì¤‘ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œì™€, ìµœê·¼ ì¡°ë‹¬ ì‹œìŠ¤í…œì—ì„œ ë°œê²¬ëœ ì‚¬ì—…ëª… ëª©ë¡ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ê¸°ì¡´ í‚¤ì›Œë“œì— ì—†ëŠ” **ìƒˆë¡œìš´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ 5~10ê°œ ì¶”ì²œ**í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.\n\n[ê¸°ì¡´ í‚¤ì›Œë“œ]\n{', '.join(sorted(list(existing_keywords)))}\n\n[ìµœê·¼ ë°œê²¬ëœ ì‚¬ì—…ëª…]\n- {project_titles_str}\n\n[ì¶”ì²œ í‚¤ì›Œë“œ]"""
        response = model.generate_content(prompt)
        new_keywords = {k.strip() for k in response.text.strip().split(',') if k.strip()}
        log_list.append(f"Geminiê°€ ì¶”ì²œí•œ ì‹ ê·œ í‚¤ì›Œë“œ: {new_keywords}")
        return new_keywords
    except Exception as e: log_list.append(f"Gemini í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"); return set()

def analyze_project_risk(df: pd.DataFrame) -> pd.DataFrame:
    ongoing_df = df[df['bid_status'].isin(['ê³µê³ ', 'ë‚™ì°°'])].copy()
    if ongoing_df.empty: return pd.DataFrame()
    ongoing_df['score'] = 0; ongoing_df['risk_reason'] = ''
    ongoing_df['bidNtceDate_dt'] = pd.to_datetime(ongoing_df['bidNtceDate'], errors='coerce')
    
    current_time = datetime.now() # ë¶„ì„ ì‹œì  ê³ ì •

    for index, row in ongoing_df.iterrows():
        reasons = []
        if pd.notna(row['bidNtceDate_dt']) and (current_time - row['bidNtceDate_dt']).days > 30 and row['bid_status'] == 'ê³µê³ ':
            ongoing_df.loc[index, 'score'] += 5; reasons.append('ê³µê³  í›„ 30ì¼ ê²½ê³¼')
        if row.get('prestandard_status') == 'í•´ë‹¹ ì—†ìŒ':
            ongoing_df.loc[index, 'score'] += 3; reasons.append('ì‚¬ì „ê·œê²© ì—†ìŒ')
        try:
            # ê¸ˆì•¡ ë°ì´í„°ëŠ” create_report_dataì—ì„œ í¬ë§·íŒ…ë˜ì–´ ì‰¼í‘œê°€ í¬í•¨ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±° í›„ ë³€í™˜
            price = int(str(row.get('presmptPrce', '0')).replace(',', ''))
            if 0 < price < 50000000: ongoing_df.loc[index, 'score'] += 2; reasons.append('ì†Œê·œëª¨ ì‚¬ì—…')
        except (ValueError, TypeError): pass
        ongoing_df.loc[index, 'risk_reason'] = ', '.join(reasons)
    ongoing_df['risk_level'] = ongoing_df['score'].apply(lambda s: 'ë†’ìŒ' if s >= 7 else ('ë³´í†µ' if s >= 4 else 'ë‚®ìŒ'))
    risk_table = ongoing_df[['bidNtceNm', 'ntcelnsttNm', 'bid_status', 'risk_level', 'risk_reason']]
    return risk_table.rename(columns={'bidNtceNm':'ì‚¬ì—…ëª…','ntcelnsttNm':'ë°œì£¼ê¸°ê´€','bid_status':'ì§„í–‰ ìƒíƒœ','risk_level':'ë¦¬ìŠ¤í¬ ë“±ê¸‰','risk_reason':'ì£¼ìš” ë¦¬ìŠ¤í¬'}).sort_values(by='ë¦¬ìŠ¤í¬ ë“±ê¸‰',key=lambda x:x.map({'ë†’ìŒ':0,'ë³´í†µ':1,'ë‚®ìŒ':2}))

def create_report_data(db_path, keywords, log_list):
    log_list.append("DBì—ì„œ ìµœì¢… ë°ì´í„° ì¡°íšŒ ì¤‘...")
    conn = sqlite3.connect(db_path); flat_df = pd.DataFrame()
    try:
        all_projects_df = pd.read_sql_query("SELECT * FROM projects ORDER BY bidNtceDate DESC", conn)
        if all_projects_df.empty: log_list.append("DBì— í”„ë¡œì íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return None
        flat_df = all_projects_df[all_projects_df['bidNtceNm'].str.contains('|'.join(keywords),case=False,na=False)].copy()
        if flat_df.empty: log_list.append("í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."); return None
        
        # ë‚ ì§œ í¬ë§·íŒ…
        for col in ['prestandard_date','bidNtceDate','cntrctDate']:
            if col in flat_df.columns: flat_df[col] = pd.to_datetime(flat_df[col],errors='coerce').dt.strftime('%Y-%m-%d')
        
        # ê¸ˆì•¡ í¬ë§·íŒ… (ì•ˆì •ì„± í–¥ìƒ)
        def format_price(x):
            if pd.isna(x):
                return ""
            try:
                # floatë¡œ ë¨¼ì € ë³€í™˜í•˜ì—¬ ì‹¤ìˆ˜í˜• ë°ì´í„°ë„ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ í•¨
                return f"{int(float(x)):,}"
            except (ValueError, TypeError):
                # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
                return ""

        for col in ['presmptPrce','cntrctAmt']:
            if col in flat_df.columns: 
                flat_df[col] = flat_df[col].apply(format_price)

        structured_columns = {('í”„ë¡œì íŠ¸ ê°œìš”','ì‚¬ì—…ëª…'):flat_df.get('bidNtceNm'), ('í”„ë¡œì íŠ¸ ê°œìš”','ë°œì£¼ê¸°ê´€'):flat_df.get('ntcelnsttNm'), ('ì§„í–‰ í˜„í™©','ì¢…í•© ìƒíƒœ'):flat_df.get('bid_status'), ('ì§„í–‰ í˜„í™©','ë‚™ì°°/ê³„ì•½ì‚¬'):flat_df.get('sucsfCorpNm'), ('ì‚¬ì „ ê·œê²© ì •ë³´','ê³µê°œ ìƒíƒœ'):flat_df.get('prestandard_status'), ('ì‚¬ì „ ê·œê²© ì •ë³´','ê³µê°œì¼'):flat_df.get('prestandard_date'), ('ì…ì°° ê³µê³  ì •ë³´','ê³µê³ ì¼'):flat_df.get('bidNtceDate'), ('ì…ì°° ê³µê³  ì •ë³´','ì¶”ì •ê°€ê²©'):flat_df.get('presmptPrce'), ('ê³„ì•½ ì²´ê²° ì •ë³´','ê³„ì•½ì¼'):flat_df.get('cntrctDate'), ('ê³„ì•½ ì²´ê²° ì •ë³´','ê³„ì•½ê¸ˆì•¡'):flat_df.get('cntrctAmt'), ('ì°¸ì¡° ë²ˆí˜¸','ì‚¬ì „ê·œê²©ë²ˆí˜¸'):flat_df.get('prestandard_no'), ('ì°¸ì¡° ë²ˆí˜¸','ì…ì°°ê³µê³ ë²ˆí˜¸'):flat_df.get('bidNtceNo')}
        structured_df = pd.DataFrame(structured_columns)
        log_list.append("ë³´ê³ ì„œìš© ë°ì´í„°í”„ë ˆì„ ìƒì„± ì™„ë£Œ.")
        return {"flat": flat_df, "structured": structured_df}
    except Exception as e: log_list.append(f"ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"); return None
    finally: conn.close()


# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_analysis(search_keywords: set, client: NaraJangteoApiClient, gemini_key: str, auto_expand_keywords: bool = True):
    log = []; all_found_data = {}
    # ë¶„ì„ ì‹œì‘ ì‹œì ì˜ ì‹œê°„ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
    fixed_end_date = datetime.now()
    log.append(f"ğŸ’¡ ì •ë³´: í˜„ì¬ ë‚ ì§œ {fixed_end_date.strftime('%Y-%m-%d')} ê¸°ì¤€ìœ¼ë¡œ ìµœì‹  ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
    start_dt_60d, start_dt_28d, start_dt_6d = fixed_end_date-timedelta(days=60), fixed_end_date-timedelta(days=28), fixed_end_date-timedelta(days=6)
    
    all_found_data['pre_standard'] = search_and_process(client, client.get_pre_standard_specs, (start_dt_60d.strftime('%Y%m%d'), fixed_end_date.strftime('%Y%m%d')), search_keywords, 'prdctClsfcNoNm', log)
    pre_standard_map = {r['bfSpecRgstNo']: r for _, r in all_found_data['pre_standard'].iterrows() if 'bfSpecRgstNo' in r and r['bfSpecRgstNo']} if not all_found_data['pre_standard'].empty else {}
    
    bid_df = search_and_process(client, client.get_bid_announcements, (start_dt_28d.strftime('%Y%m%d%H%M'), fixed_end_date.strftime('%Y%m%d%H%M')), search_keywords, 'bidNtceNm', log)
    if not bid_df.empty:
        def link_pre_standard(row):
            spec_no = row.get('bfSpecRgstNo')
            return ("í™•ì¸", spec_no, pre_standard_map[spec_no].get('registDt')) if spec_no and spec_no in pre_standard_map else ("í•´ë‹¹ ì—†ìŒ", None, None)
        bid_df[['prestandard_status', 'prestandard_no', 'prestandard_date']] = bid_df.apply(link_pre_standard, axis=1, result_type='expand')
    all_found_data['bid'] = bid_df
    upsert_project_data(bid_df, 'bid')
    
    succ_dfs = [df for code in ['1','2','3','5'] if not (df := search_and_process(client, client.get_successful_bid_info, (start_dt_6d.strftime('%Y%m%d%H%M'), fixed_end_date.strftime('%Y%m%d%H%M')), search_keywords, 'bidNtceNm', log, bsns_div_cd=code)).empty]
    all_found_data['successful_bid'] = pd.concat(succ_dfs, ignore_index=True) if succ_dfs else pd.DataFrame()
    if not all_found_data['successful_bid'].empty: upsert_project_data(all_found_data['successful_bid'], 'successful_bid')

    all_found_data['contract'] = search_and_process(client, client.get_contract_info, (start_dt_28d.strftime('%Y%m%d'), fixed_end_date.strftime('%Y%m%d')), search_keywords, 'cntrctNm', log)
    if not all_found_data['contract'].empty: upsert_project_data(all_found_data['contract'], 'contract')
    
    report_dfs = create_report_data("procurement_data.db", list(search_keywords), log)
    risk_df, report_data_bytes, gemini_report = pd.DataFrame(), None, None

    if report_dfs:
        risk_df = analyze_project_risk(report_dfs["flat"])
        excel_sheets = {"ì¢…í•© í˜„í™© ë³´ê³ ì„œ": report_dfs["structured"], "ë¦¬ìŠ¤í¬ ë¶„ì„": risk_df, "ì‚¬ì „ê·œê²© ì›ë³¸": all_found_data.get('pre_standard'), "ì…ì°°ê³µê³  ì›ë³¸": all_found_data.get('bid'), "ë‚™ì°°ì •ë³´ ì›ë³¸": all_found_data.get('successful_bid'), "ê³„ì•½ì •ë³´ ì›ë³¸": all_found_data.get('contract')}
        # ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œë¡œ ì˜¤ë¥˜ í•´ê²°
        report_data_bytes = save_integrated_excel(excel_sheets)

    if gemini_key and report_dfs:
        gemini_report = get_gemini_analysis(gemini_key, report_dfs["flat"], log)
        if auto_expand_keywords and any(not df.empty for df in all_found_data.values()):
            combined_df = pd.concat([df for df in all_found_data.values() if not df.empty], ignore_index=True)
            new_keywords = expand_keywords_with_gemini(gemini_key, combined_df, search_keywords, log)
            if new_keywords:
                updated_keywords = search_keywords.union(new_keywords)
                save_keywords(updated_keywords)
                log.append("í‚¤ì›Œë“œ íŒŒì¼ì´ ìƒˆë¡­ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # íŒŒì¼ ì´ë¦„ë„ ê³ ì •ëœ ì‹œê°„ì„ ì‚¬ìš©
    return {"log": log, "risk_df": risk_df, "report_file_data": report_data_bytes, "report_filename": f"integrated_report_{fixed_end_date.strftime('%Y%m%d_%H%M%S')}.xlsx", "gemini_report": gemini_report}