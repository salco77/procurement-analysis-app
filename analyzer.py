import os
import requests
import pandas as pd
from datetime import datetime, timedelta, date
import urllib.parse
import time
import sqlite3
import google.generativeai as genai
import io
from itertools import groupby
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# --- 1. í‚¤ì›Œë“œ ê´€ë¦¬ ---
KEYWORD_FILE = "keywords.txt"
INITIAL_KEYWORDS = ["ì§€ë¢°","ë“œë¡ ","ì‹œë®¬ë ˆì´í„°","ì‹œë®¬ë ˆì´ì…˜","ì „ì°¨","ìœ ì§€ë³´ìˆ˜","MRO","í•­ê³µ","ê°€ìƒí˜„ì‹¤","ì¦ê°•í˜„ì‹¤","í›ˆë ¨","VR","AR","MR","XR","ëŒ€í…ŒëŸ¬","ì†Œë¶€ëŒ€","CQB","íŠ¹ì „ì‚¬","ê²½ì°°ì²­"]

def load_keywords(initial_keywords: list) -> set:
    if not os.path.exists(KEYWORD_FILE):
        save_keywords(set(initial_keywords))
        return set(initial_keywords)
    try:
        with open(KEYWORD_FILE, 'r', encoding='utf-8') as f:
            return {line.strip() for line in f if line.strip()}
    except Exception as e:
        logging.error(f"Error loading keywords: {e}")
        return set(initial_keywords)

def save_keywords(keywords: set):
    try:
        with open(KEYWORD_FILE, 'w', encoding='utf-8') as f:
            for keyword in sorted(list(keywords)):
                f.write(keyword + '\n')
    except Exception as e:
        logging.error(f"Error saving keywords: {e}")

# --- 2. ìœ í‹¸ë¦¬í‹° ë° API í´ë¼ì´ì–¸íŠ¸ ---

# í—¬í¼ í•¨ìˆ˜: ì•ˆì „í•œ ì •ìˆ˜ ë³€í™˜ (Robustness í–¥ìƒ)
def safe_int(value):
    try:
        # ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬ ì¶”ê°€
        return int(float(value)) if value is not None and str(value).strip() != "" else None
    except (ValueError, TypeError):
        return None

def save_integrated_excel(data_frames: dict) -> bytes:
    """ì—¬ëŸ¬ ë°ì´í„°í”„ë ˆì„ì„ í†µí•© ì—‘ì…€ë¡œ ì €ì¥í•©ë‹ˆë‹¤. MultiIndex ë° ë°œì£¼ê³„íš ìŠ¤íƒ€ì¼ë§ ì§€ì›."""
    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            workbook = writer.book

            # ë°œì£¼ê³„íš ì‹œíŠ¸ ìŠ¤íƒ€ì¼ ì •ì˜ (ì—°í•œ ë…¸ë€ìƒ‰)
            order_plan_header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#FFF2CC', 'border': 1})

            for sheet_name, df_data in data_frames.items():
                if df_data is None or df_data.empty: continue
                
                # MultiIndex ì²˜ë¦¬ ë¡œì§ (ì¢…í•© í˜„í™© ë³´ê³ ì„œ)
                if isinstance(df_data.columns, pd.MultiIndex) and df_data.columns.nlevels == 2:
                    worksheet = workbook.add_worksheet(sheet_name)
                    writer.sheets[sheet_name] = worksheet
                    header_format = workbook.add_format({'bold': True, 'text_wrap': True, 'valign': 'vcenter', 'align': 'center', 'fg_color': '#D3D3D3', 'border': 1})
                    
                    # 1. í—¤ë” ì‘ì„± (ë³‘í•© í¬í•¨)
                    col_idx = 0
                    level0_headers = df_data.columns.get_level_values(0)
                    level1_headers = df_data.columns.get_level_values(1)

                    for header, group in groupby(level0_headers):
                        span = len(list(group))
                        if span > 1:
                            worksheet.merge_range(0, col_idx, 0, col_idx + span - 1, str(header), header_format)
                        else:
                            worksheet.write(0, col_idx, str(header), header_format)
                        for i in range(span):
                            worksheet.write(1, col_idx + i, str(level1_headers[col_idx + i]), header_format)
                        col_idx += span

                    # 2. ë°ì´í„° ì‘ì„±
                    start_row = 2
                    def clean_data(x): return x if pd.notna(x) else ""
                    
                    # Pandas ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
                    try:
                        if hasattr(df_data, 'map'):
                             data_to_write = df_data.map(clean_data).values.tolist()
                        else:
                             # applymap (êµ¬ë²„ì „ í˜¸í™˜ì„±)
                             data_to_write = df_data.applymap(clean_data).values.tolist()
                    except Exception:
                        # Fallback
                        data_to_write = df_data.fillna("").values.tolist()

                    for row_data in data_to_write:
                        worksheet.write_row(start_row, 0, row_data)
                        start_row += 1
                    
                    # 3. ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
                    for i in range(len(df_data.columns)):
                        header_len = max(len(str(level0_headers[i])), len(str(level1_headers[i])))
                        try:
                            data_max_len = max((len(str(row[i])) for row in data_to_write), default=0)
                        except Exception:
                            data_max_len = 10
                        max_len = max(header_len, data_max_len)
                        worksheet.set_column(i, i, min(60, max(10, max_len + 2)))

                # ë°œì£¼ê³„íš í˜„í™© ì‹œíŠ¸ ì²˜ë¦¬ (ìŠ¤íƒ€ì¼ ì ìš©)
                elif sheet_name == "ë°œì£¼ê³„íš í˜„í™©":
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False, startrow=1, header=False)
                    worksheet = writer.sheets[sheet_name]
                    
                    # í—¤ë” ì‘ì„± ë° ìŠ¤íƒ€ì¼ ì ìš©
                    for col_num, value in enumerate(df_data.columns.values):
                        worksheet.write(0, col_num, value, order_plan_header_format)
                    
                    # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
                    for i, col in enumerate(df_data.columns):
                        try:
                            max_len = max(df_data[col].astype(str).map(len).max(), len(str(col)))
                        except Exception:
                            max_len = 15
                        worksheet.set_column(i, i, min(60, max(10, max_len + 2)))

                else: 
                    # ì¼ë°˜ DF ì²˜ë¦¬
                    df_data.to_excel(writer, sheet_name=sheet_name, index=False)
    except Exception as e:
        logging.error(f"Error saving Excel file: {e}")
        raise

    return output.getvalue()


class NaraJangteoApiClient:
    def __init__(self, service_key: str):
        if not service_key: raise ValueError("ì„œë¹„ìŠ¤ í‚¤ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")
        self.service_key = service_key
        self.base_url_std = "http://apis.data.go.kr/1230000/ao/PubDataOpnStdService"
        self.base_url_plan = "http://apis.data.go.kr/1230000/ao/OrderPlanSttusService"

    def _make_request(self, base_url: str, endpoint: str, params: dict, log_list: list):
        try:
            url = f"{base_url}/{endpoint}?{urllib.parse.urlencode({'ServiceKey': self.service_key, 'pageNo': 1, 'numOfRows': 999, 'type': 'json', **params}, safe='/%')}"
            # íƒ€ì„ì•„ì›ƒì„ 90ì´ˆë¡œ ì„¤ì •
            response = requests.get(url, timeout=90)
            response.raise_for_status()
            data = response.json()
        except requests.exceptions.RequestException as e:
            log_list.append(f"âš ï¸ ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì‹¤íŒ¨ ({endpoint}): {e}")
            return []
        except ValueError:
            log_list.append(f"âš ï¸ API ì‘ë‹µì´ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ ({endpoint}): {response.text[:200]}...")
            return []

        response_data = data.get('response', {})
        header = response_data.get('header', {})
        if header.get('resultCode') != '00':
            log_list.append(f"âš ï¸ API ì˜¤ë¥˜ ({endpoint}): {header.get('resultMsg', 'ì˜¤ë¥˜ ë©”ì‹œì§€ ì—†ìŒ')}")
            return []
        
        body = response_data.get('body', {})
        if isinstance(body, list): return body
        if isinstance(body, dict):
            return body.get('items', [])
        return []

    # --- ë°œì£¼ê³„íší˜„í™©ì„œë¹„ìŠ¤ (OrderPlanSttusService) ---
    # years ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬
    def get_order_plans(self, year, log_list):
        # ë‹¨ì¼ ì—°ë„(str/int) ì…ë ¥ ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        years = [str(year)] if isinstance(year, (str, int)) else [str(y) for y in year]

        endpoints = {
            'ë¬¼í’ˆ': 'getOrderPlanSttusListThng',
            'ìš©ì—­': 'getOrderPlanSttusListServc',
            'ê³µì‚¬': 'getOrderPlanSttusListConst'
        }
        all_plans = []
        
        # ì—°ë„ë³„ë¡œ ë°˜ë³µ í˜¸ì¶œ
        for current_year in years:
            params = {'year': current_year}
            log_list.append(f"[{current_year}ë…„ë„] ë°œì£¼ê³„íš ì¡°íšŒ ì‹œì‘...")
            year_plans_count = 0
            for category, endpoint in endpoints.items():
                log_list.append(f"  - ì¹´í…Œê³ ë¦¬: {category} ì¡°íšŒ ì¤‘...")
                plans = self._make_request(self.base_url_plan, endpoint, params, log_list)
                if plans:
                    # ë°ì´í„°ì— ì¹´í…Œê³ ë¦¬ ë° ì—°ë„ ì •ë³´ ì¶”ê°€ (DB ì €ì¥ ì‹œ í™œìš©)
                    for plan in plans:
                        plan['category'] = category
                        plan['plan_year'] = current_year
                    all_plans.extend(plans)
                    year_plans_count += len(plans)
            log_list.append(f"[{current_year}ë…„ë„] ì´ {year_plans_count}ê±´ ìˆ˜ì‹ .")
        
        log_list.append(f"ë°œì£¼ê³„íš ì „ì²´ ì´ {len(all_plans)}ê±´ ìˆ˜ì‹  ì™„ë£Œ.")
        return all_plans

    # --- ê³µê³µë°ì´í„°ê°œë°©í‘œì¤€ì„œë¹„ìŠ¤ (PubDataOpnStdService) ---
    def get_pre_standard_specs(self, start_date, end_date, log_list):
        params = {'rgstBgnDt': start_date, 'rgstEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdPrdnmInfo", params, log_list)
    
    def get_bid_announcements(self, start_date, end_date, log_list):
        params = {'bidNtceBgnDt': start_date, 'bidNtceEndDt': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdBidPblancInfo", params, log_list)
    
    def get_successful_bid_info(self, start_date, end_date, log_list, bsns_div_cd):
        params = {'opengBgnDt': start_date, 'opengEndDt': end_date, 'bsnsDivCd': bsns_div_cd}
        return self._make_request(self.base_url_std, "getDataSetOpnStdScsbidInfo", params, log_list)
    
    def get_contract_info(self, start_date, end_date, log_list):
        params = {'cntrctCnclsBgnDate': start_date, 'cntrctCnclsEndDate': end_date}
        return self._make_request(self.base_url_std, "getDataSetOpnStdCntrctInfo", params, log_list)


def search_and_process(fetch_function, params, keywords, search_field, log_list, log_prefix=""):
    """API í˜¸ì¶œ ë° í‚¤ì›Œë“œ í•„í„°ë§ì„ ìˆ˜í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜"""
    # ë°œì£¼ê³„íšì€ fetch_function ë‚´ë¶€ì—ì„œ ë¡œê·¸ë¥¼ ì‹œì‘í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ
    if log_prefix != "ë°œì£¼ê³„íš":
        log_list.append(f"[{log_prefix}] ì¡°íšŒ ì‹œì‘...")
    
    # fetch_functionì— params ë”•ì…”ë„ˆë¦¬ë¥¼ ì–¸íŒ¨í‚¹í•˜ì—¬ ì „ë‹¬
    raw_data = fetch_function(log_list=log_list, **params)
    
    # ë°œì£¼ê³„íšì€ í•¨ìˆ˜ ë‚´ì—ì„œ ë¡œê·¸ë¥¼ ìƒì„¸íˆ ê¸°ë¡í–ˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€
    if log_prefix != "ë°œì£¼ê³„íš":
        if not raw_data: 
            # API/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜(âš ï¸)ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ì„ ê²½ìš°ì—ë§Œ "ë°ì´í„° ì—†ìŒ" ë¡œê·¸ ê¸°ë¡
            is_error = False
            if log_list:
                 # ìµœê·¼ ë¡œê·¸ í™•ì¸ (ì˜¤ë¥˜ ë¡œê·¸ëŠ” âš ï¸ë¡œ ì‹œì‘í•˜ë„ë¡ í†µì¼ë¨)
                 is_error = log_list[-1].startswith("âš ï¸")

            if not is_error:
                log_list.append("ì¡°íšŒëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        log_list.append(f"ì´ {len(raw_data)}ê±´ ìˆ˜ì‹ .")

    if not raw_data: return pd.DataFrame()

    log_list.append(f"í‚¤ì›Œë“œ í•„í„°ë§ ì‹œì‘ (ê²€ìƒ‰ í•„ë“œ: {search_field})...")
    # í‚¤ì›Œë“œ í•„í„°ë§ ë¡œì§
    filtered_data = [
        item for item in raw_data 
        if isinstance(item, dict) and item.get(search_field) and 
        # ê²€ìƒ‰ í•„ë“œ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        any(keyword.lower() in str(item[search_field]).lower() for keyword in keywords)
    ]
    log_list.append(f"í•„í„°ë§ í›„ {len(filtered_data)}ê±´ ë°œê²¬.")
    return pd.DataFrame(filtered_data)

# --- 3. ë°ì´í„°ë² ì´ìŠ¤ ---

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def setup_database():
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    
    # ê¸°ì¡´ projects í…Œì´ë¸” (ì…ì°°ê³µê³  ~ ê³„ì•½)
    cursor.execute("CREATE TABLE IF NOT EXISTS projects (bidNtceNo TEXT PRIMARY KEY, bidNtceNm TEXT, ntcelnsttNm TEXT, presmptPrce INTEGER, bid_status TEXT DEFAULT 'ê³µê³ ', bidNtceDate TEXT, sucsfCorpNm TEXT, cntrctAmt INTEGER, cntrctDate TEXT)")
    
    # ALTER TABLEë¡œ ì»¬ëŸ¼ ì¶”ê°€ ì‹œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ì˜¤ë¥˜ ë°œìƒ ë°©ì§€ (Robustness í–¥ìƒ)
    try:
        # PRAGMAë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ìŠ¤í‚¤ë§ˆ í™•ì¸
        existing_columns = [info[1] for info in cursor.execute("PRAGMA table_info(projects)")]
        if 'prestandard_status' not in existing_columns:
            cursor.execute("ALTER TABLE projects ADD COLUMN prestandard_status TEXT")
        if 'prestandard_no' not in existing_columns:
            cursor.execute("ALTER TABLE projects ADD COLUMN prestandard_no TEXT")
        if 'prestandard_date' not in existing_columns:
            cursor.execute("ALTER TABLE projects ADD COLUMN prestandard_date TEXT")
    except sqlite3.OperationalError as e:
        logging.warning(f"Error altering projects table: {e}")


    # ë°œì£¼ê³„íš í…Œì´ë¸” (order_plans)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS order_plans (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            plan_year TEXT,
            category TEXT,        -- ì¹´í…Œê³ ë¦¬ (ë¬¼í’ˆ/ìš©ì—­/ê³µì‚¬)
            dminsttNm TEXT,       -- ìˆ˜ìš”ê¸°ê´€ëª…
            prdctNm TEXT,         -- í’ˆëª… (ê²€ìƒ‰ ëŒ€ìƒ í•„ë“œ)
            asignBdgtAmt INTEGER, -- ë°°ì •ì˜ˆì‚°ì•¡
            orderInsttNm TEXT,    -- ë°œì£¼ê¸°ê´€ëª…
            orderPlanPrd TEXT,    -- ë°œì£¼ì˜ˆì •ì‹œê¸°
            cntrctMthdNm TEXT,    -- ê³„ì•½ë°©ë²•ëª…
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderPlanPrd)
        )
    """)
    
    conn.commit(); conn.close()

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def upsert_project_data(df, stage):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    
    for _, r in df.iterrows():
        try:
            if stage == 'bid':
                # safe_int í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©
                cursor.execute("""
                    INSERT OR IGNORE INTO projects 
                    (bidNtceNo, bidNtceNm, ntcelnsttNm, presmptPrce, bidNtceDate, prestandard_status, prestandard_no, prestandard_date) 
                    VALUES (?,?,?,?,?,?,?,?)
                """, (
                    r.get('bidNtceNo'), r.get('bidNtceNm'), r.get('ntcelnsttNm'), safe_int(r.get('presmptPrce')), r.get('bidNtceDate'),
                    r.get('prestandard_status'), r.get('prestandard_no'), r.get('prestandard_date')
                ))
            elif stage == 'successful_bid':
                cursor.execute("""
                    UPDATE projects SET bid_status='ë‚™ì°°', sucsfCorpNm=? 
                    WHERE bidNtceNo=? AND (bid_status='ê³µê³ ' OR bid_status IS NULL)
                """, (r.get('sucsfCorpNm'), r.get('bidNtceNo')))
            elif stage == 'contract':
                # ê³„ì•½ ë‹¨ê³„ì—ì„œëŠ” ëŒ€í‘œì—…ì²´ëª…(rprsntCorpNm) ì‚¬ìš©, safe_int ì‚¬ìš©
                cursor.execute("""
                    UPDATE projects SET bid_status='ê³„ì•½ì™„ë£Œ', sucsfCorpNm=?, cntrctAmt=?, cntrctDate=? 
                    WHERE bidNtceNo=?
                """, (r.get('rprsntCorpNm'), safe_int(r.get('cntrctAmt')), r.get('cntrctCnclsDate'), r.get('bidNtceNo')))
        except Exception as e:
            logging.error(f"Error upserting project data (stage: {stage}): {e} - Data: {r.to_dict()}")
            continue # ê°œë³„ ë ˆì½”ë“œ ì˜¤ë¥˜ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰

    conn.commit(); conn.close()

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def upsert_order_plan_data(df, log_list):
    if df.empty: return
    conn = sqlite3.connect("procurement_data.db")
    cursor = conn.cursor()
    initial_count = conn.total_changes
    
    for _, r in df.iterrows():
        try:
            # 'plan_year'ëŠ” API í˜¸ì¶œ ì‹œ ë°ì´í„°ì— ì¶”ê°€ë¨, safe_int ì‚¬ìš©
            cursor.execute("""
                INSERT OR IGNORE INTO order_plans 
                (plan_year, category, dminsttNm, prdctNm, asignBdgtAmt, orderInsttNm, orderPlanPrd, cntrctMthdNm) 
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                r.get('plan_year'),
                r.get('category'),
                r.get('dminsttNm'),
                r.get('prdctNm'),
                safe_int(r.get('asignBdgtAmt')),
                r.get('orderInsttNm'),
                r.get('orderPlanPrd'),
                r.get('cntrctMthdNm')
            ))
        except Exception as e:
            log_list.append(f"âš ï¸ ê²½ê³ : ë°œì£¼ê³„íš DB ì‚½ì… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} - ë°ì´í„°: {r.to_dict()}")
            continue
            
    conn.commit()
    new_records_count = conn.total_changes - initial_count
    log_list.append(f"ë°œì£¼ê³„íš ì •ë³´ DB ì €ì¥ ì™„ë£Œ (ì‹ ê·œ {new_records_count}ê±´).")
    conn.close()


# --- 4. AI ë¶„ì„, ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ë³´ê³ ì„œ ---

# ê¸ˆì•¡ í¬ë§·íŒ… í—¬í¼ í•¨ìˆ˜
def format_price(x):
    if pd.isna(x):
        return ""
    try:
        # ì…ë ¥ê°’ì´ ë¬¸ìì—´ì¼ ê²½ìš° ì‰¼í‘œ ì œê±° í›„ ë³€í™˜ ì‹œë„
        if isinstance(x, str):
             x = x.replace(',', '')
        return f"{int(float(x)):,}"
    except (ValueError, TypeError):
        return ""

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def get_gemini_analysis(api_key, df, log_list):
    # AI ë¶„ì„ì€ í”„ë¡œì íŠ¸ í˜„í™©(df, flat data)ì„ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰
    if df.empty: log_list.append("AIê°€ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return None
    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì‹œì‘...")
        
        # í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ì „ ê¸ˆì•¡ í¬ë§·íŒ… ì ìš© (AI ê°€ë…ì„± í–¥ìƒ)
        df_for_prompt = df.copy()
        if 'presmptPrce' in df_for_prompt.columns:
             df_for_prompt['presmptPrce'] = df_for_prompt['presmptPrce'].apply(format_price)
        if 'cntrctAmt' in df_for_prompt.columns:
             df_for_prompt['cntrctAmt'] = df_for_prompt['cntrctAmt'].apply(format_price)
             
        data_for_prompt_str = df_for_prompt[[c for c in ['prestandard_status', 'bidNtceNm', 'bid_status', 'ntcelnsttNm', 'presmptPrce', 'sucsfCorpNm', 'cntrctAmt'] if c in df_for_prompt.columns]].head(30).to_string()
        
        prompt = f"""ë‹¹ì‹ ì€ 'ìœ„ì¹˜ ë° ë™ì‘ ì¸ì‹ ê¸°ìˆ 'ì„ ê¸°ë°˜ìœ¼ë¡œ VR/MR/AR/XR ê°€ìƒí™˜ê²½ê³¼ í˜„ì‹¤ê³µê°„ì„ ì •ë°€í•˜ê²Œ ë§¤ì¹­(ê³µê°„ ì •í•©)í•˜ëŠ” ê¸°ìˆ ì„ ë³´ìœ í•œ, êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ 'ì‹ ì‚¬ì—… ì „ëµíŒ€ì¥'ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë‚˜ë¼ì¥í„° ì¡°ë‹¬ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ì  ê°•ì ì„ ê·¹ëŒ€í™”í•˜ê³  ìƒˆë¡œìš´ ì‚¬ì—… ê¸°íšŒë¥¼ ë°œêµ´í•˜ê¸° ìœ„í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì— ë§ì¶° ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n[ë¶„ì„ ë°ì´í„°]\n{data_for_prompt_str}\n\n---\n## êµ°Â·ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜ ì‚¬ì—… ê¸°íšŒ ë¶„ì„ ë³´ê³ ì„œ\n\n### 1. ì´í‰ ë° í•µì‹¬ ë™í–¥\n(ìš°ë¦¬ íšŒì‚¬ì˜ XR ë° ê³µê°„ ì •í•© ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ì€ í›ˆë ¨ ì‹œë®¬ë ˆì´ì…˜, ê°€ìƒí˜„ì‹¤, MRO ì‚¬ì—…ì˜ ì¦ê° ì¶”ì„¸ë‚˜, ì£¼ëª©í•´ì•¼ í•  ë°œì£¼ ê¸°ê´€(ìœ¡êµ°, ê²½ì°°ì²­ ë“±)ì˜ ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.)\n\n### 2. ì£¼ìš” ì‚¬ì—… ì‹¬ì¸µ ë¶„ì„\n(ìš°ë¦¬ íšŒì‚¬ ê¸°ìˆ ê³¼ì˜ ì—°ê´€ì„±ì´ ê°€ì¥ ë†’ê±°ë‚˜ ì‚¬ì—…ì  ê°€ì¹˜ê°€ í° í”„ë¡œì íŠ¸ 3~5ê°œë¥¼ ì„ ì •í•˜ì—¬ ì•„ë˜ í‘œ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. 'ë¶„ì„ ë° ì œì–¸' í•­ëª©ì—ëŠ” **ìš°ë¦¬ íšŒì‚¬ì˜ í•µì‹¬ ê¸°ìˆ ì¸ 'ê³µê°„ ì •í•©', 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹' ê¸°ìˆ ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” ì§€ì ì´ë‚˜, ê¸°ì¡´ ì‹œìŠ¤í…œì„ ê³ ë„í™”í•  ìˆ˜ ìˆëŠ” ì‚¬ì—… ê¸°íšŒ**ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.)\n\n| ì‚¬ì—…ëª… | ë°œì£¼ê¸°ê´€ | ì¶”ì •ê°€ê²©/ê³„ì•½ê¸ˆì•¡ | ì§„í–‰ ìƒíƒœ | ë¶„ì„ ë° ì œì–¸ (ìì‚¬ ê¸°ìˆ  ì—°ê³„ ë°©ì•ˆ) |\n|---|---|---|---|---|\n| (ì‚¬ì—…ëª…) | (ê¸°ê´€ëª…) | (ê¸ˆì•¡) | (ìƒíƒœ) | (ì˜ˆ: ì´ ì‚¬ì—…ì€ CQB í›ˆë ¨ ì‹œë®¬ë ˆì´í„°ë¡œ, **í˜„ì‹¤ ê³µê°„ê³¼ ê°€ìƒ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •ë°€í•˜ê²Œ ë§¤ì¹­í•˜ëŠ” ìš°ë¦¬ ê¸°ìˆ **ì´ í•µì‹¬ ê²½ìŸë ¥ì´ ë  ìˆ˜ ìˆìŒ.) |\n\n### 3. ê¸°ìˆ  ì—°ê³„ ê°€ëŠ¥ í‚¤ì›Œë“œ\n(ë°ì´í„°ì—ì„œ ì‹ë³„ëœ í‚¤ì›Œë“œ ì¤‘, ìš°ë¦¬ íšŒì‚¬ì˜ 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹' ë° 'XR ê°€ì‹œí™”' ê¸°ìˆ ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë  ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5ê°œ ì´ìƒ ì„ ì •í•˜ì—¬ ë¶ˆë › í¬ì¸íŠ¸ë¡œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.)\n\n### 4. ì°¨ê¸° ì‚¬ì—… ì „ëµ ì œì–¸\n(ìœ„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ìš°ë¦¬ íšŒì‚¬ê°€ ë‹¤ìŒ ë¶„ê¸°ì— ì§‘ì¤‘í•´ì•¼ í•  ì‚¬ì—… ì˜ì—­, ê¸°ìˆ  ê³ ë„í™” ë°©í–¥ ë“±ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì‹¤í–‰ ì „ëµì„ 1~2ê°€ì§€ ì œì–¸í•´ì£¼ì„¸ìš”.)"""
        response = model.generate_content(prompt); log_list.append("Gemini ë§ì¶¤í˜• ì „ëµ ë¶„ì„ ì™„ë£Œ.")
        return response.text
    except Exception as e: 
        log_list.append(f"âš ï¸ Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"Gemini API Error: {e}")
        return None

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def expand_keywords_with_gemini(api_key, df, existing_keywords, log_list):
    # í‚¤ì›Œë“œ í™•ì¥ì€ ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°(df)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰
    if df.empty: log_list.append("í‚¤ì›Œë“œ í™•ì¥ì„ ìœ„í•œ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); return set()
    try:
        genai.configure(api_key=api_key); model = genai.GenerativeModel('gemini-1.5-flash')
        log_list.append("Gemini APIë¡œ ì§€ëŠ¥í˜• í‚¤ì›Œë“œ í™•ì¥ ì‹œì‘...")
        # ë°œì£¼ê³„íš(prdctNm) ë° ì‚¬ì „ê·œê²©(prdctClsfcNoNm)ë„ í¬í•¨í•˜ì—¬ ë¶„ì„ ëŒ€ìƒ í™•ëŒ€
        project_titles = pd.concat([df[col] for col in ['bidNtceNm', 'cntrctNm', 'prdctClsfcNoNm', 'prdctNm'] if col in df.columns]).dropna().unique()
        project_titles_str = '\n- '.join(project_titles[:50]) # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê³ ë ¤
        
        prompt = f"""ë‹¹ì‹ ì€ êµ°/ê²½ í›ˆë ¨ ì‹œë®¬ë ˆì´í„° ì „ë¬¸ ê¸°ì—…ì˜ ì¡°ë‹¬ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìš°ë¦¬ íšŒì‚¬ëŠ” 'ìœ„ì¹˜/ë™ì‘ ì¸ì‹', 'XR ê³µê°„ ì •í•©' ê¸°ìˆ ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” í˜„ì¬ ìš°ë¦¬ê°€ ì‚¬ìš©ì¤‘ì¸ ê²€ìƒ‰ í‚¤ì›Œë“œì™€, ìµœê·¼ ì¡°ë‹¬ ì‹œìŠ¤í…œì—ì„œ ë°œê²¬ëœ ì‚¬ì—…ëª…/í’ˆëª… ëª©ë¡ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ ê³¼ ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ì„œë„ ê¸°ì¡´ í‚¤ì›Œë“œì— ì—†ëŠ” **ìƒˆë¡œìš´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ 5~10ê°œ ì¶”ì²œ**í•´ì£¼ì„¸ìš”. ê²°ê³¼ëŠ” ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”.\n\n[ê¸°ì¡´ í‚¤ì›Œë“œ]\n{', '.join(sorted(list(existing_keywords)))}\n\n[ìµœê·¼ ë°œê²¬ëœ ì‚¬ì—…ëª…/í’ˆëª…]\n- {project_titles_str}\n\n[ì¶”ì²œ í‚¤ì›Œë“œ]"""
        response = model.generate_content(prompt)
        new_keywords = {k.strip() for k in response.text.strip().split(',') if k.strip() and len(k.strip()) > 1}
        log_list.append(f"Geminiê°€ ì¶”ì²œí•œ ì‹ ê·œ í‚¤ì›Œë“œ: {new_keywords}")
        return new_keywords
    except Exception as e: 
        log_list.append(f"âš ï¸ Gemini í‚¤ì›Œë“œ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return set()

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def analyze_project_risk(df: pd.DataFrame) -> pd.DataFrame:
    # ë¦¬ìŠ¤í¬ ë¶„ì„ì€ flat data(ê¸ˆì•¡ì´ ìˆ«ìí˜•)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰
    ongoing_df = df[df['bid_status'].isin(['ê³µê³ ', 'ë‚™ì°°'])].copy()
    if ongoing_df.empty: return pd.DataFrame()
    
    ongoing_df['score'] = 0
    ongoing_df['risk_reason'] = ''
    # ë‚ ì§œ ì»¬ëŸ¼ì„ datetime ê°ì²´ë¡œ ë³€í™˜
    ongoing_df['bidNtceDate_dt'] = pd.to_datetime(ongoing_df['bidNtceDate'], errors='coerce')
    
    # ë¶„ì„ ì‹œì  ê¸°ì¤€ ì‹œê°„
    current_time = datetime.now()

    for index, row in ongoing_df.iterrows():
        reasons = []
        score = 0
        
        # ë¦¬ìŠ¤í¬ ìš”ì¸ 1: ê³µê³  í›„ 30ì¼ ê²½ê³¼ (ë¶„ì„ ì‹œì  ê¸°ì¤€)
        if pd.notna(row['bidNtceDate_dt']) and row['bid_status'] == 'ê³µê³ ':
            days_elapsed = (current_time - row['bidNtceDate_dt']).days
            if days_elapsed > 30:
                score += 5
                reasons.append(f'ê³µê³  í›„ {days_elapsed}ì¼ ê²½ê³¼')
        
        # ë¦¬ìŠ¤í¬ ìš”ì¸ 2: ì‚¬ì „ê·œê²© ë¯¸ê³µê°œ
        if row.get('prestandard_status') == 'í•´ë‹¹ ì—†ìŒ':
            score += 3
            reasons.append('ì‚¬ì „ê·œê²© ë¯¸ê³µê°œ')
        
        # ë¦¬ìŠ¤í¬ ìš”ì¸ 3: ì†Œê·œëª¨ ì‚¬ì—… (5ì²œë§Œì› ë¯¸ë§Œ)
        # flat dataì´ë¯€ë¡œ ê¸ˆì•¡ì€ ìˆ«ìí˜•ì„
        price = row.get('presmptPrce')
        # ìˆ«ìí˜•ì¸ì§€ í™•ì¸ í›„ ë¹„êµ
        if pd.notna(price) and isinstance(price, (int, float)) and 0 < price < 50000000:
            score += 2
            reasons.append('ì†Œê·œëª¨ ì‚¬ì—… (5ì²œë§Œì› ë¯¸ë§Œ)')
        
        ongoing_df.loc[index, 'score'] = score
        ongoing_df.loc[index, 'risk_reason'] = ', '.join(reasons)
    
    # ë¦¬ìŠ¤í¬ ë“±ê¸‰ ì‚°ì •
    ongoing_df['risk_level'] = ongoing_df['score'].apply(lambda s: 'ë†’ìŒ' if s >= 7 else ('ë³´í†µ' if s >= 4 else 'ë‚®ìŒ'))
    
    # ë³´ê³ ì„œìš© í…Œì´ë¸” ìƒì„± ë° ì •ë ¬
    risk_table = ongoing_df[['bidNtceNm', 'ntcelnsttNm', 'bid_status', 'risk_level', 'risk_reason']]
    return risk_table.rename(columns={'bidNtceNm':'ì‚¬ì—…ëª…','ntcelnsttNm':'ë°œì£¼ê¸°ê´€','bid_status':'ì§„í–‰ ìƒíƒœ','risk_level':'ë¦¬ìŠ¤í¬ ë“±ê¸‰','risk_reason':'ì£¼ìš” ë¦¬ìŠ¤í¬'}).sort_values(by='ë¦¬ìŠ¤í¬ ë“±ê¸‰',key=lambda x:x.map({'ë†’ìŒ':0,'ë³´í†µ':1,'ë‚®ìŒ':2}))

# [í•„ìˆ˜] ì˜¤ë¥˜ í•´ê²°ì„ ìœ„í•´ í¬í•¨ëœ í•¨ìˆ˜
def create_report_data(db_path, keywords, log_list):
    log_list.append("DBì—ì„œ ìµœì¢… ë°ì´í„° ì¡°íšŒ ì¤‘...")
    conn = sqlite3.connect(db_path)
    report_data = {} # ê²°ê³¼ë¥¼ ë‹´ì„ ë”•ì…”ë„ˆë¦¬

    try:
        # 1. í”„ë¡œì íŠ¸(ì…ì°°ê³µê³  ì´í›„) ë°ì´í„° ì²˜ë¦¬
        try:
            # bidNtceDate ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìµœì‹ ìˆœìœ¼ë¡œ ì¡°íšŒ
            all_projects_df = pd.read_sql_query("SELECT * FROM projects ORDER BY bidNtceDate DESC", conn)
        except pd.errors.DatabaseError as e:
            log_list.append(f"âš ï¸ í”„ë¡œì íŠ¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            all_projects_df = pd.DataFrame()

        
        if not all_projects_df.empty:
            # í‚¤ì›Œë“œ í•„í„°ë§
            flat_df = all_projects_df[all_projects_df['bidNtceNm'].str.contains('|'.join(keywords),case=False,na=False)].copy()
            
            if not flat_df.empty:
                # [ì¤‘ìš”] ë¶„ì„ìš© ì›ë³¸ ë°ì´í„° ì €ì¥ (ê¸ˆì•¡: ìˆ«ìí˜•)
                report_data["flat"] = flat_df.copy()

                # ë³´ê³ ì„œìš© ë°ì´í„° í¬ë§·íŒ… ì‹œì‘ (ì´í›„ flat_dfëŠ” ë³´ê³ ì„œìš©ìœ¼ë¡œ ì‚¬ìš©)
                # ë‚ ì§œ í¬ë§·íŒ… (YYYY-MM-DD)
                for col in ['prestandard_date','bidNtceDate','cntrctDate']:
                    if col in flat_df.columns: 
                        flat_df[col] = pd.to_datetime(flat_df[col],errors='coerce').dt.strftime('%Y-%m-%d')
                
                # ê¸ˆì•¡ í¬ë§·íŒ… (ê¸ˆì•¡: ë¬¸ìì—´)
                for col in ['presmptPrce','cntrctAmt']:
                    if col in flat_df.columns: 
                        flat_df[col] = flat_df[col].apply(format_price)

                # êµ¬ì¡°í™”ëœ ë°ì´í„°í”„ë ˆì„ (MultiIndex) ìƒì„±
                structured_columns = {
                    ('í”„ë¡œì íŠ¸ ê°œìš”','ì‚¬ì—…ëª…'):flat_df.get('bidNtceNm'), 
                    ('í”„ë¡œì íŠ¸ ê°œìš”','ë°œì£¼ê¸°ê´€'):flat_df.get('ntcelnsttNm'), 
                    ('ì§„í–‰ í˜„í™©','ì¢…í•© ìƒíƒœ'):flat_df.get('bid_status'), 
                    ('ì§„í–‰ í˜„í™©','ë‚™ì°°/ê³„ì•½ì‚¬'):flat_df.get('sucsfCorpNm'), 
                    ('ì‚¬ì „ ê·œê²© ì •ë³´','ê³µê°œ ìƒíƒœ'):flat_df.get('prestandard_status'), 
                    ('ì‚¬ì „ ê·œê²© ì •ë³´','ê³µê°œì¼'):flat_df.get('prestandard_date'), 
                    ('ì…ì°° ê³µê³  ì •ë³´','ê³µê³ ì¼'):flat_df.get('bidNtceDate'), 
                    ('ì…ì°° ê³µê³  ì •ë³´','ì¶”ì •ê°€ê²©'):flat_df.get('presmptPrce'), 
                    ('ê³„ì•½ ì²´ê²° ì •ë³´','ê³„ì•½ì¼'):flat_df.get('cntrctDate'), 
                    ('ê³„ì•½ ì²´ê²° ì •ë³´','ê³„ì•½ê¸ˆì•¡'):flat_df.get('cntrctAmt'), 
                    ('ì°¸ì¡° ë²ˆí˜¸','ì‚¬ì „ê·œê²©ë²ˆí˜¸'):flat_df.get('prestandard_no'), 
                    ('ì°¸ì¡° ë²ˆí˜¸','ì…ì°°ê³µê³ ë²ˆí˜¸'):flat_df.get('bidNtceNo')
                }
                report_data["structured"] = pd.DataFrame(structured_columns)
                log_list.append("í”„ë¡œì íŠ¸ í˜„í™© ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ.")

        # 2. ë°œì£¼ê³„íš ë°ì´í„° ì²˜ë¦¬
        try:
            # created_at ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìµœì‹  ë°ì´í„° ì¡°íšŒ
            all_order_plans_df = pd.read_sql_query("SELECT * FROM order_plans ORDER BY created_at DESC", conn)
        except pd.errors.DatabaseError as e:
             log_list.append(f"âš ï¸ ë°œì£¼ê³„íš DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
             all_order_plans_df = pd.DataFrame()

        if not all_order_plans_df.empty:
             # í‚¤ì›Œë“œ í•„í„°ë§ (í’ˆëª… ê¸°ì¤€ - prdctNm)
            order_plan_df = all_order_plans_df[all_order_plans_df['prdctNm'].str.contains('|'.join(keywords),case=False,na=False)].copy()

            if not order_plan_df.empty:
                # ì¤‘ë³µ ì œê±°: (ì—°ë„, ì¹´í…Œê³ ë¦¬, ê¸°ê´€ëª…, í’ˆëª…) ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœì‹  ë°ì´í„°ë§Œ ë‚¨ê¹€
                # ì´ë¯¸ created_at DESCë¡œ ì •ë ¬ë˜ì—ˆìœ¼ë¯€ë¡œ keep='first' ì‚¬ìš©
                order_plan_df = order_plan_df.drop_duplicates(subset=['plan_year', 'category', 'dminsttNm', 'prdctNm'], keep='first')

                # ê¸ˆì•¡ í¬ë§·íŒ… (ì›ë³¸ ê¸ˆì•¡ì€ ìœ ì§€í•˜ê³  í¬ë§·íŒ…ëœ ì»¬ëŸ¼ ì¶”ê°€)
                order_plan_df['asignBdgtAmt_formatted'] = order_plan_df['asignBdgtAmt'].apply(format_price)
                
                # ë³´ê³ ì„œìš© ì»¬ëŸ¼ëª… ë³€ê²½ ë° ì„ íƒ
                order_plan_report_df = order_plan_df[[
                    'plan_year', 'category', 'dminsttNm', 'prdctNm', 'asignBdgtAmt_formatted', 'orderPlanPrd', 'cntrctMthdNm'
                ]].rename(columns={
                    'plan_year': 'ë…„ë„',
                    'category': 'êµ¬ë¶„(ë¬¼í’ˆ/ìš©ì—­/ê³µì‚¬)',
                    'dminsttNm': 'ìˆ˜ìš”ê¸°ê´€ëª…',
                    'prdctNm': 'í’ˆëª… (ì‚¬ì—…ëª…)',
                    'asignBdgtAmt_formatted': 'ë°°ì •ì˜ˆì‚°ì•¡',
                    'orderPlanPrd': 'ë°œì£¼ì˜ˆì •ì‹œê¸°',
                    'cntrctMthdNm': 'ê³„ì•½ë°©ë²•'
                })
                # ìµœì¢… ì •ë ¬: ì˜ˆì‚°ì•¡ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ (ë¬¸ìì—´ë¡œ í¬ë§·íŒ…ëœ ê¸ˆì•¡ì„ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ì •ë ¬)
                report_data["order_plan"] = order_plan_report_df.sort_values(by='ë°°ì •ì˜ˆì‚°ì•¡', ascending=False, key=lambda x: pd.to_numeric(x.str.replace(',', ''), errors='coerce'))
                log_list.append("ë°œì£¼ê³„íš í˜„í™© ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ.")

        if not report_data:
             log_list.append("DBì— í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°(í”„ë¡œì íŠ¸ ë˜ëŠ” ë°œì£¼ê³„íš)ê°€ ì—†ìŠµë‹ˆë‹¤.")
             return None
             
        return report_data

    except Exception as e: 
        log_list.append(f"âš ï¸ ë³´ê³ ì„œ ë°ì´í„° ìƒì„± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.exception(e)
        return None
    finally: 
        conn.close()


# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_analysis(search_keywords: set, client: NaraJangteoApiClient, gemini_key: str, start_date: date, end_date: date, auto_expand_keywords: bool = True):
    log = []; all_found_data = {}
    
    # ì‹¤í–‰ ì‹œì  ê¸°ë¡
    execution_time = datetime.now() 

    # --- ë‚ ì§œ í˜•ì‹ ì •ì˜ ë° ë³€í™˜ ---
    fmt_date = '%Y%m%d'
    fmt_datetime = '%Y%m%d%H%M'

    # ì‹œì‘ì¼ (00ì‹œ 00ë¶„) ì„¤ì •
    start_dt = datetime.combine(start_date, datetime.min.time())
    
    # ì¢…ë£Œì¼ (23ì‹œ 59ë¶„) ì„¤ì • ë° í˜„ì¬ ì‹œê°„ê³¼ ë¹„êµ (APIëŠ” ë¯¸ë˜ ë‚ ì§œ ì¡°íšŒ ë¶ˆê°€)
    end_dt_limit = datetime.combine(end_date, datetime.max.time().replace(microsecond=0))
    end_dt = min(execution_time, end_dt_limit)

    # í¬ë§·íŒ…ëœ ë¬¸ìì—´ ìƒì„±
    start_date_str = start_dt.strftime(fmt_date)
    end_date_str = end_dt.strftime(fmt_date)
    start_datetime_str = start_dt.strftime(fmt_datetime)
    end_datetime_str = end_dt.strftime(fmt_datetime)

    log.append(f"ğŸ’¡ ë¶„ì„ ì‹œì‘: ê²€ìƒ‰ ê¸°ê°„ {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
    if end_dt != end_dt_limit:
        log.append(f"â„¹ï¸ ì°¸ê³ : ì¢…ë£Œ ì‹œê°ì´ í˜„ì¬ ì‹œê°({end_dt.strftime('%H:%M')})ìœ¼ë¡œ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")


    # --- ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ---

    # 1. ë°œì£¼ê³„íš (ì—°ê°„)
    log.append("\n--- 1. ë°œì£¼ê³„íš ì •ë³´ ì¡°íšŒ ì‹œì‘ ---")
    # ì‹œì‘ ì—°ë„ë¶€í„° ì¢…ë£Œ ì—°ë„ê¹Œì§€ ëª¨ë“  ì—°ë„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    target_years = list(range(start_date.year, end_date.year + 1))
    
    # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° (yearì— ì—°ë„ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬)
    order_plan_params = {'year': target_years}
    
    # ë°ì´í„° ì¡°íšŒ ë° í•„í„°ë§
    all_found_data['order_plan'] = search_and_process(
        client.get_order_plans, order_plan_params, search_keywords, 'prdctNm', log, 
        log_prefix="ë°œì£¼ê³„íš"
    )
    # DB ì €ì¥
    if not all_found_data['order_plan'].empty:
        upsert_order_plan_data(all_found_data['order_plan'], log)

    # 2. ì‚¬ì „ê·œê²© (ì§€ì • ê¸°ê°„)
    log.append("\n--- 2. ì‚¬ì „ê·œê²© ì •ë³´ ì¡°íšŒ ì‹œì‘ ---")
    pre_standard_params = {
        'start_date': start_date_str, 
        'end_date': end_date_str
    }
    all_found_data['pre_standard'] = search_and_process(
        client.get_pre_standard_specs, pre_standard_params, search_keywords, 'prdctClsfcNoNm', log,
        log_prefix=f"ì‚¬ì „ê·œê²©({start_date_str}~{end_date_str})"
    )
    # ì‚¬ì „ê·œê²© ë²ˆí˜¸ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    pre_standard_map = {
        r['bfSpecRgstNo']: r for _, r in all_found_data['pre_standard'].iterrows() 
        if 'bfSpecRgstNo' in r and r['bfSpecRgstNo']
    } if not all_found_data['pre_standard'].empty else {}
    
    # 3. ì…ì°°ê³µê³  (ì§€ì • ê¸°ê°„, ì‹œê°„ í¬í•¨)
    log.append("\n--- 3. ì…ì°° ê³µê³  ì •ë³´ ì¡°íšŒ ì‹œì‘ ---")
    bid_params = {
        'start_date': start_datetime_str, 
        'end_date': end_datetime_str
    }
    bid_df = search_and_process(
        client.get_bid_announcements, bid_params, search_keywords, 'bidNtceNm', log,
        log_prefix=f"ì…ì°°ê³µê³ ({start_datetime_str}~{end_datetime_str})"
    )
    
    # ì…ì°°ê³µê³ ì™€ ì‚¬ì „ê·œê²© ì—°ê²°
    if not bid_df.empty:
        def link_pre_standard(row):
            spec_no = row.get('bfSpecRgstNo')
            if spec_no and spec_no in pre_standard_map:
                return ("í™•ì¸", spec_no, pre_standard_map[spec_no].get('registDt'))
            return ("í•´ë‹¹ ì—†ìŒ", None, None)
        bid_df[['prestandard_status', 'prestandard_no', 'prestandard_date']] = bid_df.apply(link_pre_standard, axis=1, result_type='expand')
    
    all_found_data['bid'] = bid_df
    upsert_project_data(bid_df, 'bid')

    # 4. ë‚™ì°°ì •ë³´ (ì§€ì • ê¸°ê°„, ì‹œê°„ í¬í•¨)
    log.append("\n--- 4. ë‚™ì°° ì •ë³´ ì¡°íšŒ ì‹œì‘ ---")
    succ_bid_base_params = {
        'start_date': start_datetime_str, 
        'end_date': end_datetime_str
    }
    succ_dfs = []
    # ì—…ë¬´ êµ¬ë¶„ ì½”ë“œë³„ ì¡°íšŒ
    for code in ['1','2','3','5']:
        params_with_code = {**succ_bid_base_params, 'bsns_div_cd': code}
        df = search_and_process(
            client.get_successful_bid_info, params_with_code, search_keywords, 'bidNtceNm', log,
            log_prefix=f"ë‚™ì°°ì •ë³´(ì½”ë“œ:{code})"
        )
        if not df.empty:
            succ_dfs.append(df)
            
    all_found_data['successful_bid'] = pd.concat(succ_dfs, ignore_index=True) if succ_dfs else pd.DataFrame()
    if not all_found_data['successful_bid'].empty: 
        upsert_project_data(all_found_data['successful_bid'], 'successful_bid')

    # 5. ê³„ì•½ì •ë³´ (ì§€ì • ê¸°ê°„)
    log.append("\n--- 5. ê³„ì•½ ì •ë³´ ì¡°íšŒ ì‹œì‘ ---")
    contract_params = {
        'start_date': start_date_str, 
        'end_date': end_date_str
    }
    all_found_data['contract'] = search_and_process(
        client.get_contract_info, contract_params, search_keywords, 'cntrctNm', log,
        log_prefix=f"ê³„ì•½ì •ë³´({start_date_str}~{end_date_str})"
    )
    if not all_found_data['contract'].empty: 
        upsert_project_data(all_found_data['contract'], 'contract')
    
    # --- ë³´ê³ ì„œ ìƒì„± ë° í›„ì²˜ë¦¬ ---
    log.append("\n--- 6. ë³´ê³ ì„œ ìƒì„± ë° ë¶„ì„ ì‹œì‘ ---")
    # ë³´ê³ ì„œëŠ” DBì— ëˆ„ì ëœ ë°ì´í„° ì¤‘ í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
    report_dfs = create_report_data("procurement_data.db", list(search_keywords), log)
    risk_df, report_data_bytes, gemini_report = pd.DataFrame(), None, None

    # ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ë©´ ì—‘ì…€ ìƒì„± ì§„í–‰
    if report_dfs:
        # ë¦¬ìŠ¤í¬ ë¶„ì„ (DBì—ì„œ ê°€ì ¸ì˜¨ ì›ë³¸ 'flat' ë°ì´í„° ì‚¬ìš© - ìˆ«ìí˜•)
        if "flat" in report_dfs and report_dfs["flat"] is not None:
             risk_df = analyze_project_risk(report_dfs["flat"])

        # ì—‘ì…€ ì‹œíŠ¸ êµ¬ì„± (ìˆœì„œ ì§€ì •)
        excel_sheets = {
            "ì¢…í•© í˜„í™© ë³´ê³ ì„œ": report_dfs.get("structured"),
            "ë°œì£¼ê³„íš í˜„í™©": report_dfs.get("order_plan"),
            "ë¦¬ìŠ¤í¬ ë¶„ì„": risk_df,
            "ë°œì£¼ê³„íš ì›ë³¸": all_found_data.get('order_plan'),
            "ì‚¬ì „ê·œê²© ì›ë³¸": all_found_data.get('pre_standard'),
            "ì…ì°°ê³µê³  ì›ë³¸": all_found_data.get('bid'),
            "ë‚™ì°°ì •ë³´ ì›ë³¸": all_found_data.get('successful_bid'),
            "ê³„ì•½ì •ë³´ ì›ë³¸": all_found_data.get('contract')
        }
        # ì—‘ì…€ íŒŒì¼ ìƒì„±
        try:
            report_data_bytes = save_integrated_excel(excel_sheets)
            log.append("âœ… í†µí•© ì—‘ì…€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.")
        except Exception as e:
            log.append(f"âš ï¸ ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # AI ë¶„ì„ ë° í‚¤ì›Œë“œ í™•ì¥
    if gemini_key:
        if report_dfs:
            # AI ì „ëµ ë¶„ì„ (DBì—ì„œ ê°€ì ¸ì˜¨ ì›ë³¸ 'flat' ë°ì´í„° ì‚¬ìš© - ìˆ«ìí˜•)
            if "flat" in report_dfs and report_dfs["flat"] is not None:
                gemini_report = get_gemini_analysis(gemini_key, report_dfs["flat"], log)
            
            # í‚¤ì›Œë“œ í™•ì¥ (APIë¡œ ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„° í†µí•©)
            if auto_expand_keywords:
                combined_df_list = [df for df in all_found_data.values() if df is not None and not df.empty]
                if combined_df_list:
                    combined_df = pd.concat(combined_df_list, ignore_index=True)
                    new_keywords = expand_keywords_with_gemini(gemini_key, combined_df, search_keywords, log)
                    if new_keywords:
                        updated_keywords = search_keywords.union(new_keywords)
                        save_keywords(updated_keywords)
                        log.append("ğŸ‰ í‚¤ì›Œë“œ íŒŒì¼ì´ ìƒˆë¡­ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
             log.append("â„¹ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ì–´ AI ë¶„ì„ ë° í‚¤ì›Œë“œ í™•ì¥ì„ ìƒëµí•©ë‹ˆë‹¤.")

    
    # ìµœì¢… ê²°ê³¼ ë°˜í™˜
    return {
        "log": log, 
        "risk_df": risk_df, 
        "report_file_data": report_data_bytes, 
        "report_filename": f"integrated_report_{execution_time.strftime('%Y%m%d_%H%M%S')}.xlsx", 
        "gemini_report": gemini_report
    }